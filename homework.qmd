---
title: "HomeWork"
---

# Lecture

## Lecture01

## Lecture02

```{python}
import pandas as pd
df = pd.read_csv("seattle_pet_licenses.csv")
df
```

Questions:How many pets are included in this dataset?
The answer: 66042
Questions:How many variables do we have for each pet?
The answer: 7 variables
```{python}
df.info()
df['animal_s_name'].value_counts().head(3)
```
Questions:What are the three most common pet names in Seattle? 
The answer: Lucy、Bella、Charlie


## Lecture03
### 1
```{python}
import pandas as pd
url ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-03/data/nobel.csv'
df = pd.read_csv(url)
print(df.head())
df
```
Questions:How many observations and how many variables are in the dataset? What does each row represent?
The answer: 935 observations, 26 variables, and one row for each person.
```{python}
df.info()
print(df)
nobel_living = df[
    (df['country'].notna()) &  
    (df['gender'] != 'org') &  
    (df['died_date'].isna())  
]
print(nobel_living)
```
Questions:Where were most Nobel laureates based when they won their prizes?
The answer: USA

### 2
```{python}
import pandas as pd
url ='https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
df = pd.read_csv(url)
print(df.head())
df
```
```{python}
df.info()
from skimpy import clean_columns
df = clean_columns(df,case="snake")
print(df.columns)
df.fillna("-")
df.describe()
sum_table = df.describe().round(2)
sum_table
df.dropna()
```


## Lectyre04
```{python}
import pandas as pd
df = pd.read_csv("all-ages.csv")
df
result = df.groupby(["Major"]).sum().sort_values(["Unemployment_rate"])
print(result)
```
```{python}
### 按照专业分组，并把失业率从低到高升序排列
import pandas as pd
df = pd.read_csv('recent-grads.csv')
df
```
result = df.groupby(["Major"]).sum().sort_values(["ShareWomen"],ascending=False)
print(result)
```{python}
### 按照专业分组，将女生占比从高到低降序排列
import pandas as pd
df = pd.read_csv('recent-grads.csv')
df
result = df.groupby(["Major"]).sum().sort_values(["ShareWomen"],ascending=False)
print(result)
```
```{python}
### 按照专业分组，将女生占比从高到低降序排列
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
a=df['Median'].groupby(df['Major_category']).sum()
a.plot.bar()
plt.show()
```
Questions:What should I major in?

The answer: Engineering


## Lectyre05
```{python}
import pandas as pd
from lets_plot import *
LetsPlot.setup_html()
```
```{python}
df = pd.read_csv('plastic-waste.csv')
df_clean = df.dropna(subset=['plastic_waste_per_cap', 'continent'])
```
Quenstion1:Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita?
The answer:With the histogram, it is possible to observe differences in the per capita distribution of plastic waste across continents. For example, Africa shows a higher peak waste output and North America shows a wider distribution.


```{python}
# Create histograms faceted by continent
p_histogram = ggplot(df_clean, aes(x='plastic_waste_per_cap')) + \
    geom_histogram(bins=30, fill='blue', color='black', alpha=0.7) + \
    facet_wrap('continent') + \
    ggtitle('Distribution of Plastic Waste per Capita by Continent') + \
    xlab('Plastic Waste per Capita') + \
    ylab('Frequency')


p_histogram.show()
```
Quenstion2:Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots?
The answer:Fiddle plots show the complete distribution of the data, showing the shape of the data and multiple peaks in the graph. Box plots provide explicit statistical information that violin plots only reflect through shape.


```{python}
# Violin plots
p_violin = ggplot(df, aes(x='continent', y='plastic_waste_per_cap', fill='continent')) + \
    geom_violin(alpha=0.7) + \
    geom_boxplot(width=0.1, fill='white', color='black') + \
    ggtitle('Violin Plot of Plastic Waste per Capita by Continent') + \
    xlab('Continent') + \
    ylab('Plastic Waste per Capita')

p_violin.show()
```
Quenstion3:Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship.
The answer:The scatter plot presents the relationship between plastic waste per capita and poorly managed waste per capita, checking if there is a positive or other relationship.

```{python}
# Scatterplot
if 'mismanaged_plastic_waste_per_cap' in df.columns:
    p_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap')) + \
        geom_point(size=3, alpha=0.6) + \
        ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita') + \
        xlab('Plastic Waste per Capita') + \
        ylab('Mismanaged Plastic Waste per Capita')
    
    p_scatter.show()
```
Quenstion4:Colour the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated?
The answer:By distinguishing continents by color, it is possible to observe differences between continents. Certain continents may exhibit specific patterns or clusters, such as the gradual rise of Africa.

```{python}
# Colored scatterplot
p_scatter_colored = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap', color='continent')) + \
    geom_point(size=3, alpha=0.6) + \
    ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita by Continent') + \
    xlab('Plastic Waste per Capita') + \
    ylab('Mismanaged Plastic Waste per Capita')

p_scatter_colored.show()
```
Quenstion5:Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated?
The answer:The visualization of the relationship between the two demographic variables shows the association between plastic waste per capita and total population and coastal population. Through scatterplot analysis, plastic waste per capita exhibits a stronger linear relationship with coastal population.

```{python}
# Plastic waste per capita vs Total population
p_pop_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='total_pop', color='continent')) + \
    geom_point(size=3, alpha=0.6) + \
    ggtitle('Plastic Waste per Capita vs. Total Population') + \
    xlab('Plastic Waste per Capita') + \
    ylab('Total Population')

p_pop_scatter.show()
```

```{python}
# Plastic waste per capita vs Coastal population
p_coastal_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='coastal_pop', color='continent')) + \
    geom_point(size=3, alpha=0.6) + \
    ggtitle('Plastic Waste per Capita vs. Coastal Population') + \
    xlab('Plastic Waste per Capita') + \
    ylab('Coastal Population')

p_coastal_scatter.show()
```
```{python}
p_coastal_scatter = ggplot(df, aes(x='coastal_pop', y='plastic_waste_per_cap', color='continent')) + \
    geom_point(size=3, alpha=0.6) + \
    ggtitle('Coastal Population vs. Plastic Waste per Capita') + \
    xlab('coastal pop') + \
    ylab('plastic waste per cap')

p_coastal_scatter.show()
```
```{python}
p_coastal_scatter.show()
df['coastal_population_proportion'] = df['coastal_pop'] / df['total_pop']
df_filtered = df
df_filtered = df_filtered[(df_filtered['plastic_waste_per_cap'] <= 0.6) & 
(df_filtered['coastal_population_proportion'] <= 1.6)]
p_scatter = ggplot(df_filtered, aes(x='coastal_population_proportion', y='plastic_waste_per_cap', color='continent')) + \
    geom_point(size=3, alpha=0.7) + \
    geom_smooth(method='lm', color='black', se=True, linetype='solid', size=1) + \
    ggtitle('Plastic Waste per Capita vs Coastal Population Proportion') + \
    xlab('Coastal Population Proportion') + \
    ylab('Plastic Waste per Capita')


p_scatter.show()
```

# Practical

## Practical01
```{python}
#%pip install pandas matplotlib numpy pathlib pingouin lets_plot
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
#import pingouin as pg
from lets_plot import *

LetsPlot.setup_html(no_js=True)

### You don't need to use these settings yourself,
### they are just here to make the charts look nicer!
# Set the plot style for prettier charts:
plt.style.use(
    "https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt"
)
```
Questions:Explain in your own words what temperature ‘anomalies’ are. Why have researchers chosen this particular measure over other measures (such as absolute temperature)?
The answer:
(1)A temperature "anomaly" is typically defined as a temperature that falls outside the normal range or expected value, suggesting the potential for an underlying issue or change. In contrast, absolute temperature represents the average kinetic energy of molecular motion and is expressed in Kelvins (K).
(2)Researchers select temperature "anomalies" over absolute temperatures due to their greater practicality, intuitiveness, ease of comprehension, suitability for comparison with normal conditions, and adaptability to diverse research fields and objectives.
```{python}
df = pd.read_csv("NH.Ts+dSST.csv", skiprows=1)
df.head()
```
```{python}
df.info()
```
```{python}
df = pd.read_csv("NH.Ts+dSST.csv", skiprows=1)
df.info()
```
```{python}
print(df.head())
```
Questions:Try importing the data again without using the keyword argument option na_values="***" at all and see what difference it makes.
The answer: By comparing the output of.info(), you can see the difference in importing the data without parameters. Some columns may be recognized as object instead of float64 or int64, which usually means they contain non-numeric characters.
```{python}
df = pd.read_csv("NH.Ts+dSST.csv", skiprows=1)
df.head()
```
```{python}
df.info()
```
```{python}
df = df.set_index("Year")
df.head()
```
```{python}
df.tail()
```
```{python}
fig, ax = plt.subplots()
df["Jan"].plot(ax=ax)
ax.set_ylabel("y label")
ax.set_xlabel("x label")
ax.set_title("title")
plt.show()
```
```{python}
fig, ax = plt.subplots()
ax.plot(df.index, df["Jan"])
ax.set_ylabel("y label")
ax.set_xlabel("x label")
ax.set_title("title")
plt.show()
```
```{python}
plt.show()
plt.savefig("name-of-chart.pdf")
month = "Jan"
fig, ax = plt.subplots()
ax.axhline(0, color="orange")
ax.annotate("1951—1980 average", xy=(0.66, -0.2), xycoords=("figure fraction", "data"))
df[month].plot(ax=ax)
ax.set_title(
    f"Average temperature anomaly in {month} \n in the northern hemisphere (1880—{df.index.max()})"
)
ax.set_ylabel("Annual temperature anomalies")
```
Extra practice:Extra practice: The columns labelled DJF, MAM, JJA, and SON contain seasonal averages (means). For example, the MAM column contains the average of the March, April, and May columns for each year. Plot a separate line chart for each season, using average temperature anomaly for that season on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis.
The answer:
```{python}
month = "Jan"
fig, ax = plt.subplots()
ax.axhline(0, color="red")
ax.annotate("1951—1980 average",  xy=(1.96, -1.5),xycoords=("figure fraction", "data"))
df[month].plot(ax=ax)
ax.set_title(
    f"Average temperature anomaly in {month} \n in the northern hemisphere (1880—{df.index.max()})"
)
ax.set_ylabel("Annual temperature anomalies")
```
```{python}
month = "MAM"
fig, ax = plt.subplots()
ax.axhline(0, color="orange")
ax.annotate("1951—1980 average", xy=(0.66, -0.2), xycoords=("figure fraction", "data"))
df[month].plot(ax=ax)
ax.set_title(
    f"Average temperature anomaly in {month} \n in the northern hemisphere (1880—{df.index.max()})"
)
ax.set_ylabel("Annual temperature anomalies")
```
```{python}
month = "JJA"
fig, ax = plt.subplots()
ax.axhline(0, color="orange")
ax.annotate("1951—1980 average", xy=(0.66, -0.2), xycoords=("figure fraction", "data"))
df[month].plot(ax=ax)
ax.set_title(
    f"Average temperature anomaly in {month} \n in the northern hemisphere (1880—{df.index.max()})"
)
ax.set_ylabel("Annual temperature anomalies")
```
Questions:What do your charts from Questions 2 to 4(a) suggest about the relationship between temperature and time?
The answer: Temperature increases with time.
```{python}
month = "J-D"
fig, ax = plt.subplots()
ax.axhline(0, color="orange")
ax.annotate("1951-1980 average", xy=(0.68, -0.2), xycoords=("figure fraction", "data"))
df[month].plot(ax=ax)
ax.set_title(
    f"Average annual temperature anomaly in \n in the northern hemisphere (1880-{df.index.max()})"
)
ax.set_ylabel("Annual temperature anomalies")
```
Questions:Discuss the similarities and differences between the charts. (For example, are the horizontal and vertical axes variables the same, or do the lines have the same shape?)
The answer:
(1)Similaritie:The temperature changes with time, and the overall trend is upward.
(2)Differences:The horizontal and vertical axis variables of the two tables are different. For example, in 2000, Figure 1.4 shows a temperature of 0.5 and Figure 1.5 shows a temperature of 0.6.
Questions:Looking at the behaviour of temperature over time from 1000 to 1900 in Figure 1.4, are the observed patterns in your chart unusual?
The answer:From 1000 to 1900, the temperature fluctuate up and down with time, but the maximum value did not exceed 0.0, which indicates that the temperature change during this period was relatively small compared to the significant increase in industrial temperature after 1900, which I think is normal.

Questions:Based on your answers to Questions 4 and 5, do you think the government should be concerned about climate change?
The answer:My point of view is that according to the chart data, the temperature is rising, which is a sign of global warming, so the government should pay attention to climate change.
```{python}
df["Period"] = pd.cut(
    df.index,
    bins=[1921, 1950, 1980, 2010],
    labels=["1921—1950", "1951—1980", "1981—2010"],
    ordered=True,
)
df["Period"].tail(20)
```
```{python}
list_of_months = ["Jun", "Jul", "Aug"]
df[list_of_months].stack().head()
```
```{python}
fig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)
for ax, period in zip(axes, df["Period"].dropna().unique()):
    df.loc[df["Period"] == period, list_of_months].stack().hist(ax=ax)
    ax.set_title(period)
plt.suptitle("Histogram of temperature anomalies")
axes[1].set_xlabel("Summer temperature distribution")
plt.tight_layout()
```
```{python}
# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)
temp_all_months = df.loc[(df.index >= 1951) & (df.index <= 1980), "Jan":"Dec"]
# Put all the data in stacked format and give the new columns sensible names
temp_all_months = (
    temp_all_months.stack()
    .reset_index()
    .rename(columns={"level_1": "month", 0: "values"})
)
# Take a look at this data:
temp_all_months
```
```{python}
quantiles = [0.3, 0.7]
list_of_percentiles = np.quantile(temp_all_months["values"], q=quantiles)

print(f"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}")
print(f"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}")
```
```{python}
# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)
temp_all_months = df.loc[(df.index >= 1981) & (df.index <= 2010), "Jan":"Dec"]
# Put all the data in stacked format and give the new columns sensible names
temp_all_months = (
    temp_all_months.stack()
    .reset_index()
    .rename(columns={"level_1": "month", 0: "values"})
)
# Take a look at the start of this data data:
temp_all_months.head()
```
```{python}
entries_less_than_q30 = temp_all_months["values"] < list_of_percentiles[0]
proportion_under_q30 = entries_less_than_q30.mean()
print(
    f"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%"
)
```
```{python}
proportion_over_q70 = (temp_all_months["values"] > list_of_percentiles[1]).mean()
print(f"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%")
```
```{python}
temp_all_months = (
    df.loc[:, "DJF":"SON"]
    .stack()
    .reset_index()
    .rename(columns={"level_1": "Season", 0: "Values"})
)
temp_all_months["Period"] = pd.cut(
    temp_all_months["Year"],
    bins=[1921, 1950, 1980, 2010],
    labels=["1921—1950", "1951—1980", "1981—2010"],
    ordered=True,
)
# Take a look at a cut of the data using `.iloc`, which provides position
temp_all_months.iloc[-135:-125]
```
Questions:Calculate the mean (average) and variance separately for the following time periods: 1921–1950, 1951–1980, and 1981–2010.
The answer:The variance of the later period is significantly higher than that of the earlier period, which indicates that the air temperature becomes more variable.
```{python}
seasons = {
    "DJF": ["Dec", "Jan", "Feb"],
    "MAM": ["Mar", "Apr", "May"],
    "JJA": ["Jun", "Jul", "Aug"],
    "SON": ["Sep", "Oct", "Nov"]
}
for season, months in seasons.items():
    if all(month in df.columns for month in months):
        df[season] = df[months].mean(axis=1)
periods = {
    "1921-1950": (1921, 1950),
    "1951-1980": (1951, 1980),
    "1981-2010": (1981, 2010)
}
results = {}
for season in seasons.keys():
    if season in df.columns:
        results[season] = {}
        for period, (start_year, end_year) in periods.items():
            period_data = df.loc[start_year:end_year, season]
            results[season][period] = {
                "mean": period_data.mean(),
                "variance": period_data.var(),
            }
for season, period_results in results.items():
    print(f"Season: {season}")
    for period, stats in period_results.items():
        print(f"  Period: {period}")
        print(f"    Mean: {stats['mean']:.2f}")
        print(f"    Variance: {stats['variance']:.2f}")
    print()
```
```{python}
temp_all_months = (
    df.loc[:, "DJF":"SON"]
    .stack()
    .reset_index()
    .rename(columns={"level_1": "Season", 0: "Values"})
)
temp_all_months["Period"] = pd.cut(
    temp_all_months["Year"],
    bins=[1921, 1950, 1980, 2010],
    labels=["1921—1950", "1951—1980", "1981—2010"],
    ordered=True,
)
# Take a look at a cut of the data using `.iloc`, which provides position
temp_all_months.iloc[-135:-125]
```
```{python}
grp_mean_var = temp_all_months.groupby(["Season", "Period"])["Values"].agg(
    [np.mean, np.var]
)
grp_mean_var
```
```{python}
min_year = 1880
(
    ggplot(temp_all_months, aes(x="Year", y="Values", color="Season"))
    + geom_abline(slope=0, color="black", size=1)
    + geom_line(size=1)
    + labs(
        title=f"Average annual temperature anomaly in \n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})",
        y="Annual temperature anomalies",
    )
    + scale_x_continuous(format="d")
    + geom_text(
        x=min_year, y=0.1, label="1951—1980 average", hjust="left", color="black"
    )
)
```
Questions:Using the findings of the New York Times article and your answers to Questions 1 to 5, discuss whether temperature appears to be more variable over time. Would you advise the government to spend more money on mitigating the effects of extreme weather events?
The answer:As temperatures change more over time due to global warming, heat extremes are becoming more frequent and damaging. First, I suggest that the government should spend more money to alleviate the impact of extreme weather events on people's lives, such as expanding the urban green area. Second, we will advocate low-carbon travel and life for the people.
```{python}
df_co2 = pd.read_csv(r"1_C02-data.csv")
df_co2.head()
```
```{python}
df_co2_june = df_co2.loc[df_co2["Month"] == 6]
df_co2_june.head()
```
```{python}
df_temp_co2 = pd.merge(df_co2_june, df, on="Year")
df_temp_co2[["Year", "Jun", "Trend"]].head()
```
```{python}
(
    ggplot(df_temp_co2, aes(x="Jun", y="Trend"))
    + geom_point(color="black", size=3)
    + labs(
        title="Scatterplot of temperature anomalies vs carbon dioxide emissions",
        y="Carbon dioxide levels (trend, mole fraction)",
        x="Temperature anomaly (degrees Celsius)",
    )
)
```
```{python}
df_temp_co2[["Jun", "Trend"]].corr(method="pearson")
```
```{python}
(
    ggplot(df_temp_co2, aes(x="Year", y="Jun"))
    + geom_line(size=1)
    + labs(
        title="June temperature anomalies",
    )
    + scale_x_continuous(format="d")
)
```
```{python}
base_plot = ggplot(df_temp_co2) + scale_x_continuous(format="d")
plot_p = (
    base_plot
    + geom_line(aes(x="Year", y="Jun"), size=1)
    + labs(title="June temperature anomalies")
)
plot_q = (
    base_plot
    + geom_line(aes(x="Year", y="Trend"), size=1)
    + labs(title="Carbon dioxide emissions")
)
gggrid([plot_p, plot_q], ncol=2)
```
Extra practice: Choose two months and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Create a separate chart for each month. 
The answer:
```{python}
df_co2_june = df_co2.loc[df_co2["Month"] == 3]
df_co2_june.head()
```
```{python}
df_temp_co2 = pd.merge(df_co2_june, df, on="Year")
df_temp_co2[["Year", "Mar", "Trend"]].head()
```
```{python}
(
    ggplot(df_temp_co2, aes(x="Mar", y="Trend"))
    + geom_point(color="red", size=3)
    + labs(
        title="Scatterplot of temperature anomalies vs carbon dioxide emissions",
        y="Carbon dioxide levels (trend, mole fraction)",
        x="Temperature anomaly (degrees Celsius)",
    )
)
```
```{python}
df_co2_june = df_co2.loc[df_co2["Month"] == 9]
df_co2_june.head()
```
```{python}
df_temp_co2 = pd.merge(df_co2_june, df, on="Year")
df_temp_co2[["Year", "Sep", "Trend"]].head()
```
```{python}
(
    ggplot(df_temp_co2, aes(x="Sep", y="Trend"))
    + geom_point(color="blue", size=3)
    + labs(
        title="Scatterplot of temperature anomalies vs carbon dioxide emissions",
        y="Carbon dioxide levels (trend, mole fraction)",
        x="Temperature anomaly (degrees Celsius)",
    )
)
```
Questions:What do your charts and the correlation coefficients suggest about the relationship between CO2 levels and temperature anomalies?

The answer:CO2 levels and temperature have strongly correlated with each other.
Questions:Consider the example of spurious correlation described above.
Questions:(1)In your own words, explain spurious correlation and the difference between correlation and causation.

The answer:Spurious correlation: When two things seem linked but aren't really, often due to a hidden factor.Correlation vs causation: Correlation shows a link, but causation means one thing causes another.

Questions:(2)Give an example of spurious correlation, similar to the one above, for either CO2 levels or temperature anomalies.

The answer:
Example: CO2 Levels and Stock Market Performance.It might seem that there's a correlation between rising CO2 levels in the atmosphere and improved stock market performance. However, this doesn't mean that CO2 levels are directly causing the stock market to rise. Instead, both could be influenced by a common factor, such as economic growth. As economies grow, they often emit more CO2 and also tend to have better stock market performance.

Questions:(3)Choose an example of spurious correlation from Tyler Vigen’s website. Explain whether you think it is a coincidence, or whether this correlation could be due to one or more other variables.

The answer:An example is the correlation between the number of Nicolas Cage films released in a year and the number of people who die by falling into swimming pools.Is it a coincidence?Yes, it is likely a coincidence.Could it be due to one or more other variables?It could be due to the fact that both of these events are influenced by broader societal trends or random fluctuations that are not directly related to each other. For instance, the number of Nicolas Cage films released might be influenced by the film industry's production schedule, while the number of swimming pool accidents could be influenced by factors such as weather conditions, safety regulations, and public awareness. There is no plausible mechanism through which the release of Nicolas Cage films could cause an increase in swimming pool accidents, or vice versa. Therefore, it is reasonable to conclude that this correlation is spurious and due to chance or other unobserved variables.

## Practical02
```{python}
#%pip install openpyxl
import pandas as pd
data_np = pd.read_excel(
    r"doing-economics-datafile-working-in-excel-project-2 (1).xlsx",
    usecols="A:Q",
    header=1,
    index_col="Period",
)
data_n = data_np.iloc[:10, :].copy()
data_p = data_np.iloc[14:24, :].copy()
data_n.info()
```
```{python}
data_n = data_n.astype("double")
data_p = data_p.astype("double")
```
Quenstion:(a)Calculate the mean contribution in each period (row) separately for both experiments.

(b)Plot a line chart of mean contribution on the vertical axis and time period (from 1 to 10) on the horizontal axis (with a separate line for each experiment). Make sure the lines in the legend are clearly labelled according to the experiment (with punishment or without punishment).

(c)Describe any differences and similarities you see in the mean contribution over time in both experiments.
The anwser:In the two experiments, the average contributions over time changed. The difference was that the average value without penalty gradually increased and was always higher than the average value with penalty, and the average value without penalty gradually decreased with the development of time.
```{python}
import numpy as np

mean_n_c = data_n.mean(axis=1)
mean_p_c = data_p.agg(np.mean, axis=1)
import numpy as np
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
mean_n_c.plot(ax=ax, label="Without punishment")
mean_p_c.plot(ax=ax, label="With punishment")
ax.set_title("Average contributions to the public goods game")
ax.set_ylabel("Average contribution")
ax.legend()
```
```{python}
# Create new dataframe with bars in
compare_grps = pd.DataFrame(
    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],
    index=["Without punishment", "With punishment"],
)
# Rename columns to have 'round' in them
compare_grps.columns = ["Round " + str(i) for i in compare_grps.columns]
# Swap the column and index variables around with the transpose function, ready for plotting (.T is transpose)
compare_grps = compare_grps.T
# Make a bar chart
compare_grps.plot.bar(rot=0)
```
```{python}
n_c = data_n.agg(["std", "var", "mean"], 1)
n_c
```
```{python}
p_c = data_p.agg(["std", "var", "mean"], 1)
fig, ax = plt.subplots()
n_c["mean"].plot(ax=ax, label="mean")

(n_c["mean"] + 2 * n_c["std"]).plot(ax=ax, ylim=(0, None), color="red", label="±2 s.d.")

(n_c["mean"] - 2 * n_c["std"]).plot(ax=ax, ylim=(0, None), color="red", label="")
for i in range(len(data_n.columns)):
    ax.scatter(x=data_n.index, y=data_n.iloc[:, i], color="k", alpha=0.3)
ax.legend()
ax.set_ylabel("Average contribution")
ax.set_title("Contribution to public goods game without punishment")
plt.show()
```
```{python}
fig, ax = plt.subplots()
p_c["mean"].plot(ax=ax, label="mean")
# mean + 2 sd
(p_c["mean"] + 2 * p_c["std"]).plot(ax=ax, ylim=(0, None), color="red", label="±2 s.d.")
# mean - 2 sd
(p_c["mean"] - 2 * p_c["std"]).plot(ax=ax, ylim=(0, None), color="red", label="")
for i in range(len(data_p.columns)):
    ax.scatter(x=data_p.index, y=data_p.iloc[:, i], color="k", alpha=0.3)
ax.legend()
ax.set_ylabel("Average contribution")
ax.set_title("Contribution to public goods game with punishment")
plt.show()
```
```{python}
data_p.apply(lambda x: x.max() - x.min(), axis=1)
```
```{python}
# A lambda function accepting three inputs, a, b, and c, and calculating the sum of the squares
test_function = lambda a, b, c: a**2 + b**2 + c**2


# Now we apply the function by handing over (in parenthesis) the following inputs: a=3, b=4 and c=5
test_function(3, 4, 5)
```
```{python}
range_function = lambda x: x.max() - x.min()
range_p = data_p.apply(range_function, axis=1)
range_n = data_n.apply(range_function, axis=1)
fig, ax = plt.subplots()
range_p.plot(ax=ax, label="With punishment")
range_n.plot(ax=ax, label="Without punishment")
ax.set_ylim(0, None)
ax.legend()
ax.set_title("Range of contributions to the public goods game")
plt.show()
```
```{python}
funcs_to_apply = [range_function, "max", "min", "std", "mean"]
summ_p = data_p.apply(funcs_to_apply, axis=1).rename(columns={"<lambda>": "range"})
summ_n = data_n.apply(funcs_to_apply, axis=1).rename(columns={"<lambda>": "range"})
summ_n.loc[[1, 10], :].round(2)
```
```{python}
summ_p.loc[[1, 10], :].round(2)
```
```{python}
import pingouin as pg

pg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :])
```
```{python}
pg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :], paired=True)
```

## Practical03

### Practical03-01
```{python}
import matplotlib.pyplot as plt
import pandas as pd
import requests
from bs4 import BeautifulSoup
import textwrap
pd.read_csv(
    "https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv", nrows=10
)
```
```{python}
url = "http://aeturrell.com/research"
page = requests.get(url)
page.text[:300]
```
```{python}
soup = BeautifulSoup(page.text, "html.parser")
print(soup.prettify()[60000:60500])
```
```{python}
# Get all paragraphs
all_paras = soup.find_all("p")
# Just show one of the paras
all_paras[1]
```
```{python}
all_paras[1].text
```
```{python}
projects = soup.find_all("div", class_="project-content listing-pub-info")
projects = [x.text.strip() for x in projects]
projects[:4]
```
```{python}
df_list = pd.read_html(
    "https://simple.wikipedia.org/wiki/FIFA_World_Cup", match="Sweden"
)
# Retrieve first and only entry from list of dataframes
df = df_list[0]
df.head()
```

### Practical03-02
```{python}
#pip install requests
#pip install html5lib
#pip install bs4
#pip install pandas
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd
# Downloading imdb top 250 movie's data
url = 'http://www.imdb.com/chart/top'
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")
movies = soup.select('td.titleColumn')
crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]
ratings = [b.attrs.get('data-value')
		for b in soup.select('td.posterColumn span[name=ir]')]
# create a empty list for storing
# movie information
list = []

# Iterating over movies to extract
# each movie's details
for index in range(0, len(movies)):
	
	# Separating movie into: 'place',
	# 'title', 'year'
	movie_string = movies[index].get_text()
	movie = (' '.join(movie_string.split()).replace('.', ''))
	movie_title = movie[len(str(index))+1:-7]
	year = re.search('\((.*?)\)', movie_string).group(1)
	place = movie[:len(str(index))-(len(movie))]
	data = {"place": place,
			"movie_title": movie_title,
			"rating": ratings[index],
			"year": year,
			"star_cast": crew[index],
			}
	list.append(data)
for movie in list:
	print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +
		') -', 'Starring:', movie['star_cast'], movie['rating'])
#saving the list as dataframe
#then converting into .csv file
df = pd.DataFrame(list)
df.to_csv('imdb_top_250_movies.csv',index=False)
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd


# Downloading imdb top 250 movie's data
url = 'http://www.imdb.com/chart/top'
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")
movies = soup.select('td.titleColumn')
crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]
ratings = [b.attrs.get('data-value')
		for b in soup.select('td.posterColumn span[name=ir]')]




# create a empty list for storing
# movie information
list = []

# Iterating over movies to extract
# each movie's details
for index in range(0, len(movies)):
	
	# Separating movie into: 'place',
	# 'title', 'year'
	movie_string = movies[index].get_text()
	movie = (' '.join(movie_string.split()).replace('.', ''))
	movie_title = movie[len(str(index))+1:-7]
	year = re.search('\((.*?)\)', movie_string).group(1)
	place = movie[:len(str(index))-(len(movie))]
	data = {"place": place,
			"movie_title": movie_title,
			"rating": ratings[index],
			"year": year,
			"star_cast": crew[index],
			}
	list.append(data)

# printing movie details with its rating.
for movie in list:
	print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +
		') -', 'Starring:', movie['star_cast'], movie['rating'])


##.......##
df = pd.DataFrame(list)
df.to_csv('imdb_top_250_movies.csv',index=False)
```

### Practical03-03
```{python}
import requests
from bs4 import BeautifulSoup
import csv
 
# 定义请求的 URL 和 headers
url = "https://movie.douban.com/top250"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
 
# 发送 GET 请求
response = requests.get(url, headers=headers)
response.encoding = 'utf-8'  # 设置编码方式
html_content = response.text  # 获取网页的 HTML 内容
 
# 使用 Beautiful Soup 解析 HTML
soup = BeautifulSoup(html_content, 'html.parser')
 
# 提取电影名称、描述、评分和评价人数
movies = []
for item in soup.find_all('div', class_='item'):
    title = item.find('span', class_='title').get_text()  # 电影名称
    description = item.find('span', class_='inq')  # 电影描述
    rating = item.find('span', class_='rating_num').get_text()  # 评分
    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数
    
    # 如果没有描述，将其置为空字符串
    if description:
        description = description.get_text()
    else:
        description = ''
    
    movie = {
        "title": title,
        "description": description,
        "rating": rating,
        "votes": votes.replace('人评价', '').strip()
    }
    movies.append(movie)
 
# 将数据保存到 CSV 文件
with open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ['title', 'description', 'rating', 'votes']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
 
    writer.writeheader()  # 写入表头
    for movie in movies:
        writer.writerow(movie)  # 写入每一行数据
 
print("save success douban_top250.csv")
```

## Practical04
```{python} echo=False
from bs4 import BeautifulSoup
import re  
import urllib.request, urllib.error  # certain URL
import xlwt  # excel operation
 
 
def main():
    baseurl = "https://movie.douban.com/top250?start="
    datalist = getdata(baseurl)
    savepath = "douban_top250.csv"
    savedata(datalist, savepath)
 
 
# compile返回的是匹配到的模式对象
findLink = re.compile(r'<a href="(.*?)">')  # detail
findImgSrc = re.compile(r'<img.*src="(.*?)"', re.S)  # re.S  message of picture
findTitle = re.compile(r'<span class="title">(.*)</span>')  # name 
findRating = re.compile(r'<span class="rating_num" property="v:average">(.*)</span>')  # score
findJudge = re.compile(r'<span>(\d*)人评价</span>')  # number
findInq = re.compile(r'<span class="inq">(.*)</span>')  # about
findBd = re.compile(r'<p class="">(.*?)</p>', re.S)  # actor..
 
 
##获取网页数据
def getdata(baseurl):
    datalist = []
    for i in range(0, 10):
        url = baseurl + str(i * 25)  ##move on next page
        html = geturl(url)
        soup = BeautifulSoup(html, "html.parser")  #  BeautifulSoup soup，html
        for item in soup.find_all("div", class_='item'):  ##find_all 
            data = []  # save HTML 
            item = str(item)  ##trans
            link = re.findall(findLink, item)[0]  
            data.append(link)
 
            imgSrc = re.findall(findImgSrc, item)[0]
            data.append(imgSrc)
 
            titles = re.findall(findTitle, item)  ##en zh transla
            if (len(titles) == 2):
                onetitle = titles[0]
                data.append(onetitle)
                twotitle = titles[1].replace("/", "")  # can
                data.append(twotitle)
            else:
                data.append(titles)
                data.append(" ")  ##value
 
            rating = re.findall(findRating, item)[0]  # add score
            data.append(rating)
 
            judgeNum = re.findall(findJudge, item)[0]  # add number
            data.append(judgeNum)
 
            inq = re.findall(findInq, item)  # add abut
            if len(inq) != 0:
                inq = inq[0].replace("。", "")
                data.append(inq)
            else:
                data.append(" ")
 
            bd = re.findall(findBd, item)[0]
            bd = re.sub('<br(\s+)?/>(\s+)?', " ", bd)
            bd = re.sub('/', " ", bd)
            data.append(bd.strip())  # cancel
            datalist.append(data)
    return datalist
 
 
##保存数据
def savedata(datalist, savepath):
    workbook = xlwt.Workbook(encoding="utf-8", style_compression=0)  ##style_compression=0
    worksheet = workbook.add_sheet("douban_top250", cell_overwrite_ok=True)  # cell_overwrite_ok=True
    column = ("电影详情链接", "图片链接", "影片中文名", "影片外国名", "评分", "评价数", "概况", "相关信息")  ##execl
    for i in range(0, 8):
        worksheet.write(0, i, column[i])  # 将column[i] save [0]
    for i in range(0, 250):
        data = datalist[i]
        for j in range(0, 8):
            worksheet.write(i + 1, j, data[j])
    workbook.save(savepath)
 
 
##爬取网页
def geturl(url):
    head = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36"
    }
    req = urllib.request.Request(url, headers=head)
    try:  ## check error
        response = urllib.request.urlopen(req)
        html = response.read().decode("utf-8")
    except urllib.error.URLError as e:
        if hasattr(e, "code"):  
            print(e.code)
        if hasattr(e, "reason"):
            print(e.reason)
    return html
 
 
if __name__ == '__main__':
    main()
    print("爬取成功！！！")
```
```{python} echo=False
from bs4 import BeautifulSoup
import re
import urllib.request, urllib.error  # for URL requests
import csv  # for saving as CSV


def main():
    baseurl = "https://movie.douban.com/top250?start="
    datalist = getdata(baseurl)
    savepath = "douban_top250.csv"
    savedata(datalist, savepath)


# Regular expressions to extract information
findLink = re.compile(r'<a href="(.*?)">')  # detail link
findImgSrc = re.compile(r'<img.*src="(.*?)"', re.S)  # image link
findTitle = re.compile(r'<span class="title">(.*)</span>')  # movie title
findRating = re.compile(r'<span class="rating_num" property="v:average">(.*)</span>')  # rating
findJudge = re.compile(r'<span>(\d*)人评价</span>')  # number of reviews
findInq = re.compile(r'<span class="inq">(.*)</span>')  # summary
findBd = re.compile(r'<p class="">(.*?)</p>', re.S)  # additional info


# Function to get data from the website
def getdata(baseurl):
    datalist = []
    for i in range(0, 10):
        url = baseurl + str(i * 25)  # Go to the next page
        html = geturl(url)
        soup = BeautifulSoup(html, "html.parser")
        for item in soup.find_all("div", class_='item'):  # Extract movie items
            data = []  # Save movie data
            item = str(item)  # Convert to string for regex
            link = re.findall(findLink, item)[0]  # Detail link
            data.append(link)

            imgSrc = re.findall(findImgSrc, item)[0]  # Image link
            data.append(imgSrc)

            titles = re.findall(findTitle, item)  # Titles (CN and foreign)
            if len(titles) == 2:
                data.append(titles[0])  # Chinese title
                data.append(titles[1].replace("/", "").strip())  # Foreign title
            else:
                data.append(titles[0])  # Only Chinese title
                data.append(" ")  # Empty for foreign title

            rating = re.findall(findRating, item)[0]  # Rating
            data.append(rating)

            judgeNum = re.findall(findJudge, item)[0]  # Number of reviews
            data.append(judgeNum)

            inq = re.findall(findInq, item)  # Summary
            if len(inq) != 0:
                data.append(inq[0].replace("。", ""))
            else:
                data.append(" ")

            bd = re.findall(findBd, item)[0]  # Additional info
            bd = re.sub('<br(\s+)?/>(\s+)?', " ", bd)  # Replace line breaks
            bd = re.sub('/', " ", bd)  # Replace slashes
            data.append(bd.strip())

            datalist.append(data)
    return datalist


# Function to save data to a CSV file
def savedata(datalist, savepath):
    headers = ["电影详情链接", "图片链接", "影片中文名", "影片外国名", "评分", "评价数", "概况", "相关信息"]
    with open(savepath, mode='w', encoding='utf-8', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(headers)  # Write headers
        for data in datalist:
            writer.writerow(data)  # Write each movie's data


# Function to get HTML content from a URL
def geturl(url):
    head = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36"
    }
    req = urllib.request.Request(url, headers=head)
    try:
        response = urllib.request.urlopen(req)
        html = response.read().decode("utf-8")
    except urllib.error.URLError as e:
        if hasattr(e, "code"):
            print(e.code)
        if hasattr(e, "reason"):
            print(e.reason)
        return ""
    return html


if __name__ == '__main__':
    main()
    print("爬取成功并保存为CSV文件！")
```
```{python} echo=False
import pandas as pd
import matplotlib.pyplot as plt

# Load datasets
douban_file_path = 'douban_top250.csv'  
imdb_file_path = 'IMDB_Top250.csv'      

douban_data = pd.read_csv(douban_file_path, encoding='utf-8', on_bad_lines='skip')
imdb_data = pd.read_csv(imdb_file_path, encoding='utf-8', on_bad_lines='skip')

# Renaming columns for clarity and merging compatibility
douban_data.rename(columns={
    '影片中文名': 'Title',
    '评分': 'Douban_Score',
    '评价数': 'Douban_Reviews',
    '相关信息': 'Douban_Info'
}, inplace=True)

imdb_data.rename(columns={
    'Name': 'Title',
    'Year': 'Release_Year',
    'IMDB Ranking': 'IMDB_Score',
    'Genre': 'IMDB_Genre',
    'Director': 'IMDB_Director'
}, inplace=True)

# Calculate average scores for both platforms
douban_avg_score = douban_data['Douban_Score'].mean()
imdb_avg_score = imdb_data['IMDB_Score'].mean()

# Find overlapping movies by title
overlap_movies = pd.merge(douban_data, imdb_data, on='Title')

# Visualize average scores
plt.figure(figsize=(8, 5))
plt.bar(['Douban', 'IMDb'], [douban_avg_score, imdb_avg_score], alpha=0.7)
plt.title('Average Scores: Douban vs IMDb')
plt.ylabel('Average Score')
plt.show()

# Analyze release year distribution
plt.figure(figsize=(10, 5))
douban_data['Douban_Info'] = douban_data['Douban_Info'].astype(str)
douban_years = douban_data['Douban_Info'].str.extract(r'(\d{4})').dropna()
douban_years = douban_years[0].astype(int).value_counts().sort_index()

imdb_years = imdb_data['Release_Year'].value_counts().sort_index()

douban_years.plot(kind='bar', alpha=0.7, label='Douban', figsize=(10, 5))
imdb_years.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')
plt.title('Release Year Distribution')
plt.xlabel('Year')
plt.ylabel('Number of Movies')
plt.legend()
plt.show()

# Analyze genre distribution
imdb_genres = imdb_data['IMDB_Genre'].str.split(',').explode().str.strip().value_counts()
plt.figure(figsize=(10, 5))
imdb_genres.head(10).plot(kind='bar', alpha=0.7, color='orange')
plt.title('Top 10 IMDb Genres')
plt.xlabel('Genre')
plt.ylabel('Count')
plt.show()

# Top directors by movie count
douban_directors = douban_data['Douban_Info'].str.extract(r'导演: (.+?) ').dropna()
douban_top_directors = douban_directors[0].value_counts().head(10)

imdb_top_directors = imdb_data['IMDB_Director'].value_counts().head(10)

plt.figure(figsize=(10, 5))
douban_top_directors.plot(kind='bar', alpha=0.7, label='Douban', color='blue')
plt.title('Top 10 Douban Directors')
plt.xlabel('Director')
plt.ylabel('Movie Count')
plt.show()

plt.figure(figsize=(10, 5))
imdb_top_directors.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')
plt.title('Top 10 IMDb Directors')
plt.xlabel('Director')
plt.ylabel('Movie Count')
plt.show()

# Save overlapping movies to a CSV file
overlap_movies.to_csv('overlap_movies.csv', index=False)

# Print results
print(f"豆瓣平均评分: {douban_avg_score}")
print(f"IMDb平均评分: {imdb_avg_score}")
print(f"重叠电影数量: {len(overlap_movies)}")

```