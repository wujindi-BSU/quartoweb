[
  {
    "objectID": "demo2.html",
    "href": "demo2.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "demo",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis,see ?@fig-polar.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0,2,0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n    subplot_kw = {'projection':'polar'}\n)\nax.plot(theta,r)\nax.set_rticks([0.5,1,1.5,2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "Index.html",
    "href": "Index.html",
    "title": "wujindi’s website and data analysis portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio.\n\nHere, I’ll feature my projects for the Fall 2024 Modern Applied Data Analysis class\nPlease use the Menu Bar above to look around."
  },
  {
    "objectID": "Index.html#hello-and-thanks-for-visiting",
    "href": "Index.html#hello-and-thanks-for-visiting",
    "title": "wujindi’s website and data analysis portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio.\n\nHere, I’ll feature my projects for the Fall 2024 Modern Applied Data Analysis class\nPlease use the Menu Bar above to look around."
  },
  {
    "objectID": "abouteme.html",
    "href": "abouteme.html",
    "title": "About me",
    "section": "",
    "text": "Hi,I’m Wu jindi\n\n\n\nmypicture\n\n\n\nMy Background\nAs a first-year graduate student of Educational Management at Belarusian State University, my academic journey has been deeply influenced by my prior experience as a psychology teacher, where I developed a profound passion for the field of education.My research interests are centred on the application of positive psychology in educational management, with a particular focus on innovative practices that promote positive emotions, positive organisations and positive environments.\nI believe that, by employing the principles of positive psychology, we can enhance our understanding of and improve educational environments, thereby facilitating the optimal development of students’ potential and well-being.\nIn addition to my academic pursuits at GBS, I have been actively engaged in voluntary work for the past six years, primarily in the domains of teaching and tutoring during the winter and summer vacation periods. This engagement has not only facilitated the practical application of my psychological knowledge but also enriched my understanding of the importance of positive psychology in educational management.\nIt has become evident that my interests lie more in research and teaching, and the experience I have gained from volunteering reinforces the importance of positive psychology in shaping a positive managerial and social atmosphere. Consequently, my current goal is to complete my graduate studies, continue my doctoral studies, and eventually pursue a career as a college teacher.\n\n\nExperience\nThe focus of my own independent research is the role and innovation of positive psychology in educational management with a view to enhancing teacher well-being, formulating a more stable “psychological contract” and reducing staff turnover. The research revolves around positive environments, positive organisations and positive emotions. It was this project that prompted me to use R to perform some secondary data analysis. Thus far, I have used R to create simple graphs such as scatter plots, box plots and bar graphs.\n\n\nCourse Goals\nMy experience in R has revolved around accomplishing a few very specific tasks, and my “workflow” usually involves a lot of Googling. My goals for this class are to:\n\nCome away with the knowledge and skills to approach a wide set of problems and tasks using R\nPractice a clean and organized method of performing data analyses\nimprove the data analysis of my research project on positive psychology in education management in Chinese universities and create aesthetically pleasing, publishable data\nUnderstand the mechanisms of action and influencing factors of positive psychology, and better understand how the raw data was eventually transformed into experimental data for my master’s thesis.\nGet familiar with predictive modeling/machine learning\n\n\n\nHobbies & Fun Facts\nI am a member of the Guangzhou Hiking Club and also enjoy tennis and making Chinese non-heritage cultural crafts. I enjoy every moment of my life, whether it’s hiking outdoors, sweating on the court, or immersing myself in the charm of traditional Chinese culture.\nI hope that studying at Belarusian State University will be an exciting experience in life, not only in terms of learning professional knowledge, but also in terms of cross-cultural exchange and experience of local customs!\nI like to ‘city walk’ in Belarus because photography and reading are an integral part of my life. I love to capture beautiful moments in life with my camera, whether it’s a natural landscape, a cityscape or a portrait, photography allows me to see the world from different perspectives and develops my aesthetic and creativity. Reading is a source of knowledge and inspiration for me. Whether it’s literature, psychological monographs or scientific articles, books always bring me new inspirations and thoughts."
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "HomeWork",
    "section": "",
    "text": "This chapter will take you through some of the essential parts of a Python workflow.\nPrerequisites You’ll need an installation of Python and Visual Studio Code with the Python extensions to get to grips with this chapter. If you haven’t installed those yet, head back to {ref}code-preliminaries and follow the instructions there.\nWorking with Python scripts and the interactive window As a reminder, the figure below shows the typical layout of Visual Studio Code.\nA typical user view in Visual Studio Code\nWhen you create a new script (File-&gt;New File-&gt;Save as ’your_script_name.py), it will appear in the part of the screen labelled as 3.\nTo run a script, select the code you want to run, right click, and select “Run Selection/Line in Interactive Window”. You can also hit shift + enter if you set this shortcut up; if you haven’t it’s well worth doing and you can find the instructions in {ref}code-preliminaries.\nUsing the “Run Selection/Line in Interactive Window” option or using the shortcut will cause panel 5 in the above diagram (the interactive window) to appear, where you will see the code run and the outputs of your script appear.\n{tip} If you have an issue getting the code to run in the interactive window, first check the instructions in {ref}code-preliminaries. If you’re still having issues, it may be that Visual Studio Code isn’t sure which Python to run, or where Python is on your system. To fix the latter problem, hit the “Select kernel” button in the top right-hand side of the interactive window. When you are first writing a script, it’s useful to be able to move back and forth between the script and the interactive window. You might execute a line of code (put the cursor on the relevant line and hit shift and enter) in the interactive window, then manually write out some code in the interactive window’s execution box (seen at the bottom of panel 5 saying “Type code here…”), and then explore some of the variables you’ve created with the variable explorer (using the button “Variables”) at the top of the interactive window.\nBut, once you’ve honed the code in your script, it’s good to make the script a complete analytical process that you are happy running end-to-end and that—for production or ‘final’ work—you would use the “Run Current File in Interactive Window” option to run all the way through. This is good practice because what is in your script is reproducible but what you’ve entered manually in the interactive window is not. And you want the outputs from your code to be reproducible and understandable by others (including future you!), but this is hard if there are undocumented extra lines of code that you only did on the fly via the interactive window’s execution box.\nUsing installed packages and modules We already saw how to install packages in {ref}code-preliminaries. If you forgot, look back at how to do this now. In short, packages are installed using the command line or, on Windows, the Anaconda prompt. With either of these open, type conda install packagename and hit enter to both search for and install the package you need.\nWhat about using a package that you’ve installed? That’s what we’ll look at now.\nLet’s see an example of using the powerful numerical library numpy. There are different ways to import packages to use within a script or notebook; you can import the entire package in one go or just import the functions you need (if you know their names). When an entire package is imported, you can give it any name you like and the convention for numpy is to import it as the shortened ‘np’. All of the functions and methods of the package can be accessed by typing np followed by . and then typing the function name. This convention of importing packages with a given name makes your code easier to read, because you know exactly which package is doing what, and avoids any conflicts when functions from different packages have the same name.\nAs well as demonstrating importing the whole package for numpy, the example below shows importing just one specific function from numpy, inv, which does matrix inversion. Note that because inv was imported separately it can be used without an np prefix.\nimport numpy as np from numpy.linalg import inv\nmatrix = np.array([[4.0, 2.0, 4.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\nprint(“Matrix:”) print(matrix)\ninv_mat = inv(matrix) print(“Inverse:”) print(inv_mat) We could have imported all of numpy and it used it without extension using from numpy import * but this is considered bad practice as it fills our ‘namespace’ with function names that might clash with other packages and it’s less easy to read because you don’t know which function came from which package (one of Python’s mantras is “explicit is better than implict”). However, some packages are designed to be used like this, so, for example, you will see from lets_plot import * in this book.\n{note} If you want to check what packages you have installed in your Python environment, run conda list on your computer’s command line (aka the terminal or command prompt). Sometimes you might forget what a function you have imported does! Or at least, you might not be sure what all of the optional arguments are. In Visual Studio Code, you can just hover your cursor over the name of the function and a box will come up that tells you everything you need to know about it. This box is auto-generated by doc-strings; information that is written in text just under a function’s definition (def statement).\nAn alternative way to see what a function does is to use a wonderful package called rich that does many things including providing an inspect() function. You will need to use pip to install rich by running pip install rich on the command line. Here’s an example of using rich’s inpsect method on the inv() function we imported above (methods=True reports all of the functionality of inv()):\nfrom rich import inspect\ninspect(inv, help=True) {admonition} Write a code block that imports the numpy function numpy.linalg.det() as det(). Run inspect() on it. Find the determinant of [[4, 3], [1, 7]]. Modules Sometimes, you will want to call in some code from a different script that you wrote (rather than from a package provided by someone else). Imagine you have several scripts with code in, a, b, and c, all of which need to use the same underlying function that you have written. What do you do? (Note that “script with code in” is just a text file that has a .py extension and contains code.)\nA central tenet of good coding is that you do not repeat yourself. Therefore, a bad solution to this problem would be to copy and paste the same code into all three of the scripts. A good solution is to write the code that’s need just once in a separate ‘utility’ script and have the other scripts import that one function. This also adheres to another important programming principle: that of writing modular code.\nThis schematic shows the kind of situation we’re talking about:\nimport networkx as nx import matplotlib.pyplot as plt import matplotlib_inline.backend_inline\n\n\n\nplt.style.use( “https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt” ) matplotlib_inline.backend_inline.set_matplotlib_formats(“svg”)\ngraph = nx.DiGraph() graph.add_edges_from( [ (“Utility script”, “code file a”), (“Utility script”, “code file b”), (“code file a”, “code file c”), (“code file b”, “code file c”), (“Utility script”, “code file c”), ] ) colour_node = “#AFCBFF” fixed_pos = nx.spring_layout(graph, seed=100) nx.draw(graph, pos=fixed_pos, with_labels=True, node_size=6000, node_color=colour_node) extent = 1.4 plt.xlim(-extent, extent) plt.ylim(-extent, extent) plt.show(); How can we give code files a, b, and c access to the functions etc in the “Utility script”? We would define a file ‘utilities.py’ that had the following function in that we would like to use in the other code files:\n\n\n\ndef really_useful_func(number): return number*10 Then, in ‘code_script_a.py’, we would write:\nimport utilities as utils\nprint(utils.really_useful_func(20)) An alternative is to just import the function we want, with the name we want:\nfrom utilities import really_useful_func as ru_fn\nprint(ru_fn(30)) Another important example is the case where you want to run ‘utilities.py’ as a standalone script, but still want to borrow functions from it to run in other scripts. There’s a way to do this. Let’s change utilities.py to\n\n\n\ndef really_useful_func(number): return number*10\ndef default_func(): print(‘Script has run’)\nif name == ‘main’: default_func() What this says is that if we call ‘utilities.py’ from the command line, eg\npython utilities.py It will return Script has run because, by executing the script alone, we are asking for anything in the main block defined at the end of the file to be run. But we can still import anything from utilities into other scripts as before–and in that case it is not the main script, but an import, and so the main block will not be executed by default.\nYou can important several functions at once from a module (aka another script file) like this:\nfrom utilities import really_useful_func, default_func {admonition} Write your own utilities.py that has a super_useful_func that accepts a number and returns the number divided by 10. In another script, main.py, try a) importing all of utilities and running super_useful_func on a number and, b), importing just super_useful_func from utilities and running it on a number. Reading and writing files Although most applications in economics will use the pandas package to read and write tabular data, it’s sometimes useful to know how to read and write arbitrary files using the built-in Python libraries too. To open a file\nopen(‘filename’, mode) where mode could be r for read, a for append, w for write, and x to create a file. Create a file called text_example.txt and write a single line in it, ‘hello world’. To open the file and print the text, use:\nwith open(‘text_example.txt’) as f: text_in = f.read()\nprint(text_in) ‘hello world!’ is the new line character. Now let’s try adding a line to the file:\nwith open(‘text_example.txt’, ‘a’) as f: f.write(‘this is another line’) Writing and reading files using the with command is a quick and convenient shorthand for the less concise open, action, close pattern. For example, the above example can also be written as:\nf = open(‘text_example.txt’, ‘a’) f.write(‘this is another line’) f.close() Although this short example shows opening and writing a text file, this approach can be used to edit a wide range of file extensions including .json, .xml, .csv, .tsv, and many more, including binary files in addition to plain text files.\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"seattle_pet_licenses.csv\")\ndf\n\n\n\n\n\n\n\n\nanimal_s_name\nlicense_issue_date\nlicense_number\nprimary_breed\nsecondary_breed\nspecies\nzip_code\n\n\n\n\n0\nOzzy\n2005-03-29T00:00:00.000\n130651.0\nDachshund, Standard Smooth Haired\nNaN\nDog\n98104\n\n\n1\nJack\n2009-12-23T00:00:00.000\n898148.0\nSchnauzer, Miniature\nTerrier, Rat\nDog\n98107\n\n\n2\nGinger\n2006-01-20T00:00:00.000\n29654.0\nRetriever, Golden\nRetriever, Labrador\nDog\n98117\n\n\n3\nPepper\n2006-02-07T00:00:00.000\n75432.0\nManx\nMix\nCat\n98103\n\n\n4\nAddy\n2006-08-04T00:00:00.000\n729899.0\nRetriever, Golden\nNaN\nDog\n98105\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n66037\nLily\n2016-12-27T00:00:00.000\nNaN\nDomestic Shorthair\nMix\nCat\n98117\n\n\n66038\nEllie\n2016-11-29T00:00:00.000\nNaN\nGerman Shepherd\nMix\nDog\n98105\n\n\n66039\nSammy\n2016-12-05T00:00:00.000\nNaN\nTerrier\nMaltese\nDog\n98105\n\n\n66040\nBuddy\n2016-12-06T00:00:00.000\nNaN\nBullmastiff\nMix\nDog\n98105\n\n\n66041\nAku\n2016-12-07T00:00:00.000\nNaN\nChihuahua, Short Coat\nTerrier\nDog\n98106\n\n\n\n\n66042 rows × 7 columns\n\n\n\nQuestions:How many pets are included in this dataset? The answer: 66042 Questions:How many variables do we have for each pet? The answer: 7 variables\n\ndf.info()\ndf['animal_s_name'].value_counts().head(3)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\n\n\nanimal_s_name\nLucy       566\nBella      451\nCharlie    447\nName: count, dtype: int64\n\n\nQuestions:What are the three most common pet names in Seattle? The answer: Lucy、Bella、Charlie\n\n\n\n\n\n\nimport pandas as pd\nurl ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-03/data/nobel.csv'\ndf = pd.read_csv(url)\nprint(df.head())\ndf\n\n   id       firstname    surname  year category  \\\n0   1  Wilhelm Conrad    Röntgen  1901  Physics   \n1   2      Hendrik A.    Lorentz  1902  Physics   \n2   3          Pieter     Zeeman  1902  Physics   \n3   4           Henri  Becquerel  1903  Physics   \n4   5          Pierre      Curie  1903  Physics   \n\n                                         affiliation       city      country  \\\n0                                  Munich University     Munich      Germany   \n1                                  Leiden University     Leiden  Netherlands   \n2                               Amsterdam University  Amsterdam  Netherlands   \n3                                École Polytechnique      Paris       France   \n4  École municipale de physique et de chimie indu...      Paris       France   \n\n    born_date   died_date  ... died_country_code overall_motivation share  \\\n0  1845-03-27  1923-02-10  ...                DE                NaN     1   \n1  1853-07-18  1928-02-04  ...                NL                NaN     2   \n2  1865-05-25  1943-10-09  ...                NL                NaN     2   \n3  1852-12-15  1908-08-25  ...                FR                NaN     2   \n4  1859-05-15  1906-04-19  ...                FR                NaN     4   \n\n                                          motivation  born_country_original  \\\n0  \"in recognition of the extraordinary services ...  Prussia (now Germany)   \n1  \"in recognition of the extraordinary service t...        the Netherlands   \n2  \"in recognition of the extraordinary service t...        the Netherlands   \n3  \"in recognition of the extraordinary services ...                 France   \n4  \"in recognition of the extraordinary services ...                 France   \n\n       born_city_original died_country_original died_city_original  \\\n0  Lennep (now Remscheid)               Germany             Munich   \n1                  Arnhem       the Netherlands                NaN   \n2              Zonnemaire       the Netherlands          Amsterdam   \n3                   Paris                France                NaN   \n4                   Paris                France              Paris   \n\n   city_original country_original  \n0         Munich          Germany  \n1         Leiden  the Netherlands  \n2      Amsterdam  the Netherlands  \n3          Paris           France  \n4          Paris           France  \n\n[5 rows x 26 columns]\n\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n935 rows × 26 columns\n\n\n\nQuestions:How many observations and how many variables are in the dataset? What does each row represent? The answer: 935 observations, 26 variables, and one row for each person.\n\ndf.info()\nprint(df)\nnobel_living = df[\n    (df['country'].notna()) &  \n    (df['gender'] != 'org') &  \n    (df['died_date'].isna())  \n]\nprint(nobel_living)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 935 entries, 0 to 934\nData columns (total 26 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   id                     935 non-null    int64 \n 1   firstname              935 non-null    object\n 2   surname                906 non-null    object\n 3   year                   935 non-null    int64 \n 4   category               935 non-null    object\n 5   affiliation            685 non-null    object\n 6   city                   680 non-null    object\n 7   country                681 non-null    object\n 8   born_date              902 non-null    object\n 9   died_date              627 non-null    object\n 10  gender                 935 non-null    object\n 11  born_city              907 non-null    object\n 12  born_country           907 non-null    object\n 13  born_country_code      907 non-null    object\n 14  died_city              608 non-null    object\n 15  died_country           614 non-null    object\n 16  died_country_code      614 non-null    object\n 17  overall_motivation     17 non-null     object\n 18  share                  935 non-null    int64 \n 19  motivation             935 non-null    object\n 20  born_country_original  907 non-null    object\n 21  born_city_original     907 non-null    object\n 22  died_country_original  614 non-null    object\n 23  died_city_original     608 non-null    object\n 24  city_original          680 non-null    object\n 25  country_original       681 non-null    object\ndtypes: int64(3), object(23)\nmemory usage: 190.1+ KB\n      id       firstname    surname  year   category  \\\n0      1  Wilhelm Conrad    Röntgen  1901    Physics   \n1      2      Hendrik A.    Lorentz  1902    Physics   \n2      3          Pieter     Zeeman  1902    Physics   \n3      4           Henri  Becquerel  1903    Physics   \n4      5          Pierre      Curie  1903    Physics   \n..   ...             ...        ...   ...        ...   \n930  965  Sir Gregory P.     Winter  2018  Chemistry   \n931  966           Denis    Mukwege  2018      Peace   \n932  967           Nadia      Murad  2018      Peace   \n933  968      William D.   Nordhaus  2018  Economics   \n934  969         Paul M.      Romer  2018  Economics   \n\n                                           affiliation          city  \\\n0                                    Munich University        Munich   \n1                                    Leiden University        Leiden   \n2                                 Amsterdam University     Amsterdam   \n3                                  École Polytechnique         Paris   \n4    École municipale de physique et de chimie indu...         Paris   \n..                                                 ...           ...   \n930                MRC Laboratory of Molecular Biology     Cambridge   \n931                                                NaN           NaN   \n932                                                NaN           NaN   \n933                                    Yale University  New Haven CT   \n934                       NYU Stern School of Business   New York NY   \n\n            country   born_date   died_date  ... died_country_code  \\\n0           Germany  1845-03-27  1923-02-10  ...                DE   \n1       Netherlands  1853-07-18  1928-02-04  ...                NL   \n2       Netherlands  1865-05-25  1943-10-09  ...                NL   \n3            France  1852-12-15  1908-08-25  ...                FR   \n4            France  1859-05-15  1906-04-19  ...                FR   \n..              ...         ...         ...  ...               ...   \n930  United Kingdom  1951-04-14         NaN  ...               NaN   \n931             NaN  1955-03-01         NaN  ...               NaN   \n932             NaN         NaN         NaN  ...               NaN   \n933             USA  1941-05-31         NaN  ...               NaN   \n934             USA         NaN         NaN  ...               NaN   \n\n    overall_motivation share  \\\n0                  NaN     1   \n1                  NaN     2   \n2                  NaN     2   \n3                  NaN     2   \n4                  NaN     4   \n..                 ...   ...   \n930                NaN     4   \n931                NaN     2   \n932                NaN     2   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation  \\\n0    \"in recognition of the extraordinary services ...   \n1    \"in recognition of the extraordinary service t...   \n2    \"in recognition of the extraordinary service t...   \n3    \"in recognition of the extraordinary services ...   \n4    \"in recognition of the extraordinary services ...   \n..                                                 ...   \n930  \"for the phage display of peptides and antibod...   \n931  \"for their efforts to end the use of sexual vi...   \n932  \"for their efforts to end the use of sexual vi...   \n933  \"for integrating climate change into long-run ...   \n934  \"for integrating technological innovations int...   \n\n                                 born_country_original  \\\n0                                Prussia (now Germany)   \n1                                      the Netherlands   \n2                                      the Netherlands   \n3                                               France   \n4                                               France   \n..                                                 ...   \n930                                     United Kingdom   \n931  Belgian Congo (now Democratic Republic of the ...   \n932                                               Iraq   \n933                                                USA   \n934                                                USA   \n\n         born_city_original died_country_original died_city_original  \\\n0    Lennep (now Remscheid)               Germany             Munich   \n1                    Arnhem       the Netherlands                NaN   \n2                Zonnemaire       the Netherlands          Amsterdam   \n3                     Paris                France                NaN   \n4                     Paris                France              Paris   \n..                      ...                   ...                ...   \n930               Leicester                   NaN                NaN   \n931                  Bukavu                   NaN                NaN   \n932                    Kojo                   NaN                NaN   \n933          Albuquerque NM                   NaN                NaN   \n934               Denver CO                   NaN                NaN   \n\n     city_original country_original  \n0           Munich          Germany  \n1           Leiden  the Netherlands  \n2        Amsterdam  the Netherlands  \n3            Paris           France  \n4            Paris           France  \n..             ...              ...  \n930      Cambridge   United Kingdom  \n931            NaN              NaN  \n932            NaN              NaN  \n933   New Haven CT              USA  \n934    New York NY              USA  \n\n[935 rows x 26 columns]\n      id       firstname   surname  year   category  \\\n68    68       Chen Ning      Yang  1957    Physics   \n69    69       Tsung-Dao       Lee  1957    Physics   \n94    95         Leon N.    Cooper  1972    Physics   \n96    97             Leo     Esaki  1973    Physics   \n97    98            Ivar   Giaever  1973    Physics   \n..   ...             ...       ...   ...        ...   \n928  963      Frances H.    Arnold  2018  Chemistry   \n929  964       George P.     Smith  2018  Chemistry   \n930  965  Sir Gregory P.    Winter  2018  Chemistry   \n933  968      William D.  Nordhaus  2018  Economics   \n934  969         Paul M.     Romer  2018  Economics   \n\n                                      affiliation                 city  \\\n68                   Institute for Advanced Study         Princeton NJ   \n69                            Columbia University          New York NY   \n94                               Brown University        Providence RI   \n96           IBM Thomas J. Watson Research Center  Yorktown Heights NY   \n97                       General Electric Company       Schenectady NY   \n..                                            ...                  ...   \n928  California Institute of Technology (Caltech)          Pasadena CA   \n929                        University of Missouri             Columbia   \n930           MRC Laboratory of Molecular Biology            Cambridge   \n933                               Yale University         New Haven CT   \n934                  NYU Stern School of Business          New York NY   \n\n            country   born_date died_date  ... died_country_code  \\\n68              USA  1922-09-22       NaN  ...               NaN   \n69              USA  1926-11-24       NaN  ...               NaN   \n94              USA  1930-02-28       NaN  ...               NaN   \n96              USA  1925-03-12       NaN  ...               NaN   \n97              USA  1929-04-05       NaN  ...               NaN   \n..              ...         ...       ...  ...               ...   \n928             USA  1956-07-25       NaN  ...               NaN   \n929             USA  1941-03-10       NaN  ...               NaN   \n930  United Kingdom  1951-04-14       NaN  ...               NaN   \n933             USA  1941-05-31       NaN  ...               NaN   \n934             USA         NaN       NaN  ...               NaN   \n\n    overall_motivation share  \\\n68                 NaN     2   \n69                 NaN     2   \n94                 NaN     3   \n96                 NaN     4   \n97                 NaN     4   \n..                 ...   ...   \n928                NaN     2   \n929                NaN     4   \n930                NaN     4   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation born_country_original  \\\n68   \"for their penetrating investigation of the so...                 China   \n69   \"for their penetrating investigation of the so...                 China   \n94   \"for their jointly developed theory of superco...                   USA   \n96   \"for their experimental discoveries regarding ...                 Japan   \n97   \"for their experimental discoveries regarding ...                Norway   \n..                                                 ...                   ...   \n928            \"for the directed evolution of enzymes\"                   USA   \n929  \"for the phage display of peptides and antibod...                   USA   \n930  \"for the phage display of peptides and antibod...        United Kingdom   \n933  \"for integrating climate change into long-run ...                   USA   \n934  \"for integrating technological innovations int...                   USA   \n\n    born_city_original died_country_original died_city_original  \\\n68        Hofei Anhwei                   NaN                NaN   \n69            Shanghai                   NaN                NaN   \n94         New York NY                   NaN                NaN   \n96               Osaka                   NaN                NaN   \n97              Bergen                   NaN                NaN   \n..                 ...                   ...                ...   \n928      Pittsburgh PA                   NaN                NaN   \n929         Norwalk CT                   NaN                NaN   \n930          Leicester                   NaN                NaN   \n933     Albuquerque NM                   NaN                NaN   \n934          Denver CO                   NaN                NaN   \n\n           city_original country_original  \n68          Princeton NJ              USA  \n69           New York NY              USA  \n94         Providence RI              USA  \n96   Yorktown Heights NY              USA  \n97        Schenectady NY              USA  \n..                   ...              ...  \n928          Pasadena CA              USA  \n929             Columbia              USA  \n930            Cambridge   United Kingdom  \n933         New Haven CT              USA  \n934          New York NY              USA  \n\n[228 rows x 26 columns]\n\n\nQuestions:Where were most Nobel laureates based when they won their prizes? The answer: USA\n\n\n\n\nimport pandas as pd\nurl ='https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ndf = pd.read_csv(url)\nprint(df.head())\ndf\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.info()\nfrom skimpy import clean_columns\ndf = clean_columns(df,case=\"snake\")\nprint(df.columns)\ndf.fillna(\"-\")\ndf.describe()\nsum_table = df.describe().round(2)\nsum_table\ndf.dropna()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nIndex(['passenger_id', 'survived', 'pclass', 'name', 'sex', 'age', 'sib_sp',\n       'parch', 'ticket', 'fare', 'cabin', 'embarked'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n10\n11\n1\n3\nSandstrom, Miss. Marguerite Rut\nfemale\n4.0\n1\n1\nPP 9549\n16.7000\nG6\nS\n\n\n11\n12\n1\n1\nBonnell, Miss. Elizabeth\nfemale\n58.0\n0\n0\n113783\n26.5500\nC103\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n871\n872\n1\n1\nBeckwith, Mrs. Richard Leonard (Sallie Monypeny)\nfemale\n47.0\n1\n1\n11751\n52.5542\nD35\nS\n\n\n872\n873\n0\n1\nCarlsson, Mr. Frans Olof\nmale\n33.0\n0\n0\n695\n5.0000\nB51 B53 B55\nS\n\n\n879\n880\n1\n1\nPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\nfemale\n56.0\n0\n1\n11767\n83.1583\nC50\nC\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n\n\n183 rows × 12 columns\n\n\n\n\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"all-ages.csv\")\ndf\nresult = df.groupby([\"Major\"]).sum().sort_values([\"Unemployment_rate\"])\nprint(result)\n\n                                            Major_code  \\\nMajor                                                    \nEDUCATIONAL ADMINISTRATION AND SUPERVISION        2301   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING            2411   \nPHARMACOLOGY                                      3607   \nMATERIALS SCIENCE                                 5008   \nMATHEMATICS AND COMPUTER SCIENCE                  4005   \n...                                                ...   \nLIBRARY SCIENCE                                   3501   \nSCHOOL STUDENT COUNSELING                         2303   \nMILITARY TECHNOLOGIES                             3801   \nCLINICAL PSYCHOLOGY                               5202   \nMISCELLANEOUS FINE ARTS                           6099   \n\n                                                                 Major_category  \\\nMajor                                                                             \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                            Education   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                              Engineering   \nPHARMACOLOGY                                             Biology & Life Science   \nMATERIALS SCIENCE                                                   Engineering   \nMATHEMATICS AND COMPUTER SCIENCE                        Computers & Mathematics   \n...                                                                         ...   \nLIBRARY SCIENCE                                                       Education   \nSCHOOL STUDENT COUNSELING                                             Education   \nMILITARY TECHNOLOGIES                       Industrial Arts & Consumer Services   \nCLINICAL PSYCHOLOGY                                    Psychology & Social Work   \nMISCELLANEOUS FINE ARTS                                                    Arts   \n\n                                            Total  Employed  \\\nMajor                                                         \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   4037      3113   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       6264      4120   \nPHARMACOLOGY                                 5015      3481   \nMATERIALS SCIENCE                            7208      5866   \nMATHEMATICS AND COMPUTER SCIENCE             7184      5874   \n...                                           ...       ...   \nLIBRARY SCIENCE                             16193      7091   \nSCHOOL STUDENT COUNSELING                    2396      1492   \nMILITARY TECHNOLOGIES                        4315      1650   \nCLINICAL PSYCHOLOGY                          7638      5128   \nMISCELLANEOUS FINE ARTS                      8511      6431   \n\n                                            Employed_full_time_year_round  \\\nMajor                                                                       \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                           2468   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                               3350   \nPHARMACOLOGY                                                         2579   \nMATERIALS SCIENCE                                                    4505   \nMATHEMATICS AND COMPUTER SCIENCE                                     5039   \n...                                                                   ...   \nLIBRARY SCIENCE                                                      4330   \nSCHOOL STUDENT COUNSELING                                            1093   \nMILITARY TECHNOLOGIES                                                1708   \nCLINICAL PSYCHOLOGY                                                  3297   \nMISCELLANEOUS FINE ARTS                                              3802   \n\n                                            Unemployed  Unemployment_rate  \\\nMajor                                                                       \nEDUCATIONAL ADMINISTRATION AND SUPERVISION           0           0.000000   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING               0           0.000000   \nPHARMACOLOGY                                        57           0.016111   \nMATERIALS SCIENCE                                  134           0.022333   \nMATHEMATICS AND COMPUTER SCIENCE                   150           0.024900   \n...                                                ...                ...   \nLIBRARY SCIENCE                                    743           0.094843   \nSCHOOL STUDENT COUNSELING                          169           0.101746   \nMILITARY TECHNOLOGIES                              187           0.101796   \nCLINICAL PSYCHOLOGY                                587           0.102712   \nMISCELLANEOUS FINE ARTS                           1190           0.156147   \n\n                                            Median  P25th     P75th  \nMajor                                                                \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   58000  44750   79000.0  \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       85000  55000  125000.0  \nPHARMACOLOGY                                 60000  35000  105000.0  \nMATERIALS SCIENCE                            75000  60000  100000.0  \nMATHEMATICS AND COMPUTER SCIENCE             92000  53000  136000.0  \n...                                            ...    ...       ...  \nLIBRARY SCIENCE                              40000  30000   55000.0  \nSCHOOL STUDENT COUNSELING                    41000  33200   50000.0  \nMILITARY TECHNOLOGIES                        64000  39750   90000.0  \nCLINICAL PSYCHOLOGY                          45000  26100   62000.0  \nMISCELLANEOUS FINE ARTS                      45000  30000   60000.0  \n\n[173 rows x 10 columns]\n\n\n\n### 按照专业分组，并把失业率从低到高升序排列\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\n\n\n\n\n\n\n\n\nRank\nMajor_code\nMajor\nTotal\nMen\nWomen\nMajor_category\nShareWomen\nSample_size\nEmployed\n...\nPart_time\nFull_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\nCollege_jobs\nNon_college_jobs\nLow_wage_jobs\n\n\n\n\n0\n1\n2419\nPETROLEUM ENGINEERING\n2339.0\n2057.0\n282.0\nEngineering\n0.120564\n36\n1976\n...\n270\n1207\n37\n0.018381\n110000\n95000\n125000\n1534\n364\n193\n\n\n1\n2\n2416\nMINING AND MINERAL ENGINEERING\n756.0\n679.0\n77.0\nEngineering\n0.101852\n7\n640\n...\n170\n388\n85\n0.117241\n75000\n55000\n90000\n350\n257\n50\n\n\n2\n3\n2415\nMETALLURGICAL ENGINEERING\n856.0\n725.0\n131.0\nEngineering\n0.153037\n3\n648\n...\n133\n340\n16\n0.024096\n73000\n50000\n105000\n456\n176\n0\n\n\n3\n4\n2417\nNAVAL ARCHITECTURE AND MARINE ENGINEERING\n1258.0\n1123.0\n135.0\nEngineering\n0.107313\n16\n758\n...\n150\n692\n40\n0.050125\n70000\n43000\n80000\n529\n102\n0\n\n\n4\n5\n2405\nCHEMICAL ENGINEERING\n32260.0\n21239.0\n11021.0\nEngineering\n0.341631\n289\n25694\n...\n5180\n16697\n1672\n0.061098\n65000\n50000\n75000\n18314\n4440\n972\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n169\n3609\nZOOLOGY\n8409.0\n3050.0\n5359.0\nBiology & Life Science\n0.637293\n47\n6259\n...\n2190\n3602\n304\n0.046320\n26000\n20000\n39000\n2771\n2947\n743\n\n\n169\n170\n5201\nEDUCATIONAL PSYCHOLOGY\n2854.0\n522.0\n2332.0\nPsychology & Social Work\n0.817099\n7\n2125\n...\n572\n1211\n148\n0.065112\n25000\n24000\n34000\n1488\n615\n82\n\n\n170\n171\n5202\nCLINICAL PSYCHOLOGY\n2838.0\n568.0\n2270.0\nPsychology & Social Work\n0.799859\n13\n2101\n...\n648\n1293\n368\n0.149048\n25000\n25000\n40000\n986\n870\n622\n\n\n171\n172\n5203\nCOUNSELING PSYCHOLOGY\n4626.0\n931.0\n3695.0\nPsychology & Social Work\n0.798746\n21\n3777\n...\n965\n2738\n214\n0.053621\n23400\n19200\n26000\n2403\n1245\n308\n\n\n172\n173\n3501\nLIBRARY SCIENCE\n1098.0\n134.0\n964.0\nEducation\n0.877960\n2\n742\n...\n237\n410\n87\n0.104946\n22000\n20000\n22000\n288\n338\n192\n\n\n\n\n173 rows × 21 columns\n\n\n\nresult = df.groupby([“Major”]).sum().sort_values([“ShareWomen”],ascending=False) print(result)\n\n### 按照专业分组，将女生占比从高到低降序排列\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\nresult = df.groupby([\"Major\"]).sum().sort_values([\"ShareWomen\"],ascending=False)\nprint(result)\n\n                                               Rank  Major_code     Total  \\\nMajor                                                                       \nEARLY CHILDHOOD EDUCATION                       165        2307   37589.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   164        6102   38279.0   \nMEDICAL ASSISTING SERVICES                       52        6104   11123.0   \nELEMENTARY EDUCATION                            139        2304  170862.0   \nFAMILY AND CONSUMER SCIENCES                    151        2901   58001.0   \n...                                             ...         ...       ...   \nMINING AND MINERAL ENGINEERING                    2        2416     756.0   \nCONSTRUCTION SERVICES                            27        5601   18498.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      67        2504    4790.0   \nFOOD SCIENCE                                     22        1104       0.0   \nMILITARY TECHNOLOGIES                            74        3801     124.0   \n\n                                                   Men     Women  \\\nMajor                                                              \nEARLY CHILDHOOD EDUCATION                       1167.0   36422.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   1225.0   37054.0   \nMEDICAL ASSISTING SERVICES                       803.0   10320.0   \nELEMENTARY EDUCATION                           13029.0  157833.0   \nFAMILY AND CONSUMER SCIENCES                    5166.0   52835.0   \n...                                                ...       ...   \nMINING AND MINERAL ENGINEERING                   679.0      77.0   \nCONSTRUCTION SERVICES                          16820.0    1678.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     4419.0     371.0   \nFOOD SCIENCE                                       0.0       0.0   \nMILITARY TECHNOLOGIES                            124.0       0.0   \n\n                                                                    Major_category  \\\nMajor                                                                                \nEARLY CHILDHOOD EDUCATION                                                Education   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                               Health   \nMEDICAL ASSISTING SERVICES                                                  Health   \nELEMENTARY EDUCATION                                                     Education   \nFAMILY AND CONSUMER SCIENCES                   Industrial Arts & Consumer Services   \n...                                                                            ...   \nMINING AND MINERAL ENGINEERING                                         Engineering   \nCONSTRUCTION SERVICES                          Industrial Arts & Consumer Services   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                            Engineering   \nFOOD SCIENCE                                       Agriculture & Natural Resources   \nMILITARY TECHNOLOGIES                          Industrial Arts & Consumer Services   \n\n                                               ShareWomen  Sample_size  \\\nMajor                                                                    \nEARLY CHILDHOOD EDUCATION                        0.968954          342   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES    0.967998           95   \nMEDICAL ASSISTING SERVICES                       0.927807           67   \nELEMENTARY EDUCATION                             0.923745         1629   \nFAMILY AND CONSUMER SCIENCES                     0.910933          518   \n...                                                   ...          ...   \nMINING AND MINERAL ENGINEERING                   0.101852            7   \nCONSTRUCTION SERVICES                            0.090713          295   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      0.077453           71   \nFOOD SCIENCE                                     0.000000           36   \nMILITARY TECHNOLOGIES                            0.000000            4   \n\n                                               Employed  Full_time  Part_time  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                         32551      27569       7001   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES     29763      19975      13862   \nMEDICAL ASSISTING SERVICES                         9168       5643       4107   \nELEMENTARY EDUCATION                             149339     123177      37965   \nFAMILY AND CONSUMER SCIENCES                      46624      36747      15872   \n...                                                 ...        ...        ...   \nMINING AND MINERAL ENGINEERING                      640        556        170   \nCONSTRUCTION SERVICES                             16318      15690       1751   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES        4186       4175        247   \nFOOD SCIENCE                                       3149       2558       1121   \nMILITARY TECHNOLOGIES                                 0        111          0   \n\n                                               Full_time_year_round  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                                     20748   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                 14460   \nMEDICAL ASSISTING SERVICES                                     4290   \nELEMENTARY EDUCATION                                          86540   \nFAMILY AND CONSUMER SCIENCES                                  26906   \n...                                                             ...   \nMINING AND MINERAL ENGINEERING                                  388   \nCONSTRUCTION SERVICES                                         12313   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                    3607   \nFOOD SCIENCE                                                   1735   \nMILITARY TECHNOLOGIES                                           111   \n\n                                               Unemployed  Unemployment_rate  \\\nMajor                                                                          \nEARLY CHILDHOOD EDUCATION                            1360           0.040105   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES        1487           0.047584   \nMEDICAL ASSISTING SERVICES                            407           0.042507   \nELEMENTARY EDUCATION                                 7297           0.046586   \nFAMILY AND CONSUMER SCIENCES                         3355           0.067128   \n...                                                   ...                ...   \nMINING AND MINERAL ENGINEERING                         85           0.117241   \nCONSTRUCTION SERVICES                                1042           0.060023   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES           250           0.056357   \nFOOD SCIENCE                                          338           0.096931   \nMILITARY TECHNOLOGIES                                   0           0.000000   \n\n                                               Median  P25th  P75th  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                       28000  21000  35000   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   28000  20000  40000   \nMEDICAL ASSISTING SERVICES                      42000  30000  65000   \nELEMENTARY EDUCATION                            32000  23400  38000   \nFAMILY AND CONSUMER SCIENCES                    30000  22900  40000   \n...                                               ...    ...    ...   \nMINING AND MINERAL ENGINEERING                  75000  55000  90000   \nCONSTRUCTION SERVICES                           50000  36000  60000   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     40000  27000  52000   \nFOOD SCIENCE                                    53000  32000  70000   \nMILITARY TECHNOLOGIES                           40000  40000  40000   \n\n                                               College_jobs  Non_college_jobs  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                             23515              7705   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES         19957              9404   \nMEDICAL ASSISTING SERVICES                             2091              6948   \nELEMENTARY EDUCATION                                 108085             36972   \nFAMILY AND CONSUMER SCIENCES                          20985             20133   \n...                                                     ...               ...   \nMINING AND MINERAL ENGINEERING                          350               257   \nCONSTRUCTION SERVICES                                  3275              5351   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES            1861              2121   \nFOOD SCIENCE                                           1183              1274   \nMILITARY TECHNOLOGIES                                     0                 0   \n\n                                               Low_wage_jobs  \nMajor                                                         \nEARLY CHILDHOOD EDUCATION                               2868  \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES           5125  \nMEDICAL ASSISTING SERVICES                              1270  \nELEMENTARY EDUCATION                                   11502  \nFAMILY AND CONSUMER SCIENCES                            5248  \n...                                                      ...  \nMINING AND MINERAL ENGINEERING                            50  \nCONSTRUCTION SERVICES                                    703  \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES              406  \nFOOD SCIENCE                                             485  \nMILITARY TECHNOLOGIES                                      0  \n\n[173 rows x 20 columns]\n\n\n\n### 按照专业分组，将女生占比从高到低降序排列\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\na=df['Median'].groupby(df['Major_category']).sum()\na.plot.bar()\nplt.show()\n\n\n\n\n\n\n\n\nQuestions:What should I major in?\nThe answer: Engineering\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\ndf = pd.read_csv('plastic-waste.csv')\ndf_clean = df.dropna(subset=['plastic_waste_per_cap', 'continent'])\n\nQuenstion1:Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? The answer:With the histogram, it is possible to observe differences in the per capita distribution of plastic waste across continents. For example, Africa shows a higher peak waste output and North America shows a wider distribution.\n\n# Create histograms faceted by continent\np_histogram = ggplot(df_clean, aes(x='plastic_waste_per_cap')) + \\\n    geom_histogram(bins=30, fill='blue', color='black', alpha=0.7) + \\\n    facet_wrap('continent') + \\\n    ggtitle('Distribution of Plastic Waste per Capita by Continent') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Frequency')\n\n\np_histogram.show()\n\n   \n   \n\n\nQuenstion2:Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots? The answer:Fiddle plots show the complete distribution of the data, showing the shape of the data and multiple peaks in the graph. Box plots provide explicit statistical information that violin plots only reflect through shape.\n\n# Violin plots\np_violin = ggplot(df, aes(x='continent', y='plastic_waste_per_cap', fill='continent')) + \\\n    geom_violin(alpha=0.7) + \\\n    geom_boxplot(width=0.1, fill='white', color='black') + \\\n    ggtitle('Violin Plot of Plastic Waste per Capita by Continent') + \\\n    xlab('Continent') + \\\n    ylab('Plastic Waste per Capita')\n\np_violin.show()\n\n   \n   \n\n\nQuenstion3:Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. The answer:The scatter plot presents the relationship between plastic waste per capita and poorly managed waste per capita, checking if there is a positive or other relationship.\n\n# Scatterplot\nif 'mismanaged_plastic_waste_per_cap' in df.columns:\n    p_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap')) + \\\n        geom_point(size=3, alpha=0.6) + \\\n        ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita') + \\\n        xlab('Plastic Waste per Capita') + \\\n        ylab('Mismanaged Plastic Waste per Capita')\n    \n    p_scatter.show()\n\n   \n   \n\n\nQuenstion4:Colour the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? The answer:By distinguishing continents by color, it is possible to observe differences between continents. Certain continents may exhibit specific patterns or clusters, such as the gradual rise of Africa.\n\n# Colored scatterplot\np_scatter_colored = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita by Continent') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Mismanaged Plastic Waste per Capita')\n\np_scatter_colored.show()\n\n   \n   \n\n\nQuenstion5:Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated? The answer:The visualization of the relationship between the two demographic variables shows the association between plastic waste per capita and total population and coastal population. Through scatterplot analysis, plastic waste per capita exhibits a stronger linear relationship with coastal population.\n\n# Plastic waste per capita vs Total population\np_pop_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='total_pop', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste per Capita vs. Total Population') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Total Population')\n\np_pop_scatter.show()\n\n   \n   \n\n\n\n# Plastic waste per capita vs Coastal population\np_coastal_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='coastal_pop', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste per Capita vs. Coastal Population') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Coastal Population')\n\np_coastal_scatter.show()\n\n   \n   \n\n\n\np_coastal_scatter = ggplot(df, aes(x='coastal_pop', y='plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Coastal Population vs. Plastic Waste per Capita') + \\\n    xlab('coastal pop') + \\\n    ylab('plastic waste per cap')\n\np_coastal_scatter.show()\n\n   \n   \n\n\n\np_coastal_scatter.show()\ndf['coastal_population_proportion'] = df['coastal_pop'] / df['total_pop']\ndf_filtered = df\ndf_filtered = df_filtered[(df_filtered['plastic_waste_per_cap'] &lt;= 0.6) & \n(df_filtered['coastal_population_proportion'] &lt;= 1.6)]\np_scatter = ggplot(df_filtered, aes(x='coastal_population_proportion', y='plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.7) + \\\n    geom_smooth(method='lm', color='black', se=True, linetype='solid', size=1) + \\\n    ggtitle('Plastic Waste per Capita vs Coastal Population Proportion') + \\\n    xlab('Coastal Population Proportion') + \\\n    ylab('Plastic Waste per Capita')\n\n\np_scatter.show()"
  },
  {
    "objectID": "homework.html#lecture02",
    "href": "homework.html#lecture02",
    "title": "HomeWork",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"seattle_pet_licenses.csv\")\ndf\n\n\n\n\n\n\n\n\nanimal_s_name\nlicense_issue_date\nlicense_number\nprimary_breed\nsecondary_breed\nspecies\nzip_code\n\n\n\n\n0\nOzzy\n2005-03-29T00:00:00.000\n130651.0\nDachshund, Standard Smooth Haired\nNaN\nDog\n98104\n\n\n1\nJack\n2009-12-23T00:00:00.000\n898148.0\nSchnauzer, Miniature\nTerrier, Rat\nDog\n98107\n\n\n2\nGinger\n2006-01-20T00:00:00.000\n29654.0\nRetriever, Golden\nRetriever, Labrador\nDog\n98117\n\n\n3\nPepper\n2006-02-07T00:00:00.000\n75432.0\nManx\nMix\nCat\n98103\n\n\n4\nAddy\n2006-08-04T00:00:00.000\n729899.0\nRetriever, Golden\nNaN\nDog\n98105\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n66037\nLily\n2016-12-27T00:00:00.000\nNaN\nDomestic Shorthair\nMix\nCat\n98117\n\n\n66038\nEllie\n2016-11-29T00:00:00.000\nNaN\nGerman Shepherd\nMix\nDog\n98105\n\n\n66039\nSammy\n2016-12-05T00:00:00.000\nNaN\nTerrier\nMaltese\nDog\n98105\n\n\n66040\nBuddy\n2016-12-06T00:00:00.000\nNaN\nBullmastiff\nMix\nDog\n98105\n\n\n66041\nAku\n2016-12-07T00:00:00.000\nNaN\nChihuahua, Short Coat\nTerrier\nDog\n98106\n\n\n\n\n66042 rows × 7 columns\n\n\n\nQuestions:How many pets are included in this dataset? The answer: 66042 Questions:How many variables do we have for each pet? The answer: 7 variables\n\ndf.info()\ndf['animal_s_name'].value_counts().head(3)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\n\n\nanimal_s_name\nLucy       566\nBella      451\nCharlie    447\nName: count, dtype: int64\n\n\nQuestions:What are the three most common pet names in Seattle? The answer: Lucy、Bella、Charlie"
  },
  {
    "objectID": "homework.html#lecture03",
    "href": "homework.html#lecture03",
    "title": "HomeWork",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-03/data/nobel.csv'\ndf = pd.read_csv(url)\nprint(df.head())\ndf\n\n   id       firstname    surname  year category  \\\n0   1  Wilhelm Conrad    Röntgen  1901  Physics   \n1   2      Hendrik A.    Lorentz  1902  Physics   \n2   3          Pieter     Zeeman  1902  Physics   \n3   4           Henri  Becquerel  1903  Physics   \n4   5          Pierre      Curie  1903  Physics   \n\n                                         affiliation       city      country  \\\n0                                  Munich University     Munich      Germany   \n1                                  Leiden University     Leiden  Netherlands   \n2                               Amsterdam University  Amsterdam  Netherlands   \n3                                École Polytechnique      Paris       France   \n4  École municipale de physique et de chimie indu...      Paris       France   \n\n    born_date   died_date  ... died_country_code overall_motivation share  \\\n0  1845-03-27  1923-02-10  ...                DE                NaN     1   \n1  1853-07-18  1928-02-04  ...                NL                NaN     2   \n2  1865-05-25  1943-10-09  ...                NL                NaN     2   \n3  1852-12-15  1908-08-25  ...                FR                NaN     2   \n4  1859-05-15  1906-04-19  ...                FR                NaN     4   \n\n                                          motivation  born_country_original  \\\n0  \"in recognition of the extraordinary services ...  Prussia (now Germany)   \n1  \"in recognition of the extraordinary service t...        the Netherlands   \n2  \"in recognition of the extraordinary service t...        the Netherlands   \n3  \"in recognition of the extraordinary services ...                 France   \n4  \"in recognition of the extraordinary services ...                 France   \n\n       born_city_original died_country_original died_city_original  \\\n0  Lennep (now Remscheid)               Germany             Munich   \n1                  Arnhem       the Netherlands                NaN   \n2              Zonnemaire       the Netherlands          Amsterdam   \n3                   Paris                France                NaN   \n4                   Paris                France              Paris   \n\n   city_original country_original  \n0         Munich          Germany  \n1         Leiden  the Netherlands  \n2      Amsterdam  the Netherlands  \n3          Paris           France  \n4          Paris           France  \n\n[5 rows x 26 columns]\n\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n935 rows × 26 columns\n\n\n\nQuestions:How many observations and how many variables are in the dataset? What does each row represent? The answer: 935 observations, 26 variables, and one row for each person.\n\ndf.info()\nprint(df)\nnobel_living = df[\n    (df['country'].notna()) &  \n    (df['gender'] != 'org') &  \n    (df['died_date'].isna())  \n]\nprint(nobel_living)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 935 entries, 0 to 934\nData columns (total 26 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   id                     935 non-null    int64 \n 1   firstname              935 non-null    object\n 2   surname                906 non-null    object\n 3   year                   935 non-null    int64 \n 4   category               935 non-null    object\n 5   affiliation            685 non-null    object\n 6   city                   680 non-null    object\n 7   country                681 non-null    object\n 8   born_date              902 non-null    object\n 9   died_date              627 non-null    object\n 10  gender                 935 non-null    object\n 11  born_city              907 non-null    object\n 12  born_country           907 non-null    object\n 13  born_country_code      907 non-null    object\n 14  died_city              608 non-null    object\n 15  died_country           614 non-null    object\n 16  died_country_code      614 non-null    object\n 17  overall_motivation     17 non-null     object\n 18  share                  935 non-null    int64 \n 19  motivation             935 non-null    object\n 20  born_country_original  907 non-null    object\n 21  born_city_original     907 non-null    object\n 22  died_country_original  614 non-null    object\n 23  died_city_original     608 non-null    object\n 24  city_original          680 non-null    object\n 25  country_original       681 non-null    object\ndtypes: int64(3), object(23)\nmemory usage: 190.1+ KB\n      id       firstname    surname  year   category  \\\n0      1  Wilhelm Conrad    Röntgen  1901    Physics   \n1      2      Hendrik A.    Lorentz  1902    Physics   \n2      3          Pieter     Zeeman  1902    Physics   \n3      4           Henri  Becquerel  1903    Physics   \n4      5          Pierre      Curie  1903    Physics   \n..   ...             ...        ...   ...        ...   \n930  965  Sir Gregory P.     Winter  2018  Chemistry   \n931  966           Denis    Mukwege  2018      Peace   \n932  967           Nadia      Murad  2018      Peace   \n933  968      William D.   Nordhaus  2018  Economics   \n934  969         Paul M.      Romer  2018  Economics   \n\n                                           affiliation          city  \\\n0                                    Munich University        Munich   \n1                                    Leiden University        Leiden   \n2                                 Amsterdam University     Amsterdam   \n3                                  École Polytechnique         Paris   \n4    École municipale de physique et de chimie indu...         Paris   \n..                                                 ...           ...   \n930                MRC Laboratory of Molecular Biology     Cambridge   \n931                                                NaN           NaN   \n932                                                NaN           NaN   \n933                                    Yale University  New Haven CT   \n934                       NYU Stern School of Business   New York NY   \n\n            country   born_date   died_date  ... died_country_code  \\\n0           Germany  1845-03-27  1923-02-10  ...                DE   \n1       Netherlands  1853-07-18  1928-02-04  ...                NL   \n2       Netherlands  1865-05-25  1943-10-09  ...                NL   \n3            France  1852-12-15  1908-08-25  ...                FR   \n4            France  1859-05-15  1906-04-19  ...                FR   \n..              ...         ...         ...  ...               ...   \n930  United Kingdom  1951-04-14         NaN  ...               NaN   \n931             NaN  1955-03-01         NaN  ...               NaN   \n932             NaN         NaN         NaN  ...               NaN   \n933             USA  1941-05-31         NaN  ...               NaN   \n934             USA         NaN         NaN  ...               NaN   \n\n    overall_motivation share  \\\n0                  NaN     1   \n1                  NaN     2   \n2                  NaN     2   \n3                  NaN     2   \n4                  NaN     4   \n..                 ...   ...   \n930                NaN     4   \n931                NaN     2   \n932                NaN     2   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation  \\\n0    \"in recognition of the extraordinary services ...   \n1    \"in recognition of the extraordinary service t...   \n2    \"in recognition of the extraordinary service t...   \n3    \"in recognition of the extraordinary services ...   \n4    \"in recognition of the extraordinary services ...   \n..                                                 ...   \n930  \"for the phage display of peptides and antibod...   \n931  \"for their efforts to end the use of sexual vi...   \n932  \"for their efforts to end the use of sexual vi...   \n933  \"for integrating climate change into long-run ...   \n934  \"for integrating technological innovations int...   \n\n                                 born_country_original  \\\n0                                Prussia (now Germany)   \n1                                      the Netherlands   \n2                                      the Netherlands   \n3                                               France   \n4                                               France   \n..                                                 ...   \n930                                     United Kingdom   \n931  Belgian Congo (now Democratic Republic of the ...   \n932                                               Iraq   \n933                                                USA   \n934                                                USA   \n\n         born_city_original died_country_original died_city_original  \\\n0    Lennep (now Remscheid)               Germany             Munich   \n1                    Arnhem       the Netherlands                NaN   \n2                Zonnemaire       the Netherlands          Amsterdam   \n3                     Paris                France                NaN   \n4                     Paris                France              Paris   \n..                      ...                   ...                ...   \n930               Leicester                   NaN                NaN   \n931                  Bukavu                   NaN                NaN   \n932                    Kojo                   NaN                NaN   \n933          Albuquerque NM                   NaN                NaN   \n934               Denver CO                   NaN                NaN   \n\n     city_original country_original  \n0           Munich          Germany  \n1           Leiden  the Netherlands  \n2        Amsterdam  the Netherlands  \n3            Paris           France  \n4            Paris           France  \n..             ...              ...  \n930      Cambridge   United Kingdom  \n931            NaN              NaN  \n932            NaN              NaN  \n933   New Haven CT              USA  \n934    New York NY              USA  \n\n[935 rows x 26 columns]\n      id       firstname   surname  year   category  \\\n68    68       Chen Ning      Yang  1957    Physics   \n69    69       Tsung-Dao       Lee  1957    Physics   \n94    95         Leon N.    Cooper  1972    Physics   \n96    97             Leo     Esaki  1973    Physics   \n97    98            Ivar   Giaever  1973    Physics   \n..   ...             ...       ...   ...        ...   \n928  963      Frances H.    Arnold  2018  Chemistry   \n929  964       George P.     Smith  2018  Chemistry   \n930  965  Sir Gregory P.    Winter  2018  Chemistry   \n933  968      William D.  Nordhaus  2018  Economics   \n934  969         Paul M.     Romer  2018  Economics   \n\n                                      affiliation                 city  \\\n68                   Institute for Advanced Study         Princeton NJ   \n69                            Columbia University          New York NY   \n94                               Brown University        Providence RI   \n96           IBM Thomas J. Watson Research Center  Yorktown Heights NY   \n97                       General Electric Company       Schenectady NY   \n..                                            ...                  ...   \n928  California Institute of Technology (Caltech)          Pasadena CA   \n929                        University of Missouri             Columbia   \n930           MRC Laboratory of Molecular Biology            Cambridge   \n933                               Yale University         New Haven CT   \n934                  NYU Stern School of Business          New York NY   \n\n            country   born_date died_date  ... died_country_code  \\\n68              USA  1922-09-22       NaN  ...               NaN   \n69              USA  1926-11-24       NaN  ...               NaN   \n94              USA  1930-02-28       NaN  ...               NaN   \n96              USA  1925-03-12       NaN  ...               NaN   \n97              USA  1929-04-05       NaN  ...               NaN   \n..              ...         ...       ...  ...               ...   \n928             USA  1956-07-25       NaN  ...               NaN   \n929             USA  1941-03-10       NaN  ...               NaN   \n930  United Kingdom  1951-04-14       NaN  ...               NaN   \n933             USA  1941-05-31       NaN  ...               NaN   \n934             USA         NaN       NaN  ...               NaN   \n\n    overall_motivation share  \\\n68                 NaN     2   \n69                 NaN     2   \n94                 NaN     3   \n96                 NaN     4   \n97                 NaN     4   \n..                 ...   ...   \n928                NaN     2   \n929                NaN     4   \n930                NaN     4   \n933                NaN     2   \n934                NaN     2   \n\n                                            motivation born_country_original  \\\n68   \"for their penetrating investigation of the so...                 China   \n69   \"for their penetrating investigation of the so...                 China   \n94   \"for their jointly developed theory of superco...                   USA   \n96   \"for their experimental discoveries regarding ...                 Japan   \n97   \"for their experimental discoveries regarding ...                Norway   \n..                                                 ...                   ...   \n928            \"for the directed evolution of enzymes\"                   USA   \n929  \"for the phage display of peptides and antibod...                   USA   \n930  \"for the phage display of peptides and antibod...        United Kingdom   \n933  \"for integrating climate change into long-run ...                   USA   \n934  \"for integrating technological innovations int...                   USA   \n\n    born_city_original died_country_original died_city_original  \\\n68        Hofei Anhwei                   NaN                NaN   \n69            Shanghai                   NaN                NaN   \n94         New York NY                   NaN                NaN   \n96               Osaka                   NaN                NaN   \n97              Bergen                   NaN                NaN   \n..                 ...                   ...                ...   \n928      Pittsburgh PA                   NaN                NaN   \n929         Norwalk CT                   NaN                NaN   \n930          Leicester                   NaN                NaN   \n933     Albuquerque NM                   NaN                NaN   \n934          Denver CO                   NaN                NaN   \n\n           city_original country_original  \n68          Princeton NJ              USA  \n69           New York NY              USA  \n94         Providence RI              USA  \n96   Yorktown Heights NY              USA  \n97        Schenectady NY              USA  \n..                   ...              ...  \n928          Pasadena CA              USA  \n929             Columbia              USA  \n930            Cambridge   United Kingdom  \n933         New Haven CT              USA  \n934          New York NY              USA  \n\n[228 rows x 26 columns]\n\n\nQuestions:Where were most Nobel laureates based when they won their prizes? The answer: USA\n\n\n\n\nimport pandas as pd\nurl ='https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ndf = pd.read_csv(url)\nprint(df.head())\ndf\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.info()\nfrom skimpy import clean_columns\ndf = clean_columns(df,case=\"snake\")\nprint(df.columns)\ndf.fillna(\"-\")\ndf.describe()\nsum_table = df.describe().round(2)\nsum_table\ndf.dropna()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nIndex(['passenger_id', 'survived', 'pclass', 'name', 'sex', 'age', 'sib_sp',\n       'parch', 'ticket', 'fare', 'cabin', 'embarked'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n10\n11\n1\n3\nSandstrom, Miss. Marguerite Rut\nfemale\n4.0\n1\n1\nPP 9549\n16.7000\nG6\nS\n\n\n11\n12\n1\n1\nBonnell, Miss. Elizabeth\nfemale\n58.0\n0\n0\n113783\n26.5500\nC103\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n871\n872\n1\n1\nBeckwith, Mrs. Richard Leonard (Sallie Monypeny)\nfemale\n47.0\n1\n1\n11751\n52.5542\nD35\nS\n\n\n872\n873\n0\n1\nCarlsson, Mr. Frans Olof\nmale\n33.0\n0\n0\n695\n5.0000\nB51 B53 B55\nS\n\n\n879\n880\n1\n1\nPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\nfemale\n56.0\n0\n1\n11767\n83.1583\nC50\nC\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n\n\n183 rows × 12 columns"
  },
  {
    "objectID": "homework.html#lectyre04",
    "href": "homework.html#lectyre04",
    "title": "HomeWork",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"all-ages.csv\")\ndf\nresult = df.groupby([\"Major\"]).sum().sort_values([\"Unemployment_rate\"])\nprint(result)\n\n                                            Major_code  \\\nMajor                                                    \nEDUCATIONAL ADMINISTRATION AND SUPERVISION        2301   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING            2411   \nPHARMACOLOGY                                      3607   \nMATERIALS SCIENCE                                 5008   \nMATHEMATICS AND COMPUTER SCIENCE                  4005   \n...                                                ...   \nLIBRARY SCIENCE                                   3501   \nSCHOOL STUDENT COUNSELING                         2303   \nMILITARY TECHNOLOGIES                             3801   \nCLINICAL PSYCHOLOGY                               5202   \nMISCELLANEOUS FINE ARTS                           6099   \n\n                                                                 Major_category  \\\nMajor                                                                             \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                            Education   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                              Engineering   \nPHARMACOLOGY                                             Biology & Life Science   \nMATERIALS SCIENCE                                                   Engineering   \nMATHEMATICS AND COMPUTER SCIENCE                        Computers & Mathematics   \n...                                                                         ...   \nLIBRARY SCIENCE                                                       Education   \nSCHOOL STUDENT COUNSELING                                             Education   \nMILITARY TECHNOLOGIES                       Industrial Arts & Consumer Services   \nCLINICAL PSYCHOLOGY                                    Psychology & Social Work   \nMISCELLANEOUS FINE ARTS                                                    Arts   \n\n                                            Total  Employed  \\\nMajor                                                         \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   4037      3113   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       6264      4120   \nPHARMACOLOGY                                 5015      3481   \nMATERIALS SCIENCE                            7208      5866   \nMATHEMATICS AND COMPUTER SCIENCE             7184      5874   \n...                                           ...       ...   \nLIBRARY SCIENCE                             16193      7091   \nSCHOOL STUDENT COUNSELING                    2396      1492   \nMILITARY TECHNOLOGIES                        4315      1650   \nCLINICAL PSYCHOLOGY                          7638      5128   \nMISCELLANEOUS FINE ARTS                      8511      6431   \n\n                                            Employed_full_time_year_round  \\\nMajor                                                                       \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                           2468   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                               3350   \nPHARMACOLOGY                                                         2579   \nMATERIALS SCIENCE                                                    4505   \nMATHEMATICS AND COMPUTER SCIENCE                                     5039   \n...                                                                   ...   \nLIBRARY SCIENCE                                                      4330   \nSCHOOL STUDENT COUNSELING                                            1093   \nMILITARY TECHNOLOGIES                                                1708   \nCLINICAL PSYCHOLOGY                                                  3297   \nMISCELLANEOUS FINE ARTS                                              3802   \n\n                                            Unemployed  Unemployment_rate  \\\nMajor                                                                       \nEDUCATIONAL ADMINISTRATION AND SUPERVISION           0           0.000000   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING               0           0.000000   \nPHARMACOLOGY                                        57           0.016111   \nMATERIALS SCIENCE                                  134           0.022333   \nMATHEMATICS AND COMPUTER SCIENCE                   150           0.024900   \n...                                                ...                ...   \nLIBRARY SCIENCE                                    743           0.094843   \nSCHOOL STUDENT COUNSELING                          169           0.101746   \nMILITARY TECHNOLOGIES                              187           0.101796   \nCLINICAL PSYCHOLOGY                                587           0.102712   \nMISCELLANEOUS FINE ARTS                           1190           0.156147   \n\n                                            Median  P25th     P75th  \nMajor                                                                \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   58000  44750   79000.0  \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       85000  55000  125000.0  \nPHARMACOLOGY                                 60000  35000  105000.0  \nMATERIALS SCIENCE                            75000  60000  100000.0  \nMATHEMATICS AND COMPUTER SCIENCE             92000  53000  136000.0  \n...                                            ...    ...       ...  \nLIBRARY SCIENCE                              40000  30000   55000.0  \nSCHOOL STUDENT COUNSELING                    41000  33200   50000.0  \nMILITARY TECHNOLOGIES                        64000  39750   90000.0  \nCLINICAL PSYCHOLOGY                          45000  26100   62000.0  \nMISCELLANEOUS FINE ARTS                      45000  30000   60000.0  \n\n[173 rows x 10 columns]\n\n\n\n### 按照专业分组，并把失业率从低到高升序排列\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\n\n\n\n\n\n\n\n\nRank\nMajor_code\nMajor\nTotal\nMen\nWomen\nMajor_category\nShareWomen\nSample_size\nEmployed\n...\nPart_time\nFull_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\nCollege_jobs\nNon_college_jobs\nLow_wage_jobs\n\n\n\n\n0\n1\n2419\nPETROLEUM ENGINEERING\n2339.0\n2057.0\n282.0\nEngineering\n0.120564\n36\n1976\n...\n270\n1207\n37\n0.018381\n110000\n95000\n125000\n1534\n364\n193\n\n\n1\n2\n2416\nMINING AND MINERAL ENGINEERING\n756.0\n679.0\n77.0\nEngineering\n0.101852\n7\n640\n...\n170\n388\n85\n0.117241\n75000\n55000\n90000\n350\n257\n50\n\n\n2\n3\n2415\nMETALLURGICAL ENGINEERING\n856.0\n725.0\n131.0\nEngineering\n0.153037\n3\n648\n...\n133\n340\n16\n0.024096\n73000\n50000\n105000\n456\n176\n0\n\n\n3\n4\n2417\nNAVAL ARCHITECTURE AND MARINE ENGINEERING\n1258.0\n1123.0\n135.0\nEngineering\n0.107313\n16\n758\n...\n150\n692\n40\n0.050125\n70000\n43000\n80000\n529\n102\n0\n\n\n4\n5\n2405\nCHEMICAL ENGINEERING\n32260.0\n21239.0\n11021.0\nEngineering\n0.341631\n289\n25694\n...\n5180\n16697\n1672\n0.061098\n65000\n50000\n75000\n18314\n4440\n972\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n169\n3609\nZOOLOGY\n8409.0\n3050.0\n5359.0\nBiology & Life Science\n0.637293\n47\n6259\n...\n2190\n3602\n304\n0.046320\n26000\n20000\n39000\n2771\n2947\n743\n\n\n169\n170\n5201\nEDUCATIONAL PSYCHOLOGY\n2854.0\n522.0\n2332.0\nPsychology & Social Work\n0.817099\n7\n2125\n...\n572\n1211\n148\n0.065112\n25000\n24000\n34000\n1488\n615\n82\n\n\n170\n171\n5202\nCLINICAL PSYCHOLOGY\n2838.0\n568.0\n2270.0\nPsychology & Social Work\n0.799859\n13\n2101\n...\n648\n1293\n368\n0.149048\n25000\n25000\n40000\n986\n870\n622\n\n\n171\n172\n5203\nCOUNSELING PSYCHOLOGY\n4626.0\n931.0\n3695.0\nPsychology & Social Work\n0.798746\n21\n3777\n...\n965\n2738\n214\n0.053621\n23400\n19200\n26000\n2403\n1245\n308\n\n\n172\n173\n3501\nLIBRARY SCIENCE\n1098.0\n134.0\n964.0\nEducation\n0.877960\n2\n742\n...\n237\n410\n87\n0.104946\n22000\n20000\n22000\n288\n338\n192\n\n\n\n\n173 rows × 21 columns\n\n\n\nresult = df.groupby([“Major”]).sum().sort_values([“ShareWomen”],ascending=False) print(result)\n\n### 按照专业分组，将女生占比从高到低降序排列\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\nresult = df.groupby([\"Major\"]).sum().sort_values([\"ShareWomen\"],ascending=False)\nprint(result)\n\n                                               Rank  Major_code     Total  \\\nMajor                                                                       \nEARLY CHILDHOOD EDUCATION                       165        2307   37589.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   164        6102   38279.0   \nMEDICAL ASSISTING SERVICES                       52        6104   11123.0   \nELEMENTARY EDUCATION                            139        2304  170862.0   \nFAMILY AND CONSUMER SCIENCES                    151        2901   58001.0   \n...                                             ...         ...       ...   \nMINING AND MINERAL ENGINEERING                    2        2416     756.0   \nCONSTRUCTION SERVICES                            27        5601   18498.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      67        2504    4790.0   \nFOOD SCIENCE                                     22        1104       0.0   \nMILITARY TECHNOLOGIES                            74        3801     124.0   \n\n                                                   Men     Women  \\\nMajor                                                              \nEARLY CHILDHOOD EDUCATION                       1167.0   36422.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   1225.0   37054.0   \nMEDICAL ASSISTING SERVICES                       803.0   10320.0   \nELEMENTARY EDUCATION                           13029.0  157833.0   \nFAMILY AND CONSUMER SCIENCES                    5166.0   52835.0   \n...                                                ...       ...   \nMINING AND MINERAL ENGINEERING                   679.0      77.0   \nCONSTRUCTION SERVICES                          16820.0    1678.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     4419.0     371.0   \nFOOD SCIENCE                                       0.0       0.0   \nMILITARY TECHNOLOGIES                            124.0       0.0   \n\n                                                                    Major_category  \\\nMajor                                                                                \nEARLY CHILDHOOD EDUCATION                                                Education   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                               Health   \nMEDICAL ASSISTING SERVICES                                                  Health   \nELEMENTARY EDUCATION                                                     Education   \nFAMILY AND CONSUMER SCIENCES                   Industrial Arts & Consumer Services   \n...                                                                            ...   \nMINING AND MINERAL ENGINEERING                                         Engineering   \nCONSTRUCTION SERVICES                          Industrial Arts & Consumer Services   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                            Engineering   \nFOOD SCIENCE                                       Agriculture & Natural Resources   \nMILITARY TECHNOLOGIES                          Industrial Arts & Consumer Services   \n\n                                               ShareWomen  Sample_size  \\\nMajor                                                                    \nEARLY CHILDHOOD EDUCATION                        0.968954          342   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES    0.967998           95   \nMEDICAL ASSISTING SERVICES                       0.927807           67   \nELEMENTARY EDUCATION                             0.923745         1629   \nFAMILY AND CONSUMER SCIENCES                     0.910933          518   \n...                                                   ...          ...   \nMINING AND MINERAL ENGINEERING                   0.101852            7   \nCONSTRUCTION SERVICES                            0.090713          295   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      0.077453           71   \nFOOD SCIENCE                                     0.000000           36   \nMILITARY TECHNOLOGIES                            0.000000            4   \n\n                                               Employed  Full_time  Part_time  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                         32551      27569       7001   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES     29763      19975      13862   \nMEDICAL ASSISTING SERVICES                         9168       5643       4107   \nELEMENTARY EDUCATION                             149339     123177      37965   \nFAMILY AND CONSUMER SCIENCES                      46624      36747      15872   \n...                                                 ...        ...        ...   \nMINING AND MINERAL ENGINEERING                      640        556        170   \nCONSTRUCTION SERVICES                             16318      15690       1751   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES        4186       4175        247   \nFOOD SCIENCE                                       3149       2558       1121   \nMILITARY TECHNOLOGIES                                 0        111          0   \n\n                                               Full_time_year_round  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                                     20748   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                 14460   \nMEDICAL ASSISTING SERVICES                                     4290   \nELEMENTARY EDUCATION                                          86540   \nFAMILY AND CONSUMER SCIENCES                                  26906   \n...                                                             ...   \nMINING AND MINERAL ENGINEERING                                  388   \nCONSTRUCTION SERVICES                                         12313   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                    3607   \nFOOD SCIENCE                                                   1735   \nMILITARY TECHNOLOGIES                                           111   \n\n                                               Unemployed  Unemployment_rate  \\\nMajor                                                                          \nEARLY CHILDHOOD EDUCATION                            1360           0.040105   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES        1487           0.047584   \nMEDICAL ASSISTING SERVICES                            407           0.042507   \nELEMENTARY EDUCATION                                 7297           0.046586   \nFAMILY AND CONSUMER SCIENCES                         3355           0.067128   \n...                                                   ...                ...   \nMINING AND MINERAL ENGINEERING                         85           0.117241   \nCONSTRUCTION SERVICES                                1042           0.060023   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES           250           0.056357   \nFOOD SCIENCE                                          338           0.096931   \nMILITARY TECHNOLOGIES                                   0           0.000000   \n\n                                               Median  P25th  P75th  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                       28000  21000  35000   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   28000  20000  40000   \nMEDICAL ASSISTING SERVICES                      42000  30000  65000   \nELEMENTARY EDUCATION                            32000  23400  38000   \nFAMILY AND CONSUMER SCIENCES                    30000  22900  40000   \n...                                               ...    ...    ...   \nMINING AND MINERAL ENGINEERING                  75000  55000  90000   \nCONSTRUCTION SERVICES                           50000  36000  60000   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     40000  27000  52000   \nFOOD SCIENCE                                    53000  32000  70000   \nMILITARY TECHNOLOGIES                           40000  40000  40000   \n\n                                               College_jobs  Non_college_jobs  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                             23515              7705   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES         19957              9404   \nMEDICAL ASSISTING SERVICES                             2091              6948   \nELEMENTARY EDUCATION                                 108085             36972   \nFAMILY AND CONSUMER SCIENCES                          20985             20133   \n...                                                     ...               ...   \nMINING AND MINERAL ENGINEERING                          350               257   \nCONSTRUCTION SERVICES                                  3275              5351   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES            1861              2121   \nFOOD SCIENCE                                           1183              1274   \nMILITARY TECHNOLOGIES                                     0                 0   \n\n                                               Low_wage_jobs  \nMajor                                                         \nEARLY CHILDHOOD EDUCATION                               2868  \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES           5125  \nMEDICAL ASSISTING SERVICES                              1270  \nELEMENTARY EDUCATION                                   11502  \nFAMILY AND CONSUMER SCIENCES                            5248  \n...                                                      ...  \nMINING AND MINERAL ENGINEERING                            50  \nCONSTRUCTION SERVICES                                    703  \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES              406  \nFOOD SCIENCE                                             485  \nMILITARY TECHNOLOGIES                                      0  \n\n[173 rows x 20 columns]\n\n\n\n### 按照专业分组，将女生占比从高到低降序排列\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\na=df['Median'].groupby(df['Major_category']).sum()\na.plot.bar()\nplt.show()\n\n\n\n\n\n\n\n\nQuestions:What should I major in?\nThe answer: Engineering"
  },
  {
    "objectID": "homework.html#lectyre05",
    "href": "homework.html#lectyre05",
    "title": "HomeWork",
    "section": "",
    "text": "import pandas as pd\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\ndf = pd.read_csv('plastic-waste.csv')\ndf_clean = df.dropna(subset=['plastic_waste_per_cap', 'continent'])\n\nQuenstion1:Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? The answer:With the histogram, it is possible to observe differences in the per capita distribution of plastic waste across continents. For example, Africa shows a higher peak waste output and North America shows a wider distribution.\n\n# Create histograms faceted by continent\np_histogram = ggplot(df_clean, aes(x='plastic_waste_per_cap')) + \\\n    geom_histogram(bins=30, fill='blue', color='black', alpha=0.7) + \\\n    facet_wrap('continent') + \\\n    ggtitle('Distribution of Plastic Waste per Capita by Continent') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Frequency')\n\n\np_histogram.show()\n\n   \n   \n\n\nQuenstion2:Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots? The answer:Fiddle plots show the complete distribution of the data, showing the shape of the data and multiple peaks in the graph. Box plots provide explicit statistical information that violin plots only reflect through shape.\n\n# Violin plots\np_violin = ggplot(df, aes(x='continent', y='plastic_waste_per_cap', fill='continent')) + \\\n    geom_violin(alpha=0.7) + \\\n    geom_boxplot(width=0.1, fill='white', color='black') + \\\n    ggtitle('Violin Plot of Plastic Waste per Capita by Continent') + \\\n    xlab('Continent') + \\\n    ylab('Plastic Waste per Capita')\n\np_violin.show()\n\n   \n   \n\n\nQuenstion3:Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. The answer:The scatter plot presents the relationship between plastic waste per capita and poorly managed waste per capita, checking if there is a positive or other relationship.\n\n# Scatterplot\nif 'mismanaged_plastic_waste_per_cap' in df.columns:\n    p_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap')) + \\\n        geom_point(size=3, alpha=0.6) + \\\n        ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita') + \\\n        xlab('Plastic Waste per Capita') + \\\n        ylab('Mismanaged Plastic Waste per Capita')\n    \n    p_scatter.show()\n\n   \n   \n\n\nQuenstion4:Colour the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? The answer:By distinguishing continents by color, it is possible to observe differences between continents. Certain continents may exhibit specific patterns or clusters, such as the gradual rise of Africa.\n\n# Colored scatterplot\np_scatter_colored = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita by Continent') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Mismanaged Plastic Waste per Capita')\n\np_scatter_colored.show()\n\n   \n   \n\n\nQuenstion5:Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated? The answer:The visualization of the relationship between the two demographic variables shows the association between plastic waste per capita and total population and coastal population. Through scatterplot analysis, plastic waste per capita exhibits a stronger linear relationship with coastal population.\n\n# Plastic waste per capita vs Total population\np_pop_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='total_pop', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste per Capita vs. Total Population') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Total Population')\n\np_pop_scatter.show()\n\n   \n   \n\n\n\n# Plastic waste per capita vs Coastal population\np_coastal_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='coastal_pop', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste per Capita vs. Coastal Population') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Coastal Population')\n\np_coastal_scatter.show()\n\n   \n   \n\n\n\np_coastal_scatter = ggplot(df, aes(x='coastal_pop', y='plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Coastal Population vs. Plastic Waste per Capita') + \\\n    xlab('coastal pop') + \\\n    ylab('plastic waste per cap')\n\np_coastal_scatter.show()\n\n   \n   \n\n\n\np_coastal_scatter.show()\ndf['coastal_population_proportion'] = df['coastal_pop'] / df['total_pop']\ndf_filtered = df\ndf_filtered = df_filtered[(df_filtered['plastic_waste_per_cap'] &lt;= 0.6) & \n(df_filtered['coastal_population_proportion'] &lt;= 1.6)]\np_scatter = ggplot(df_filtered, aes(x='coastal_population_proportion', y='plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.7) + \\\n    geom_smooth(method='lm', color='black', se=True, linetype='solid', size=1) + \\\n    ggtitle('Plastic Waste per Capita vs Coastal Population Proportion') + \\\n    xlab('Coastal Population Proportion') + \\\n    ylab('Plastic Waste per Capita')\n\n\np_scatter.show()"
  },
  {
    "objectID": "homework.html#practical01",
    "href": "homework.html#practical01",
    "title": "HomeWork",
    "section": "Practical01",
    "text": "Practical01\n\n#%pip install pandas matplotlib numpy pathlib pingouin lets_plot\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\n#import pingouin as pg\nfrom lets_plot import *\n\nLetsPlot.setup_html(no_js=True)\n\n### You don't need to use these settings yourself,\n### they are just here to make the charts look nicer!\n# Set the plot style for prettier charts:\nplt.style.use(\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\nQuestions:Explain in your own words what temperature ‘anomalies’ are. Why have researchers chosen this particular measure over other measures (such as absolute temperature)? The answer: (1)A temperature “anomaly” is typically defined as a temperature that falls outside the normal range or expected value, suggesting the potential for an underlying issue or change. In contrast, absolute temperature represents the average kinetic energy of molecular motion and is expressed in Kelvins (K). (2)Researchers select temperature “anomalies” over absolute temperatures due to their greater practicality, intuitiveness, ease of comprehension, suitability for comparison with normal conditions, and adaptability to diverse research fields and objectives.\n\ndf = pd.read_csv(\"NH.Ts+dSST.csv\", skiprows=1)\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.53\n-0.23\n-0.30\n-0.05\n-0.18\n-0.22\n-0.25\n-0.24\n-0.30\n-0.43\n-0.42\n-0.30\n0.00\n0.00\n-0.20\n-0.22\n-0.32\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.28\n-0.44\n-0.37\n-0.24\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.25\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.33\n-0.68\n-0.21\n-0.17\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.57\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.44\n-0.15\n-0.29\n-0.33\n-0.64\n-0.23\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.41\n-0.41\n-0.52\n-0.45\n-0.44\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.49\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     145 non-null    float64\n 11  Nov     145 non-null    float64\n 12  Dec     145 non-null    float64\n 13  J-D     145 non-null    float64\n 14  D-N     145 non-null    float64\n 15  DJF     145 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     145 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\ndf = pd.read_csv(\"NH.Ts+dSST.csv\", skiprows=1)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     145 non-null    float64\n 11  Nov     145 non-null    float64\n 12  Dec     145 non-null    float64\n 13  J-D     145 non-null    float64\n 14  D-N     145 non-null    float64\n 15  DJF     145 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     145 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\nprint(df.head())\n\n   Year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov  \\\n0  1880 -0.39 -0.53 -0.23 -0.30 -0.05 -0.18 -0.22 -0.25 -0.24 -0.30 -0.43   \n1  1881 -0.31 -0.25 -0.06 -0.02  0.05 -0.34  0.09 -0.06 -0.28 -0.44 -0.37   \n2  1882  0.25  0.21  0.02 -0.30 -0.23 -0.29 -0.28 -0.15 -0.25 -0.52 -0.33   \n3  1883 -0.57 -0.66 -0.15 -0.30 -0.26 -0.12 -0.06 -0.23 -0.34 -0.17 -0.44   \n4  1884 -0.16 -0.11 -0.64 -0.59 -0.36 -0.41 -0.41 -0.52 -0.45 -0.44 -0.58   \n\n    Dec   J-D   D-N   DJF   MAM   JJA   SON  \n0 -0.42 -0.30  0.00  0.00 -0.20 -0.22 -0.32  \n1 -0.24 -0.19 -0.20 -0.33 -0.01 -0.10 -0.37  \n2 -0.68 -0.21 -0.17  0.08 -0.17 -0.24 -0.37  \n3 -0.15 -0.29 -0.33 -0.64 -0.23 -0.14 -0.32  \n4 -0.47 -0.43 -0.40 -0.14 -0.53 -0.45 -0.49  \n\n\nQuestions:Try importing the data again without using the keyword argument option na_values=“***” at all and see what difference it makes. The answer: By comparing the output of.info(), you can see the difference in importing the data without parameters. Some columns may be recognized as object instead of float64 or int64, which usually means they contain non-numeric characters.\n\ndf = pd.read_csv(\"NH.Ts+dSST.csv\", skiprows=1)\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.53\n-0.23\n-0.30\n-0.05\n-0.18\n-0.22\n-0.25\n-0.24\n-0.30\n-0.43\n-0.42\n-0.30\n0.00\n0.00\n-0.20\n-0.22\n-0.32\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.28\n-0.44\n-0.37\n-0.24\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.25\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.33\n-0.68\n-0.21\n-0.17\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.57\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.44\n-0.15\n-0.29\n-0.33\n-0.64\n-0.23\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.41\n-0.41\n-0.52\n-0.45\n-0.44\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.49\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     145 non-null    float64\n 11  Nov     145 non-null    float64\n 12  Dec     145 non-null    float64\n 13  J-D     145 non-null    float64\n 14  D-N     145 non-null    float64\n 15  DJF     145 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     145 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\ndf = df.set_index(\"Year\")\ndf.head()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1880\n-0.39\n-0.53\n-0.23\n-0.30\n-0.05\n-0.18\n-0.22\n-0.25\n-0.24\n-0.30\n-0.43\n-0.42\n-0.30\n0.00\n0.00\n-0.20\n-0.22\n-0.32\n\n\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.28\n-0.44\n-0.37\n-0.24\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n1882\n0.25\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.33\n-0.68\n-0.21\n-0.17\n0.08\n-0.17\n-0.24\n-0.37\n\n\n1883\n-0.57\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.44\n-0.15\n-0.29\n-0.33\n-0.64\n-0.23\n-0.14\n-0.32\n\n\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.41\n-0.41\n-0.52\n-0.45\n-0.44\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.49\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020\n1.59\n1.69\n1.66\n1.39\n1.27\n1.14\n1.10\n1.12\n1.19\n1.21\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.33\n\n\n2021\n1.25\n0.96\n1.20\n1.13\n1.05\n1.21\n1.07\n1.02\n1.05\n1.29\n1.29\n1.17\n1.14\n1.14\n1.13\n1.13\n1.10\n1.21\n\n\n2022\n1.24\n1.16\n1.41\n1.09\n1.02\n1.13\n1.06\n1.17\n1.15\n1.31\n1.09\n1.06\n1.16\n1.17\n1.19\n1.17\n1.12\n1.19\n\n\n2023\n1.29\n1.29\n1.64\n1.01\n1.13\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.50\n1.43\n1.22\n1.26\n1.40\n1.84\n\n\n2024\n1.67\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.58\n1.72\n1.90\n0.00\n0.00\n1.67\n1.82\n1.67\n1.46\n1.73\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf[\"Jan\"].plot(ax=ax)\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.show()\nplt.savefig(\"name-of-chart.pdf\")\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n&lt;Figure size 1800x900 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nExtra practice:Extra practice: The columns labelled DJF, MAM, JJA, and SON contain seasonal averages (means). For example, the MAM column contains the average of the March, April, and May columns for each year. Plot a separate line chart for each season, using average temperature anomaly for that season on the vertical axis and time (from 1880 to the latest year available) on the horizontal axis. The answer:\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"red\")\nax.annotate(\"1951—1980 average\",  xy=(1.96, -1.5),xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\n\nmonth = \"MAM\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\n\nmonth = \"JJA\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\nQuestions:What do your charts from Questions 2 to 4(a) suggest about the relationship between temperature and time? The answer: Temperature increases with time.\n\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951-1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880-{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\")\n\nText(0, 0.5, 'Annual temperature anomalies')\n\n\n\n\n\n\n\n\n\nQuestions:Discuss the similarities and differences between the charts. (For example, are the horizontal and vertical axes variables the same, or do the lines have the same shape?) The answer: (1)Similaritie:The temperature changes with time, and the overall trend is upward. (2)Differences:The horizontal and vertical axis variables of the two tables are different. For example, in 2000, Figure 1.4 shows a temperature of 0.5 and Figure 1.5 shows a temperature of 0.6. Questions:Looking at the behaviour of temperature over time from 1000 to 1900 in Figure 1.4, are the observed patterns in your chart unusual? The answer:From 1000 to 1900, the temperature fluctuate up and down with time, but the maximum value did not exceed 0.0, which indicates that the temperature change during this period was relatively small compared to the significant increase in industrial temperature after 1900, which I think is normal.\nQuestions:Based on your answers to Questions 4 and 5, do you think the government should be concerned about climate change? The answer:My point of view is that according to the chart data, the temperature is rising, which is a sign of global warming, so the government should pay attention to climate change.\n\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.22\n      Aug   -0.25\n1881  Jun   -0.34\n      Jul    0.09\ndtype: float64\n\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.18\n\n\n3\n1951\nApr\n0.06\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.10\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.21\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.80\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"Season\", 0: \"Values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nSeason\nValues\nPeriod\n\n\n\n\n445\n1991\nMAM\n0.45\n1981—2010\n\n\n446\n1991\nJJA\n0.42\n1981—2010\n\n\n447\n1991\nSON\n0.32\n1981—2010\n\n\n448\n1992\nDJF\n0.43\n1981—2010\n\n\n449\n1992\nMAM\n0.29\n1981—2010\n\n\n450\n1992\nJJA\n-0.04\n1981—2010\n\n\n451\n1992\nSON\n-0.15\n1981—2010\n\n\n452\n1993\nDJF\n0.37\n1981—2010\n\n\n453\n1993\nMAM\n0.31\n1981—2010\n\n\n454\n1993\nJJA\n0.12\n1981—2010\n\n\n\n\n\n\n\nQuestions:Calculate the mean (average) and variance separately for the following time periods: 1921–1950, 1951–1980, and 1981–2010. The answer:The variance of the later period is significantly higher than that of the earlier period, which indicates that the air temperature becomes more variable.\n\nseasons = {\n    \"DJF\": [\"Dec\", \"Jan\", \"Feb\"],\n    \"MAM\": [\"Mar\", \"Apr\", \"May\"],\n    \"JJA\": [\"Jun\", \"Jul\", \"Aug\"],\n    \"SON\": [\"Sep\", \"Oct\", \"Nov\"]\n}\nfor season, months in seasons.items():\n    if all(month in df.columns for month in months):\n        df[season] = df[months].mean(axis=1)\nperiods = {\n    \"1921-1950\": (1921, 1950),\n    \"1951-1980\": (1951, 1980),\n    \"1981-2010\": (1981, 2010)\n}\nresults = {}\nfor season in seasons.keys():\n    if season in df.columns:\n        results[season] = {}\n        for period, (start_year, end_year) in periods.items():\n            period_data = df.loc[start_year:end_year, season]\n            results[season][period] = {\n                \"mean\": period_data.mean(),\n                \"variance\": period_data.var(),\n            }\nfor season, period_results in results.items():\n    print(f\"Season: {season}\")\n    for period, stats in period_results.items():\n        print(f\"  Period: {period}\")\n        print(f\"    Mean: {stats['mean']:.2f}\")\n        print(f\"    Variance: {stats['variance']:.2f}\")\n    print()\n\nSeason: DJF\n  Period: 1921-1950\n    Mean: -0.03\n    Variance: 0.05\n  Period: 1951-1980\n    Mean: -0.00\n    Variance: 0.04\n  Period: 1981-2010\n    Mean: 0.53\n    Variance: 0.07\n\nSeason: MAM\n  Period: 1921-1950\n    Mean: -0.05\n    Variance: 0.03\n  Period: 1951-1980\n    Mean: -0.00\n    Variance: 0.03\n  Period: 1981-2010\n    Mean: 0.51\n    Variance: 0.08\n\nSeason: JJA\n  Period: 1921-1950\n    Mean: -0.06\n    Variance: 0.02\n  Period: 1951-1980\n    Mean: 0.00\n    Variance: 0.01\n  Period: 1981-2010\n    Mean: 0.40\n    Variance: 0.07\n\nSeason: SON\n  Period: 1921-1950\n    Mean: 0.08\n    Variance: 0.03\n  Period: 1951-1980\n    Mean: -0.00\n    Variance: 0.03\n  Period: 1981-2010\n    Mean: 0.43\n    Variance: 0.11\n\n\n\n\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"Season\", 0: \"Values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nSeason\nValues\nPeriod\n\n\n\n\n445\n1991\nMAM\n0.450000\n1981—2010\n\n\n446\n1991\nJJA\n0.416667\n1981—2010\n\n\n447\n1991\nSON\n0.320000\n1981—2010\n\n\n448\n1992\nDJF\n0.440000\n1981—2010\n\n\n449\n1992\nMAM\n0.293333\n1981—2010\n\n\n450\n1992\nJJA\n-0.046667\n1981—2010\n\n\n451\n1992\nSON\n-0.150000\n1981—2010\n\n\n452\n1993\nDJF\n0.376667\n1981—2010\n\n\n453\n1993\nMAM\n0.310000\n1981—2010\n\n\n454\n1993\nJJA\n0.120000\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n    [np.mean, np.var]\n)\ngrp_mean_var\n\nC:\\Users\\45202\\AppData\\Local\\Temp\\ipykernel_31480\\1563140002.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\nC:\\Users\\45202\\AppData\\Local\\Temp\\ipykernel_31480\\1563140002.py:1: FutureWarning: The provided callable &lt;function mean at 0x000001F3E3577E20&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  grp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\nC:\\Users\\45202\\AppData\\Local\\Temp\\ipykernel_31480\\1563140002.py:1: FutureWarning: The provided callable &lt;function var at 0x000001F3E35800E0&gt; is currently using SeriesGroupBy.var. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"var\" instead.\n  grp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nSeason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.026782\n0.051532\n\n\n1951—1980\n-0.000444\n0.036994\n\n\n1981—2010\n0.528556\n0.072543\n\n\nJJA\n1921—1950\n-0.053218\n0.021588\n\n\n1951—1980\n0.000556\n0.014572\n\n\n1981—2010\n0.400667\n0.068148\n\n\nMAM\n1921—1950\n-0.041034\n0.031272\n\n\n1951—1980\n-0.000333\n0.025462\n\n\n1981—2010\n0.509000\n0.076248\n\n\nSON\n1921—1950\n0.083563\n0.028007\n\n\n1951—1980\n-0.000111\n0.025743\n\n\n1981—2010\n0.429556\n0.111079\n\n\n\n\n\n\n\n\nmin_year = 1880\n(\n    ggplot(temp_all_months, aes(x=\"Year\", y=\"Values\", color=\"Season\"))\n    + geom_abline(slope=0, color=\"black\", size=1)\n    + geom_line(size=1)\n    + labs(\n        title=f\"Average annual temperature anomaly in \\n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\",\n        y=\"Annual temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n    + geom_text(\n        x=min_year, y=0.1, label=\"1951—1980 average\", hjust=\"left\", color=\"black\"\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n              \n            \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                    1951—1980 average\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1880\n              \n            \n          \n          \n            \n            \n            \n              \n                1900\n              \n            \n          \n          \n            \n            \n            \n              \n                1920\n              \n            \n          \n          \n            \n            \n            \n              \n                1940\n              \n            \n          \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2020\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -1.0\n              \n            \n          \n          \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.5\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n          \n            \n              \n                1.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Average annual temperature anomaly in \n      \n      \n         in the northern hemisphere (1880—2024)\n      \n    \n    \n      \n        Annual temperature anomalies\n      \n    \n    \n      \n        Year\n      \n    \n    \n      \n      \n      \n        \n          \n            Season\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                DJF\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                MAM\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                JJA\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                SON\n              \n            \n          \n        \n      \n    \n    \n    \n  \n  \n  \n\n\n\nQuestions:Using the findings of the New York Times article and your answers to Questions 1 to 5, discuss whether temperature appears to be more variable over time. Would you advise the government to spend more money on mitigating the effects of extreme weather events? The answer:As temperatures change more over time due to global warming, heat extremes are becoming more frequent and damaging. First, I suggest that the government should spend more money to alleviate the impact of extreme weather events on people’s lives, such as expanding the urban green area. Second, we will advocate low-carbon travel and life for the people.\n\ndf_co2 = pd.read_csv(r\"1_C02-data.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Jun\", y=\"Trend\"))\n    + geom_point(color=\"black\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.000000\n0.915419\n\n\nTrend\n0.915419\n1.000000\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Year\", y=\"Jun\"))\n    + geom_line(size=1)\n    + labs(\n        title=\"June temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1970\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                1990\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2010\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.6\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n        \n      \n    \n    \n      \n        June temperature anomalies\n      \n    \n    \n      \n        Jun\n      \n    \n    \n      \n        Year\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\nbase_plot = ggplot(df_temp_co2) + scale_x_continuous(format=\"d\")\nplot_p = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Jun\"), size=1)\n    + labs(title=\"June temperature anomalies\")\n)\nplot_q = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Trend\"), size=1)\n    + labs(title=\"Carbon dioxide emissions\")\n)\ngggrid([plot_p, plot_q], ncol=2)\n\n\n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  0.0\n                \n              \n            \n            \n              \n                \n                  0.2\n                \n              \n            \n            \n              \n                \n                  0.4\n                \n              \n            \n            \n              \n                \n                  0.6\n                \n              \n            \n            \n              \n                \n                  0.8\n                \n              \n            \n            \n              \n                \n                  1.0\n                \n              \n            \n          \n        \n      \n      \n        \n          June temperature anomalies\n        \n      \n      \n        \n          Jun\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  320\n                \n              \n            \n            \n              \n                \n                  340\n                \n              \n            \n            \n              \n                \n                  360\n                \n              \n            \n            \n              \n                \n                  380\n                \n              \n            \n            \n              \n                \n                  400\n                \n              \n            \n          \n        \n      \n      \n        \n          Carbon dioxide emissions\n        \n      \n      \n        \n          Trend\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n\n\n\nExtra practice: Choose two months and add the CO2 trend data to the temperature dataset from Part 1.1, making sure that the data corresponds to the correct year. Create a separate chart for each month. The answer:\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 3]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n12\n1959\n3\n316.71\n316.71\n315.62\n\n\n24\n1960\n3\n317.58\n317.58\n316.49\n\n\n36\n1961\n3\n318.54\n318.54\n317.47\n\n\n48\n1962\n3\n319.68\n319.68\n318.57\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Mar\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nMar\nTrend\n\n\n\n\n0\n1958\n0.16\n314.62\n\n\n1\n1959\n0.33\n315.62\n\n\n2\n1960\n-0.40\n316.49\n\n\n3\n1961\n0.22\n317.47\n\n\n4\n1962\n0.30\n318.57\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Mar\", y=\"Trend\"))\n    + geom_point(color=\"red\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n            \n            \n            \n              \n                1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                2.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 9]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n6\n1958\n9\n313.20\n313.20\n315.91\n\n\n18\n1959\n9\n313.84\n313.84\n316.55\n\n\n30\n1960\n9\n314.16\n314.16\n316.87\n\n\n42\n1961\n9\n314.80\n314.80\n317.49\n\n\n54\n1962\n9\n316.26\n316.26\n319.03\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Sep\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nSep\nTrend\n\n\n\n\n0\n1958\n0.07\n315.91\n\n\n1\n1959\n0.13\n316.55\n\n\n2\n1960\n0.12\n316.87\n\n\n3\n1961\n-0.03\n317.49\n\n\n4\n1962\n-0.02\n319.03\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Sep\", y=\"Trend\"))\n    + geom_point(color=\"blue\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n            \n            \n            \n              \n                1.2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                330\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                350\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                370\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                390\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\nQuestions:What do your charts and the correlation coefficients suggest about the relationship between CO2 levels and temperature anomalies?\nThe answer:CO2 levels and temperature have strongly correlated with each other. Questions:Consider the example of spurious correlation described above. Questions:(1)In your own words, explain spurious correlation and the difference between correlation and causation.\nThe answer:Spurious correlation: When two things seem linked but aren’t really, often due to a hidden factor.Correlation vs causation: Correlation shows a link, but causation means one thing causes another.\nQuestions:(2)Give an example of spurious correlation, similar to the one above, for either CO2 levels or temperature anomalies.\nThe answer: Example: CO2 Levels and Stock Market Performance.It might seem that there’s a correlation between rising CO2 levels in the atmosphere and improved stock market performance. However, this doesn’t mean that CO2 levels are directly causing the stock market to rise. Instead, both could be influenced by a common factor, such as economic growth. As economies grow, they often emit more CO2 and also tend to have better stock market performance.\nQuestions:(3)Choose an example of spurious correlation from Tyler Vigen’s website. Explain whether you think it is a coincidence, or whether this correlation could be due to one or more other variables.\nThe answer:An example is the correlation between the number of Nicolas Cage films released in a year and the number of people who die by falling into swimming pools.Is it a coincidence?Yes, it is likely a coincidence.Could it be due to one or more other variables?It could be due to the fact that both of these events are influenced by broader societal trends or random fluctuations that are not directly related to each other. For instance, the number of Nicolas Cage films released might be influenced by the film industry’s production schedule, while the number of swimming pool accidents could be influenced by factors such as weather conditions, safety regulations, and public awareness. There is no plausible mechanism through which the release of Nicolas Cage films could cause an increase in swimming pool accidents, or vice versa. Therefore, it is reasonable to conclude that this correlation is spurious and due to chance or other unobserved variables."
  },
  {
    "objectID": "homework.html#practical02",
    "href": "homework.html#practical02",
    "title": "HomeWork",
    "section": "Practical02",
    "text": "Practical02\n\n#%pip install openpyxl\nimport pandas as pd\ndata_np = pd.read_excel(\n    r\"doing-economics-datafile-working-in-excel-project-2 (1).xlsx\",\n    usecols=\"A:Q\",\n    header=1,\n    index_col=\"Period\",\n)\ndata_n = data_np.iloc[:10, :].copy()\ndata_p = data_np.iloc[14:24, :].copy()\ndata_n.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 1 to 10\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Copenhagen       10 non-null     object\n 1   Dnipropetrovs’k  10 non-null     object\n 2   Minsk            10 non-null     object\n 3   St. Gallen       10 non-null     object\n 4   Muscat           10 non-null     object\n 5   Samara           10 non-null     object\n 6   Zurich           10 non-null     object\n 7   Boston           10 non-null     object\n 8   Bonn             10 non-null     object\n 9   Chengdu          10 non-null     object\n 10  Seoul            10 non-null     object\n 11  Riyadh           10 non-null     object\n 12  Nottingham       10 non-null     object\n 13  Athens           10 non-null     object\n 14  Istanbul         10 non-null     object\n 15  Melbourne        10 non-null     object\ndtypes: object(16)\nmemory usage: 1.3+ KB\n\n\nE:\\phthon\\python_install3.11.19\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n  warn(msg)\n\n\n\ndata_n = data_n.astype(\"double\")\ndata_p = data_p.astype(\"double\")\n\nQuenstion:(a)Calculate the mean contribution in each period (row) separately for both experiments.\n(b)Plot a line chart of mean contribution on the vertical axis and time period (from 1 to 10) on the horizontal axis (with a separate line for each experiment). Make sure the lines in the legend are clearly labelled according to the experiment (with punishment or without punishment).\n(c)Describe any differences and similarities you see in the mean contribution over time in both experiments. The anwser:In the two experiments, the average contributions over time changed. The difference was that the average value without penalty gradually increased and was always higher than the average value with penalty, and the average value without penalty gradually decreased with the development of time.\n\nimport numpy as np\n\nmean_n_c = data_n.mean(axis=1)\nmean_p_c = data_p.agg(np.mean, axis=1)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nmean_n_c.plot(ax=ax, label=\"Without punishment\")\nmean_p_c.plot(ax=ax, label=\"With punishment\")\nax.set_title(\"Average contributions to the public goods game\")\nax.set_ylabel(\"Average contribution\")\nax.legend()\n\nC:\\Users\\45202\\AppData\\Local\\Temp\\ipykernel_31480\\1486267859.py:4: FutureWarning: The provided callable &lt;function mean at 0x000001F3E3577E20&gt; is currently using DataFrame.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  mean_p_c = data_p.agg(np.mean, axis=1)\n\n\n\n\n\n\n\n\n\n\n# Create new dataframe with bars in\ncompare_grps = pd.DataFrame(\n    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],\n    index=[\"Without punishment\", \"With punishment\"],\n)\n# Rename columns to have 'round' in them\ncompare_grps.columns = [\"Round \" + str(i) for i in compare_grps.columns]\n# Swap the column and index variables around with the transpose function, ready for plotting (.T is transpose)\ncompare_grps = compare_grps.T\n# Make a bar chart\ncompare_grps.plot.bar(rot=0)\n\n\n\n\n\n\n\n\n\nn_c = data_n.agg([\"std\", \"var\", \"mean\"], 1)\nn_c\n\n\n\n\n\n\n\n\nstd\nvar\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n1\n2.020724\n4.083325\n10.578313\n\n\n2\n2.238129\n5.009220\n10.628398\n\n\n3\n2.329569\n5.426891\n10.407079\n\n\n4\n2.068213\n4.277504\n9.813033\n\n\n5\n2.108329\n4.445049\n9.305433\n\n\n6\n2.240881\n5.021549\n8.454844\n\n\n7\n2.136614\n4.565117\n7.837568\n\n\n8\n2.349442\n5.519880\n7.376388\n\n\n9\n2.413845\n5.826645\n6.392985\n\n\n10\n2.187126\n4.783520\n4.383769\n\n\n\n\n\n\n\n\np_c = data_p.agg([\"std\", \"var\", \"mean\"], 1)\nfig, ax = plt.subplots()\nn_c[\"mean\"].plot(ax=ax, label=\"mean\")\n\n(n_c[\"mean\"] + 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n\n(n_c[\"mean\"] - 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_n.columns)):\n    ax.scatter(x=data_n.index, y=data_n.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\np_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(p_c[\"mean\"] + 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(p_c[\"mean\"] - 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_p.columns)):\n    ax.scatter(x=data_p.index, y=data_p.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game with punishment\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndata_p.apply(lambda x: x.max() - x.min(), axis=1)\n\nPeriod\n1     10.199675\n2     12.185065\n3     12.689935\n4     12.625000\n5     12.140375\n6     12.827541\n7     13.098931\n8     13.482621\n9     13.496754\n10    11.307360\ndtype: float64\n\n\n\n# A lambda function accepting three inputs, a, b, and c, and calculating the sum of the squares\ntest_function = lambda a, b, c: a**2 + b**2 + c**2\n\n\n# Now we apply the function by handing over (in parenthesis) the following inputs: a=3, b=4 and c=5\ntest_function(3, 4, 5)\n\n50\n\n\n\nrange_function = lambda x: x.max() - x.min()\nrange_p = data_p.apply(range_function, axis=1)\nrange_n = data_n.apply(range_function, axis=1)\nfig, ax = plt.subplots()\nrange_p.plot(ax=ax, label=\"With punishment\")\nrange_n.plot(ax=ax, label=\"Without punishment\")\nax.set_ylim(0, None)\nax.legend()\nax.set_title(\"Range of contributions to the public goods game\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfuncs_to_apply = [range_function, \"max\", \"min\", \"std\", \"mean\"]\nsumm_p = data_p.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\nsumm_n = data_n.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\nsumm_n.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n6.14\n14.10\n7.96\n2.02\n10.58\n\n\n10\n7.38\n8.68\n1.30\n2.19\n4.38\n\n\n\n\n\n\n\n\nsumm_p.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n10.20\n16.02\n5.82\n3.21\n10.64\n\n\n10\n11.31\n17.51\n6.20\n3.90\n12.87\n\n\n\n\n\n\n\n\nimport pingouin as pg\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :])\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.063782\n30\ntwo-sided\n0.949567\n[-2.0, 1.87]\n0.02255\n0.337\n0.050437\n\n\n\n\n\n\n\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :], paired=True)\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.149959\n15\ntwo-sided\n0.882795\n[-0.92, 0.8]\n0.02255\n0.258\n0.05082"
  },
  {
    "objectID": "homework.html#practical03",
    "href": "homework.html#practical03",
    "title": "HomeWork",
    "section": "Practical03",
    "text": "Practical03\n\nPractical03-01\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)\n\n\n\n\n\n\n\n\nrownames\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\n\n0\n1\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n1\n2\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n2\n3\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n3\n4\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n4\n5\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n5\n6\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n6\n7\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\nNaN\n25\n1011\nNaN\nNaN\n\n\n7\n8\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\nNaN\n30\n1006\nNaN\nNaN\n\n\n8\n9\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\nNaN\n35\n1004\nNaN\nNaN\n\n\n9\n10\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\nNaN\n40\n1002\nNaN\nNaN\n\n\n\n\n\n\n\n\nurl = \"http://aeturrell.com/research\"\npage = requests.get(url)\npage.text[:300]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.6.39\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n&lt;meta name=\"author\" content=\"Arthur Turrell\"&gt;\\n'\n\n\n\nsoup = BeautifulSoup(page.text, \"html.parser\")\nprint(soup.prettify()[60000:60500])\n\nTJDdmFjYW5jaWVzJTJDQ09WSUQtMTk=\" data-index=\"1\" data-listing-date-modified-sort=\"NaN\" data-listing-date-sort=\"1651359600000\" data-listing-file-modified-sort=\"1687564711698\" data-listing-reading-time-sort=\"1\" data-listing-word-count-sort=\"182\"&gt;\n         &lt;div class=\"project-content listing-pub-info\"&gt;\n          &lt;p&gt;\n           Draca, Mirko, Emma Duchini, Roland Rathelot, Arthur Turrell, and Giulia Vattuone. Revolution in Progress? The Rise of Remote Work in the UK.\n           &lt;i&gt;\n            Univers\n\n\n\n# Get all paragraphs\nall_paras = soup.find_all(\"p\")\n# Just show one of the paras\nall_paras[1]\n\n&lt;p&gt;Blundell, Jack, Emma Duchini, Stefania Simion, and Arthur Turrell. \"Pay transparency and gender equality.\" &lt;i&gt;American Economic Journal: Economic Policy&lt;/i&gt; (2024). doi: &lt;a href=\"https://www.aeaweb.org/articles?id=10.1257/pol.20220766&amp;from=f\"&gt;&lt;code&gt;10.1257/pol.20220766&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n\n\nall_paras[1].text\n\n'Blundell, Jack, Emma Duchini, Stefania Simion, and Arthur Turrell. \"Pay transparency and gender equality.\" American Economic Journal: Economic Policy (2024). doi: 10.1257/pol.20220766'\n\n\n\nprojects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\nprojects = [x.text.strip() for x in projects]\nprojects[:4]\n\n['Blundell, Jack, Emma Duchini, Stefania Simion, and Arthur Turrell. \"Pay transparency and gender equality.\" American Economic Journal: Economic Policy (2024). doi: 10.1257/pol.20220766',\n 'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013']\n\n\n\ndf_list = pd.read_html(\n    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n)\n# Retrieve first and only entry from list of dataframes\ndf = df_list[0]\ndf.head()\n\n\n\n\n\n\n\n\nYears\nHosts\nWinners\nScore\nRunner's-up\nThird place\nScore.1\nFourth place\n\n\n\n\n0\n1930 Details\nUruguay\nUruguay\n4 - 2\nArgentina\nUnited States\n[note 1]\nYugoslavia\n\n\n1\n1934 Details\nItaly\nItaly\n2 - 1\nCzechoslovakia\nGermany\n3 - 2\nAustria\n\n\n2\n1938 Details\nFrance\nItaly\n4 - 2\nHungary\nBrazil\n4 - 2\nSweden\n\n\n3\n1950 Details\nBrazil\nUruguay\n2 - 1\nBrazil\nSweden\n[note 2]\nSpain\n\n\n4\n1954 Details\nSwitzerland\nWest Germany\n3 - 2\nHungary\nAustria\n3 - 1\nUruguay\n\n\n\n\n\n\n\n\n\nPractical03-02\n\n#pip install requests\n#pip install html5lib\n#pip install bs4\n#pip install pandas\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n# printing movie details with its rating.\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n##.......##\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\n\nPractical03-03\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n \n# 定义请求的 URL 和 headers\nurl = \"https://movie.douban.com/top250\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\n \n# 发送 GET 请求\nresponse = requests.get(url, headers=headers)\nresponse.encoding = 'utf-8'  # 设置编码方式\nhtml_content = response.text  # 获取网页的 HTML 内容\n \n# 使用 Beautiful Soup 解析 HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n \n# 提取电影名称、描述、评分和评价人数\nmovies = []\nfor item in soup.find_all('div', class_='item'):\n    title = item.find('span', class_='title').get_text()  # 电影名称\n    description = item.find('span', class_='inq')  # 电影描述\n    rating = item.find('span', class_='rating_num').get_text()  # 评分\n    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n    \n    # 如果没有描述，将其置为空字符串\n    if description:\n        description = description.get_text()\n    else:\n        description = ''\n    \n    movie = {\n        \"title\": title,\n        \"description\": description,\n        \"rating\": rating,\n        \"votes\": votes.replace('人评价', '').strip()\n    }\n    movies.append(movie)\n \n# 将数据保存到 CSV 文件\nwith open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = ['title', 'description', 'rating', 'votes']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n \n    writer.writeheader()  # 写入表头\n    for movie in movies:\n        writer.writerow(movie)  # 写入每一行数据\n \nprint(\"save success douban_top250.csv\")\n\nsave success douban_top250.csv"
  },
  {
    "objectID": "homework.html#practical04",
    "href": "homework.html#practical04",
    "title": "HomeWork",
    "section": "Practical04",
    "text": "Practical04\n```vdvhuyta echo=False from bs4 import BeautifulSoup import re\nimport urllib.request, urllib.error # certain URL import xlwt # excel operation\ndef main(): baseurl = “https://movie.douban.com/top250?start=” datalist = getdata(baseurl) savepath = “douban_top250.csv” savedata(datalist, savepath)"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfood = pd.read_csv('en.openfoodfacts.org.products.tsv',sep='\\t')\n\nC:\\Users\\45202\\AppData\\Local\\Temp\\ipykernel_24396\\1762735028.py:3: DtypeWarning: Columns (0,3,5,19,20,24,25,26,27,28,36,37,38,39,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  food = pd.read_csv('en.openfoodfacts.org.products.tsv',sep='\\t')\n\n\n\n\n\n\nfood.head()\n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\nlast_modified_t\nlast_modified_datetime\nproduct_name\ngeneric_name\nquantity\n...\nfruits-vegetables-nuts_100g\nfruits-vegetables-nuts-estimate_100g\ncollagen-meat-protein-ratio_100g\ncocoa_100g\nchlorophyl_100g\ncarbon-footprint_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\nglycemic-index_100g\nwater-hardness_100g\n\n\n\n\n0\n3087\nhttp://world-en.openfoodfacts.org/product/0000...\nopenfoodfacts-contributors\n1474103866\n2016-09-17T09:17:46Z\n1474103893\n2016-09-17T09:18:13Z\nFarine de blé noir\nNaN\n1kg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n4530\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nBanana Chips Sweetened (Whole)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n14.0\n14.0\nNaN\nNaN\n\n\n2\n4559\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nPeanuts\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\n\n\n3\n16087\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055731\n2017-03-09T10:35:31Z\n1489055731\n2017-03-09T10:35:31Z\nOrganic Salted Nut Mix\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n12.0\nNaN\nNaN\n\n\n4\n16094\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055653\n2017-03-09T10:34:13Z\n1489055653\n2017-03-09T10:34:13Z\nOrganic Polenta\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 163 columns\n\n\n\n\n\n\n\nfood.shape[0]\n\n356027\n\n\n\n\n\n\nfood.shape[1]\nfood.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 356027 entries, 0 to 356026\nColumns: 163 entries, code to water-hardness_100g\ndtypes: float64(107), object(56)\nmemory usage: 442.8+ MB\n\n\n\n\n\n\nfood.columns\n\nIndex(['code', 'url', 'creator', 'created_t', 'created_datetime',\n       'last_modified_t', 'last_modified_datetime', 'product_name',\n       'generic_name', 'quantity',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n       'nutrition-score-uk_100g', 'glycemic-index_100g',\n       'water-hardness_100g'],\n      dtype='object', length=163)\n\n\n\n\n\n\nfood.columns[104]\n\n'-glucose_100g'\n\n\n\n\n\n\nfood.dtypes[food.columns[104]]\n\ndtype('float64')\n\n\n\n\n\n\nfood.index\n\nRangeIndex(start=0, stop=356027, step=1)\n\n\n\n\n\n\nfood.values[18][7]\n\n'Lotus Organic Brown Jasmine Rice'\n\n\n\n\n\n\n\n\n\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n\n\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\n\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\n\n\n# Solution 1\nchipo.info()\n\n# Solution 2\n\nchipo.shape\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n(4622, 5)\n\n\n\n\n\n\nchipo.shape[1]\n\n5\n\n\n\n\n\n\nchipo.head(0)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n\n\n\n\n\n\n\n\n\nchipo.index\n\nRangeIndex(start=0, stop=4622, step=1)\n\n\n\n\n\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\n\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\n\nchipo.groupby(by=“choice_description”).sum().sort_values(‘quantity’,ascending=False).head(1) #### Step 12. How many items were orderd in total?\n\nchipo.item_name.count()\n\nnp.int64(4622)\n\n\n\n\n\n\n#### Step 13.a. Check the item price type\nchipo.item_price.dtype\n#### Step 13.b. Create a lambda function and change the type of item price\ndollarizer = lambda x: float(x[1:-1])\nchipo.item_price = chipo.item_price.apply(dollarizer)\n#### Step 13.c. Check the item price type\nchipo.item_price.dtype\n\ndtype('float64')\n\n\n\n\n\n\nrevenue =  (chipo.item_price * chipo.quantity).sum()\nprint('Revenue is : $ '+ str(revenue))\n\nRevenue is : $ 39237.02\n\n\n\n\n\n\nchipo.order_id.value_counts().count()\n\nnp.int64(1834)\n\n\n\n\n\n\n# Solution 1\n\nchipo['revenue'] = chipo['quantity'] * chipo['item_price']\norder_grouped = chipo.groupby(by=['order_id']).sum()\norder_grouped['revenue'].mean()\n# Solution 2\n\nchipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\nnp.float64(21.39423118865867)\n\n\n\n\n\n\nchipo.item_name.value_counts().count()\n\nnp.int64(50)\n\n\n\n\n\n\n\n\n\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\n\n\nimport pandas as pd\n\n\n\n\n\n\n\n\nusers = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n                      sep='|', index_col='user_id')\n\n\n\n\n\nusers.head(25)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n1\n24\nM\ntechnician\n85711\n\n\n2\n53\nF\nother\n94043\n\n\n3\n23\nM\nwriter\n32067\n\n\n4\n24\nM\ntechnician\n43537\n\n\n5\n33\nF\nother\n15213\n\n\n6\n42\nM\nexecutive\n98101\n\n\n7\n57\nM\nadministrator\n91344\n\n\n8\n36\nM\nadministrator\n05201\n\n\n9\n29\nM\nstudent\n01002\n\n\n10\n53\nM\nlawyer\n90703\n\n\n11\n39\nF\nother\n30329\n\n\n12\n28\nF\nother\n06405\n\n\n13\n47\nM\neducator\n29206\n\n\n14\n45\nM\nscientist\n55106\n\n\n15\n49\nF\neducator\n97301\n\n\n16\n21\nM\nentertainment\n10309\n\n\n17\n30\nM\nprogrammer\n06355\n\n\n18\n35\nF\nother\n37212\n\n\n19\n40\nM\nlibrarian\n02138\n\n\n20\n42\nF\nhomemaker\n95660\n\n\n21\n26\nM\nwriter\n30068\n\n\n22\n25\nM\nwriter\n40206\n\n\n23\n30\nF\nartist\n48197\n\n\n24\n21\nF\nartist\n94533\n\n\n25\n39\nM\nengineer\n55107\n\n\n\n\n\n\n\n\n\n\n\nusers.tail(10)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n934\n61\nM\nengineer\n22902\n\n\n935\n42\nM\ndoctor\n66221\n\n\n936\n24\nM\nother\n32789\n\n\n937\n48\nM\neducator\n98072\n\n\n938\n38\nF\ntechnician\n55038\n\n\n939\n26\nF\nstudent\n33319\n\n\n940\n32\nM\nadministrator\n02215\n\n\n941\n20\nM\nstudent\n97229\n\n\n942\n48\nF\nlibrarian\n78209\n\n\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n\n\n\n\n\n\nusers.shape[0]\n\n943\n\n\n\n\n\n\nusers.shape[1]\n\n4\n\n\n\n\n\n\nusers.columns\n\nIndex(['age', 'gender', 'occupation', 'zip_code'], dtype='object')\n\n\n\n\n\n\n# \"the index\" (aka \"the labels\")\nusers.index\n\nIndex([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       934, 935, 936, 937, 938, 939, 940, 941, 942, 943],\n      dtype='int64', name='user_id', length=943)\n\n\n\n\n\n\nusers.dtypes\n\nage            int64\ngender        object\noccupation    object\nzip_code      object\ndtype: object\n\n\n\n\n\n\nusers.occupation\n\n#or\n\nusers['occupation']\n\nuser_id\n1         technician\n2              other\n3             writer\n4         technician\n5              other\n           ...      \n939          student\n940    administrator\n941          student\n942        librarian\n943          student\nName: occupation, Length: 943, dtype: object\n\n\n\n\n\n\nusers.occupation.nunique()\n#or by using value_counts() which returns the count of unique elements\n#users.occupation.value_counts().count()\n\n21\n\n\n\n\n\n\n#Because \"most\" is asked\nusers.occupation.value_counts().head(1).index[0]\n\n#or\n#to have the top 5\n\n# users.occupation.value_counts().head()\n\n'student'\n\n\n\n\n\n\nusers.describe() #Notice: by default, only the numeric columns are returned.\n\n\n\n\n\n\n\n\nage\n\n\n\n\ncount\n943.000000\n\n\nmean\n34.051962\n\n\nstd\n12.192740\n\n\nmin\n7.000000\n\n\n25%\n25.000000\n\n\n50%\n31.000000\n\n\n75%\n43.000000\n\n\nmax\n73.000000\n\n\n\n\n\n\n\n\n\n\n\nusers.describe(include = \"all\") #Notice: By default, only the numeric columns are returned.\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\n\n\ncount\n943.000000\n943\n943\n943\n\n\nunique\nNaN\n2\n21\n795\n\n\ntop\nNaN\nM\nstudent\n55414\n\n\nfreq\nNaN\n670\n196\n9\n\n\nmean\n34.051962\nNaN\nNaN\nNaN\n\n\nstd\n12.192740\nNaN\nNaN\nNaN\n\n\nmin\n7.000000\nNaN\nNaN\nNaN\n\n\n25%\n25.000000\nNaN\nNaN\nNaN\n\n\n50%\n31.000000\nNaN\nNaN\nNaN\n\n\n75%\n43.000000\nNaN\nNaN\nNaN\n\n\nmax\n73.000000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\nusers.occupation.describe()\n\ncount         943\nunique         21\ntop       student\nfreq          196\nName: occupation, dtype: object\n\n\n\n\n\n\nround(users.age.mean())\n\n34\n\n\n\n\n\n\nusers.age.value_counts().tail() #7, 10, 11, 66 and 73 years -&gt; only 1 occurrence\n\nage\n7     1\n11    1\n66    1\n10    1\n73    1\nName: count, dtype: int64"
  },
  {
    "objectID": "labs.html#ex1_world-food-facts",
    "href": "labs.html#ex1_world-food-facts",
    "title": "Labs",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfood = pd.read_csv('en.openfoodfacts.org.products.tsv',sep='\\t')\n\nC:\\Users\\45202\\AppData\\Local\\Temp\\ipykernel_24396\\1762735028.py:3: DtypeWarning: Columns (0,3,5,19,20,24,25,26,27,28,36,37,38,39,48) have mixed types. Specify dtype option on import or set low_memory=False.\n  food = pd.read_csv('en.openfoodfacts.org.products.tsv',sep='\\t')\n\n\n\n\n\n\nfood.head()\n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\nlast_modified_t\nlast_modified_datetime\nproduct_name\ngeneric_name\nquantity\n...\nfruits-vegetables-nuts_100g\nfruits-vegetables-nuts-estimate_100g\ncollagen-meat-protein-ratio_100g\ncocoa_100g\nchlorophyl_100g\ncarbon-footprint_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\nglycemic-index_100g\nwater-hardness_100g\n\n\n\n\n0\n3087\nhttp://world-en.openfoodfacts.org/product/0000...\nopenfoodfacts-contributors\n1474103866\n2016-09-17T09:17:46Z\n1474103893\n2016-09-17T09:18:13Z\nFarine de blé noir\nNaN\n1kg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n4530\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nBanana Chips Sweetened (Whole)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n14.0\n14.0\nNaN\nNaN\n\n\n2\n4559\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nPeanuts\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\n\n\n3\n16087\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055731\n2017-03-09T10:35:31Z\n1489055731\n2017-03-09T10:35:31Z\nOrganic Salted Nut Mix\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n12.0\nNaN\nNaN\n\n\n4\n16094\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055653\n2017-03-09T10:34:13Z\n1489055653\n2017-03-09T10:34:13Z\nOrganic Polenta\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 163 columns\n\n\n\n\n\n\n\nfood.shape[0]\n\n356027\n\n\n\n\n\n\nfood.shape[1]\nfood.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 356027 entries, 0 to 356026\nColumns: 163 entries, code to water-hardness_100g\ndtypes: float64(107), object(56)\nmemory usage: 442.8+ MB\n\n\n\n\n\n\nfood.columns\n\nIndex(['code', 'url', 'creator', 'created_t', 'created_datetime',\n       'last_modified_t', 'last_modified_datetime', 'product_name',\n       'generic_name', 'quantity',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n       'nutrition-score-uk_100g', 'glycemic-index_100g',\n       'water-hardness_100g'],\n      dtype='object', length=163)\n\n\n\n\n\n\nfood.columns[104]\n\n'-glucose_100g'\n\n\n\n\n\n\nfood.dtypes[food.columns[104]]\n\ndtype('float64')\n\n\n\n\n\n\nfood.index\n\nRangeIndex(start=0, stop=356027, step=1)\n\n\n\n\n\n\nfood.values[18][7]\n\n'Lotus Organic Brown Jasmine Rice'"
  },
  {
    "objectID": "labs.html#ex2_chipotle",
    "href": "labs.html#ex2_chipotle",
    "title": "Labs",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n\n\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\n\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\n\n\n# Solution 1\nchipo.info()\n\n# Solution 2\n\nchipo.shape\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n(4622, 5)\n\n\n\n\n\n\nchipo.shape[1]\n\n5\n\n\n\n\n\n\nchipo.head(0)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n\n\n\n\n\n\n\n\n\nchipo.index\n\nRangeIndex(start=0, stop=4622, step=1)\n\n\n\n\n\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\n\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\n\nchipo.groupby(by=“choice_description”).sum().sort_values(‘quantity’,ascending=False).head(1) #### Step 12. How many items were orderd in total?\n\nchipo.item_name.count()\n\nnp.int64(4622)\n\n\n\n\n\n\n#### Step 13.a. Check the item price type\nchipo.item_price.dtype\n#### Step 13.b. Create a lambda function and change the type of item price\ndollarizer = lambda x: float(x[1:-1])\nchipo.item_price = chipo.item_price.apply(dollarizer)\n#### Step 13.c. Check the item price type\nchipo.item_price.dtype\n\ndtype('float64')\n\n\n\n\n\n\nrevenue =  (chipo.item_price * chipo.quantity).sum()\nprint('Revenue is : $ '+ str(revenue))\n\nRevenue is : $ 39237.02\n\n\n\n\n\n\nchipo.order_id.value_counts().count()\n\nnp.int64(1834)\n\n\n\n\n\n\n# Solution 1\n\nchipo['revenue'] = chipo['quantity'] * chipo['item_price']\norder_grouped = chipo.groupby(by=['order_id']).sum()\norder_grouped['revenue'].mean()\n# Solution 2\n\nchipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\nnp.float64(21.39423118865867)\n\n\n\n\n\n\nchipo.item_name.value_counts().count()\n\nnp.int64(50)"
  },
  {
    "objectID": "labs.html#ex3_occupation",
    "href": "labs.html#ex3_occupation",
    "title": "Labs",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\n\n\nimport pandas as pd\n\n\n\n\n\n\n\n\nusers = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n                      sep='|', index_col='user_id')\n\n\n\n\n\nusers.head(25)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n1\n24\nM\ntechnician\n85711\n\n\n2\n53\nF\nother\n94043\n\n\n3\n23\nM\nwriter\n32067\n\n\n4\n24\nM\ntechnician\n43537\n\n\n5\n33\nF\nother\n15213\n\n\n6\n42\nM\nexecutive\n98101\n\n\n7\n57\nM\nadministrator\n91344\n\n\n8\n36\nM\nadministrator\n05201\n\n\n9\n29\nM\nstudent\n01002\n\n\n10\n53\nM\nlawyer\n90703\n\n\n11\n39\nF\nother\n30329\n\n\n12\n28\nF\nother\n06405\n\n\n13\n47\nM\neducator\n29206\n\n\n14\n45\nM\nscientist\n55106\n\n\n15\n49\nF\neducator\n97301\n\n\n16\n21\nM\nentertainment\n10309\n\n\n17\n30\nM\nprogrammer\n06355\n\n\n18\n35\nF\nother\n37212\n\n\n19\n40\nM\nlibrarian\n02138\n\n\n20\n42\nF\nhomemaker\n95660\n\n\n21\n26\nM\nwriter\n30068\n\n\n22\n25\nM\nwriter\n40206\n\n\n23\n30\nF\nartist\n48197\n\n\n24\n21\nF\nartist\n94533\n\n\n25\n39\nM\nengineer\n55107\n\n\n\n\n\n\n\n\n\n\n\nusers.tail(10)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n934\n61\nM\nengineer\n22902\n\n\n935\n42\nM\ndoctor\n66221\n\n\n936\n24\nM\nother\n32789\n\n\n937\n48\nM\neducator\n98072\n\n\n938\n38\nF\ntechnician\n55038\n\n\n939\n26\nF\nstudent\n33319\n\n\n940\n32\nM\nadministrator\n02215\n\n\n941\n20\nM\nstudent\n97229\n\n\n942\n48\nF\nlibrarian\n78209\n\n\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n\n\n\n\n\n\nusers.shape[0]\n\n943\n\n\n\n\n\n\nusers.shape[1]\n\n4\n\n\n\n\n\n\nusers.columns\n\nIndex(['age', 'gender', 'occupation', 'zip_code'], dtype='object')\n\n\n\n\n\n\n# \"the index\" (aka \"the labels\")\nusers.index\n\nIndex([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       934, 935, 936, 937, 938, 939, 940, 941, 942, 943],\n      dtype='int64', name='user_id', length=943)\n\n\n\n\n\n\nusers.dtypes\n\nage            int64\ngender        object\noccupation    object\nzip_code      object\ndtype: object\n\n\n\n\n\n\nusers.occupation\n\n#or\n\nusers['occupation']\n\nuser_id\n1         technician\n2              other\n3             writer\n4         technician\n5              other\n           ...      \n939          student\n940    administrator\n941          student\n942        librarian\n943          student\nName: occupation, Length: 943, dtype: object\n\n\n\n\n\n\nusers.occupation.nunique()\n#or by using value_counts() which returns the count of unique elements\n#users.occupation.value_counts().count()\n\n21\n\n\n\n\n\n\n#Because \"most\" is asked\nusers.occupation.value_counts().head(1).index[0]\n\n#or\n#to have the top 5\n\n# users.occupation.value_counts().head()\n\n'student'\n\n\n\n\n\n\nusers.describe() #Notice: by default, only the numeric columns are returned.\n\n\n\n\n\n\n\n\nage\n\n\n\n\ncount\n943.000000\n\n\nmean\n34.051962\n\n\nstd\n12.192740\n\n\nmin\n7.000000\n\n\n25%\n25.000000\n\n\n50%\n31.000000\n\n\n75%\n43.000000\n\n\nmax\n73.000000\n\n\n\n\n\n\n\n\n\n\n\nusers.describe(include = \"all\") #Notice: By default, only the numeric columns are returned.\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\n\n\ncount\n943.000000\n943\n943\n943\n\n\nunique\nNaN\n2\n21\n795\n\n\ntop\nNaN\nM\nstudent\n55414\n\n\nfreq\nNaN\n670\n196\n9\n\n\nmean\n34.051962\nNaN\nNaN\nNaN\n\n\nstd\n12.192740\nNaN\nNaN\nNaN\n\n\nmin\n7.000000\nNaN\nNaN\nNaN\n\n\n25%\n25.000000\nNaN\nNaN\nNaN\n\n\n50%\n31.000000\nNaN\nNaN\nNaN\n\n\n75%\n43.000000\nNaN\nNaN\nNaN\n\n\nmax\n73.000000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\nusers.occupation.describe()\n\ncount         943\nunique         21\ntop       student\nfreq          196\nName: occupation, dtype: object\n\n\n\n\n\n\nround(users.age.mean())\n\n34\n\n\n\n\n\n\nusers.age.value_counts().tail() #7, 10, 11, 66 and 73 years -&gt; only 1 occurrence\n\nage\n7     1\n11    1\n66    1\n10    1\n73    1\nName: count, dtype: int64"
  },
  {
    "objectID": "labs.html#ex1_chipotle",
    "href": "labs.html#ex1_chipotle",
    "title": "Labs",
    "section": "Ex1_Chipotle",
    "text": "Ex1_Chipotle\n\nEx1 - Filtering and Sorting Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n\nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. How many products cost more than $10.00?\n\n# clean the item_price column and transform it in a float\n# 清理[item_price]列的数据，将其转换成float类型。\n# float(value[1 : -1])表示字符串切片，将第2位至最后一位截取出来，这里作用是将价格最前面的$符号过滤掉，只保留后面的数字。\n# value的取值就是循环取后面[item_price]列的所有值，全部转换成浮点数，保存至prices这个列表里。\nprices = [float(value[1 : -1]) for value in chipo.item_price]\n\n# reassign the column with the cleaned prices\n# 重新将清理过后的数据赋值给[item_price]列。\nchipo.item_price = prices\n\n# delete the duplicates in item_name and quantity\n# 删除掉[item_name]列与[quantity]列中的重复项。\n# 后面跟的item_name、quantity、choice_description表示参考的列名，这里表示一行里这参考的三列都重复就删除这一行。默认参考所有列。\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity','choice_description'])\n\n# chipo_filtered\n\n# select only the products with quantity equals to 1\n# 筛选出数量为1的商品。\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\nchipo_one_prod\n\n# 方法一：\n# 使用nunique()获取指定坐轴中不同元素的数量。\n# 这里显示价格大于10的商品的数量。\nchipo_one_prod[chipo_one_prod['item_price']&gt;10].item_name.nunique()\n# 这里显示所有价格大于10的商品，返回一个DataFrame。\nchipo_one_prod[chipo_one_prod['item_price']&gt;10]\n\n\n# 方法二： \n# 直接使用query()函数查询。函数作用是使用布尔表达式来查询DataFrame的列，最后返回的DataFrame类型的查询结果。\n# 这里使用'item_price &gt; 10'这个表达式，最后得到一个[item_price]列的值都大于10的DataFrame。再使用item_name.nunique()获取商品名称并得到名称去重之后的数量。\nchipo.query('item_price &gt; 10').item_name.nunique()\n\n31\n\n\n\n\nStep 5. What is the price of each item?\n\n###### print a data frame with only two columns item_name and item_price\n# delete the duplicates in item_name and quantity\n# 删除[item_name]与[quantity]中的重复项。\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity'])\n\n# []里的是筛选条件，这里是筛选出[item_name]的值为'Chicken Bowl'，并且[quantity]的值为1的数据。\nchipo[(chipo['item_name'] == 'Chicken Bowl') & (chipo['quantity'] == 1)]\n\n# select only the products with quantity equals to 1\n# 筛选出数量为1的商品。\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\n\n# select only the item_name and item_price columns\n# 将[item_name]与[item_price]这两列单独筛选出来。\nprice_per_item = chipo_one_prod[['item_name', 'item_price']]\n\n# sort the values from the most to less expensive\n# 按照价格从高到底排列。\n# 使用sort_values()函数进行排序，by表示排序要参考的列，ascending=False表示降序排序，默认升序排序。\nprice_per_item.sort_values(by = \"item_price\", ascending = False).head(20)\n\n\n\n\n\n\n\n\nitem_name\nitem_price\n\n\n\n\n1132\nCarnitas Salad Bowl\n11.89\n\n\n1229\nBarbacoa Salad Bowl\n11.89\n\n\n606\nSteak Salad Bowl\n11.89\n\n\n39\nBarbacoa Bowl\n11.75\n\n\n7\nSteak Burrito\n11.75\n\n\n168\nBarbacoa Crispy Tacos\n11.75\n\n\n57\nVeggie Burrito\n11.25\n\n\n62\nVeggie Bowl\n11.25\n\n\n186\nVeggie Salad Bowl\n11.25\n\n\n738\nVeggie Soft Tacos\n11.25\n\n\n250\nChicken Salad\n10.98\n\n\n5\nChicken Bowl\n10.98\n\n\n8\nSteak Soft Tacos\n9.25\n\n\n92\nSteak Crispy Tacos\n9.25\n\n\n554\nCarnitas Crispy Tacos\n9.25\n\n\n237\nCarnitas Soft Tacos\n9.25\n\n\n56\nBarbacoa Soft Tacos\n9.25\n\n\n33\nCarnitas Bowl\n8.99\n\n\n664\nSteak Salad\n8.99\n\n\n21\nBarbacoa Burrito\n8.99\n\n\n\n\n\n\n\n\n\nStep 6. Sort by the name of the item\n\nchipo.item_name.sort_values()\n\n# OR\n\nchipo.sort_values(by = \"item_name\")\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3389\n1360\n2\n6 Pack Soft Drink\n[Diet Coke]\n12.98\n\n\n341\n148\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n1849\n749\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n1860\n754\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n2713\n1076\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n...\n...\n...\n...\n...\n...\n\n\n2384\n948\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa, [Fajita Vegetables,...\n8.75\n\n\n781\n322\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Black Beans, Cheese, Sou...\n8.75\n\n\n2851\n1132\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa (Medium), [Black Bea...\n8.49\n\n\n1699\n688\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n11.25\n\n\n1395\n567\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa (Mild), [Pinto Beans, Rice...\n8.49\n\n\n\n\n4622 rows × 5 columns\n\n\n\n\n\nStep 7. What was the quantity of the most expensive item ordered?\n\nchipo.sort_values(by = \"item_price\", ascending = False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3598\n1443\n15\nChips and Fresh Tomato Salsa\nNaN\n44.25\n\n\n\n\n\n\n\n\n\nStep 8. How many times was a Veggie Salad Bowl ordered?\n\nchipo_salad = chipo[chipo.item_name == \"Veggie Salad Bowl\"]\n# chipo_salad = chipo.query('item_name == \"Veggie Salad Bowl\"')\n\nlen(chipo_salad)\n\n18\n\n\n\n\nStep 9. How many times did someone order more than one Canned Soda?\n\nchipo_drink_steak_bowl = chipo[(chipo.item_name == \"Canned Soda\") & (chipo.quantity &gt; 1)]\n# chipo_drink_steak_bowl = chipo.query('item_name == \"Canned Soda\" & quantity &gt; 1')\n\nlen(chipo_drink_steak_bowl)\n\n20"
  },
  {
    "objectID": "labs.html#ex2_euro12",
    "href": "labs.html#ex2_euro12",
    "title": "Labs",
    "section": "Ex2_Euro12",
    "text": "Ex2_Euro12\n\nEx2 - Filtering and Sorting Data\nThis time we are going to pull data directly from the internet.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called euro12.\n\neuro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv', sep=',')\neuro12\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n13\n81.3%\n41\n62\n2\n9\n0\n9\n9\n16\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n9\n60.1%\n53\n73\n8\n7\n0\n11\n11\n19\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n10\n66.7%\n25\n38\n8\n4\n0\n7\n7\n15\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n22\n88.1%\n43\n45\n6\n5\n0\n11\n11\n16\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n6\n54.6%\n36\n51\n5\n6\n0\n11\n11\n19\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n20\n74.1%\n101\n89\n16\n16\n0\n18\n18\n19\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n12\n70.6%\n35\n30\n3\n5\n0\n7\n7\n15\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n6\n66.7%\n48\n56\n3\n7\n1\n7\n7\n17\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n10\n71.5%\n73\n90\n10\n12\n0\n14\n14\n16\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n17\n65.4%\n43\n51\n11\n6\n1\n10\n10\n17\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n10\n77.0%\n34\n43\n4\n6\n0\n7\n7\n16\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n8\n61.6%\n35\n51\n7\n7\n0\n9\n9\n18\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n13\n76.5%\n48\n31\n4\n5\n0\n9\n9\n18\n\n\n\n\n16 rows × 35 columns\n\n\n\n\n\nStep 4. Select only the Goal column.\n\neuro12.Goals\n\n0      4\n1      4\n2      4\n3      5\n4      3\n5     10\n6      5\n7      6\n8      2\n9      2\n10     6\n11     1\n12     5\n13    12\n14     5\n15     2\nName: Goals, dtype: int64\n\n\n\n\nStep 5. How many team participated in the Euro2012?\n\neuro12.shape[0]\n\n16\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\neuro12.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 35 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Team                        16 non-null     object \n 1   Goals                       16 non-null     int64  \n 2   Shots on target             16 non-null     int64  \n 3   Shots off target            16 non-null     int64  \n 4   Shooting Accuracy           16 non-null     object \n 5   % Goals-to-shots            16 non-null     object \n 6   Total shots (inc. Blocked)  16 non-null     int64  \n 7   Hit Woodwork                16 non-null     int64  \n 8   Penalty goals               16 non-null     int64  \n 9   Penalties not scored        16 non-null     int64  \n 10  Headed goals                16 non-null     int64  \n 11  Passes                      16 non-null     int64  \n 12  Passes completed            16 non-null     int64  \n 13  Passing Accuracy            16 non-null     object \n 14  Touches                     16 non-null     int64  \n 15  Crosses                     16 non-null     int64  \n 16  Dribbles                    16 non-null     int64  \n 17  Corners Taken               16 non-null     int64  \n 18  Tackles                     16 non-null     int64  \n 19  Clearances                  16 non-null     int64  \n 20  Interceptions               16 non-null     int64  \n 21  Clearances off line         15 non-null     float64\n 22  Clean Sheets                16 non-null     int64  \n 23  Blocks                      16 non-null     int64  \n 24  Goals conceded              16 non-null     int64  \n 25  Saves made                  16 non-null     int64  \n 26  Saves-to-shots ratio        16 non-null     object \n 27  Fouls Won                   16 non-null     int64  \n 28  Fouls Conceded              16 non-null     int64  \n 29  Offsides                    16 non-null     int64  \n 30  Yellow Cards                16 non-null     int64  \n 31  Red Cards                   16 non-null     int64  \n 32  Subs on                     16 non-null     int64  \n 33  Subs off                    16 non-null     int64  \n 34  Players Used                16 non-null     int64  \ndtypes: float64(1), int64(29), object(5)\nmemory usage: 4.5+ KB\n\n\n\n\nStep 7. View only the columns Team, Yellow Cards and Red Cards and assign them to a dataframe called discipline\n\n# filter only giving the column names\n\ndiscipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]\ndiscipline\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n2\nDenmark\n4\n0\n\n\n3\nEngland\n5\n0\n\n\n4\nFrance\n6\n0\n\n\n5\nGermany\n4\n0\n\n\n6\nGreece\n9\n1\n\n\n7\nItaly\n16\n0\n\n\n8\nNetherlands\n5\n0\n\n\n9\nPoland\n7\n1\n\n\n10\nPortugal\n12\n0\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n12\nRussia\n6\n0\n\n\n13\nSpain\n11\n0\n\n\n14\nSweden\n7\n0\n\n\n15\nUkraine\n5\n0\n\n\n\n\n\n\n\n\n\nStep 8. Sort the teams by Red Cards, then to Yellow Cards\n\ndiscipline.sort_values(['Red Cards', 'Yellow Cards'], ascending = False)\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n6\nGreece\n9\n1\n\n\n9\nPoland\n7\n1\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n7\nItaly\n16\n0\n\n\n10\nPortugal\n12\n0\n\n\n13\nSpain\n11\n0\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n14\nSweden\n7\n0\n\n\n4\nFrance\n6\n0\n\n\n12\nRussia\n6\n0\n\n\n3\nEngland\n5\n0\n\n\n8\nNetherlands\n5\n0\n\n\n15\nUkraine\n5\n0\n\n\n2\nDenmark\n4\n0\n\n\n5\nGermany\n4\n0\n\n\n\n\n\n\n\n\n\nStep 9. Calculate the mean Yellow Cards given per Team\n\nround(discipline['Yellow Cards'].mean())\n\n7\n\n\n\n\nStep 10. Filter teams that scored more than 6 goals\n\neuro12[euro12.Goals &gt; 6]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 11. Select the teams that start with G\n\neuro12[euro12.Team.str.startswith('G')]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 12. Select the first 7 columns\n\n# use .iloc to slices via the position of the passed integers\n# : means all, 0:7 means from 0 to 7\n\neuro12.iloc[: , 0:7]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n\n\n\n\n\n\n\n\n\nStep 13. Select all columns except the last 3.\n\n# use negative to exclude the last 3 columns\n\neuro12.iloc[: , :-3]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nClean Sheets\nBlocks\nGoals conceded\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n0\n10\n3\n13\n81.3%\n41\n62\n2\n9\n0\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n1\n10\n6\n9\n60.1%\n53\n73\n8\n7\n0\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n1\n10\n5\n10\n66.7%\n25\n38\n8\n4\n0\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n2\n29\n3\n22\n88.1%\n43\n45\n6\n5\n0\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n1\n7\n5\n6\n54.6%\n36\n51\n5\n6\n0\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n1\n11\n6\n10\n62.6%\n63\n49\n12\n4\n0\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n1\n23\n7\n13\n65.1%\n67\n48\n12\n9\n1\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n2\n18\n7\n20\n74.1%\n101\n89\n16\n16\n0\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n0\n9\n5\n12\n70.6%\n35\n30\n3\n5\n0\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n0\n8\n3\n6\n66.7%\n48\n56\n3\n7\n1\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n2\n11\n4\n10\n71.5%\n73\n90\n10\n12\n0\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n0\n23\n9\n17\n65.4%\n43\n51\n11\n6\n1\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n0\n8\n3\n10\n77.0%\n34\n43\n4\n6\n0\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n5\n8\n1\n15\n93.8%\n102\n83\n19\n11\n0\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n1\n12\n5\n8\n61.6%\n35\n51\n7\n7\n0\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n0\n4\n4\n13\n76.5%\n48\n31\n4\n5\n0\n\n\n\n\n16 rows × 32 columns\n\n\n\n\n\nStep 14. Present only the Shooting Accuracy from England, Italy and Russia\n\n# .loc is another way to slice, using the labels of the columns and indexes\n\neuro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']]\n\n\n\n\n\n\n\n\nTeam\nShooting Accuracy\n\n\n\n\n3\nEngland\n50.0%\n\n\n7\nItaly\n43.0%\n\n\n12\nRussia\n22.5%"
  },
  {
    "objectID": "labs.html#ex1_chipotle-1",
    "href": "labs.html#ex1_chipotle-1",
    "title": "Labs",
    "section": "Ex1_Chipotle",
    "text": "Ex1_Chipotle\n\nVisualizing Chipotle’s Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set this so the graphs open internally\n%matplotlib inline\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. Create a histogram of the top 5 items bought\n\n# get the Series of the names\nx = chipo.item_name\n\n# use the Counter class from collections to create a dictionary with keys(text) and frequency\nletter_counts = Counter(x)\n\n# convert the dictionary to a DataFrame\ndf = pd.DataFrame.from_dict(letter_counts, orient='index')\n\n# sort the values from the top to the least value and slice the first 5 items\ndf = df[0].sort_values(ascending = True)[45:50]\n\n# create the plot\ndf.plot(kind='bar')\n\n# Set the title and labels\nplt.xlabel('Items')\nplt.ylabel('Number of Times Ordered')\nplt.title('Most ordered Chipotle\\'s Items')\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 6. Create a scatterplot with the number of items orderered per order price\n\n\nHint: Price should be in the X-axis and Items ordered in the Y-axis\n\n# create a list of prices\nchipo.item_price = [float(value[1:-1]) for value in chipo.item_price] # strip the dollar sign and trailing space\n\n# then groupby the orders and sum\norders = chipo.groupby('order_id').sum()\n\n# creates the scatterplot\n# plt.scatter(orders.quantity, orders.item_price, s = 50, c = 'green')\nplt.scatter(x = orders.item_price, y = orders.quantity, s = 50, c = 'green')\n\n# Set the title and labels\nplt.xlabel('Order Price')\nplt.ylabel('Items ordered')\nplt.title('Number of items ordered per order price')\nplt.ylim(0)\n\n\n\n\n\n\n\n\n\n\nStep 7. BONUS: Create a question and a graph to answer your own question.\n\n# Question: What is the distribution of order prices?\n# Create a histogram of order prices\nplt.hist(orders.item_price, bins=20, color='blue', edgecolor='black')\n\n# Set the title and labels\nplt.xlabel('Order Price')\nplt.ylabel('Frequency')\nplt.title('Distribution of Order Prices')\n\n# Show the plot\n\nText(0.5, 1.0, 'Distribution of Order Prices')"
  },
  {
    "objectID": "labs.html#ex2_scores",
    "href": "labs.html#ex2_scores",
    "title": "Labs",
    "section": "Ex2_Scores",
    "text": "Ex2_Scores\n\nScores\n\n\nIntroduction:\nThis time you will create the data.\nExercise based on Chris Albon work, the credits belong to him.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\nStep 2. Create the DataFrame that should look like the one below.\n\nraw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n            'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], \n            'female': [0, 1, 1, 0, 1],\n            'age': [42, 52, 36, 24, 73], \n            'preTestScore': [4, 24, 31, 2, 3],\n            'postTestScore': [25, 94, 57, 62, 70]}\n\ndf = pd.DataFrame(raw_data)\ndf\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nfemale\nage\npreTestScore\npostTestScore\n\n\n\n\n0\nJason\nMiller\n0\n42\n4\n25\n\n\n1\nMolly\nJacobson\n1\n52\n24\n94\n\n\n2\nTina\nAli\n1\n36\n31\n57\n\n\n3\nJake\nMilner\n0\n24\n2\n62\n\n\n4\nAmy\nCooze\n1\n73\n3\n70\n\n\n\n\n\n\n\n\n\nStep 3. Create a Scatterplot of preTestScore and postTestScore, with the size of each point determined by age\n\n#### Hint: Don't forget to place the labels\ndf.plot.scatter(x='preTestScore' , y='postTestScore'  , s=df['age'].values)\n\n\n\n\n\n\n\n\n\n\nStep 4. Create a Scatterplot of preTestScore and postTestScore.\n\n### This time the size should be 4.5 times the postTestScore and the color determined by sex\ndf.plot.scatter(x='preTestScore' , y='postTestScore' , s=df['postTestScore']*4.5 , c='female' , colormap='viridis')\n### BONUS: Create your own question and answer it.\n# Question: How does age correlate with postTestScore?\n\n# Plotting the relationship between age and postTestScore\nplt.scatter(df['age'], df['postTestScore'], color='blue')\nplt.xlabel('Age')\nplt.ylabel('Post-Test Score')\nplt.title('Scatterplot of Age vs Post-Test Score')\nplt.show()"
  },
  {
    "objectID": "hw-majors.html",
    "href": "hw-majors.html",
    "title": "HW - What should I major in?",
    "section": "",
    "text": "The first step in the process of turning information into knowledge process is to summarize and describe the raw information - the data. In this assignment we explore data on college majors and earnings, specifically the data begin the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "hw-majors.html#packages",
    "href": "hw-majors.html#packages",
    "title": "HW - What should I major in?",
    "section": "Packages",
    "text": "Packages\nUse pandas for data warnagling and processing,letsplot and matplotlib for visualization."
  },
  {
    "objectID": "hw-majors.html#data",
    "href": "hw-majors.html#data",
    "title": "HW - What should I major in?",
    "section": "Data",
    "text": "Data\nThe data can be found on Kaggle.\nLet’s think about some questions we might want to answer with these data:\n\nWhich major has the lowest unemployment rate?\nWhich major has the highest percentage of women?\nHow do the distributions of median income compare across major categories?\nDo women tend to choose majors with lower or higher earnings?\n\nIn the next section we aim to answer these questions."
  },
  {
    "objectID": "hw-majors.html#which-major-has-the-lowest-unemployment-rate",
    "href": "hw-majors.html#which-major-has-the-lowest-unemployment-rate",
    "title": "HW - What should I major in?",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all you need to use pandas sorting and grouping functions."
  },
  {
    "objectID": "hw-majors.html#which-major-has-the-highest-percentage-of-women",
    "href": "hw-majors.html#which-major-has-the-highest-percentage-of-women",
    "title": "HW - What should I major in?",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors."
  },
  {
    "objectID": "hw-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "hw-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "HW - What should I major in?",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: [Wikipedia](https://en.wikipedia.org/wiki/Percentile)\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\n\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories. Create a hishogram. Consider the binwidth we chose for our histogram. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice.\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\nWhich major category has the highest typical (you’ll need to decide what this means) median income?\nWhich major category is the least popular in this sample?"
  },
  {
    "objectID": "hw-majors.html#all-stem-fields-arent-the-same",
    "href": "hw-majors.html#all-stem-fields-arent-the-same",
    "title": "HW - What should I major in?",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a list called stem_categories that lists the major categories that are considered STEM fields.\nThen, use this to create a new variable in our dataframe indicating whether a major is STEM or not.\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’ median earnings.\n\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "hw-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "hw-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "HW - What should I major in?",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nCreate a scatterplot of median income vs. proportion of women in that major, coloured by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "hw-majors.html#further-exploration",
    "href": "hw-majors.html#further-exploration",
    "title": "HW - What should I major in?",
    "section": "Further exploration",
    "text": "Further exploration\n\nAsk a question of interest to you, and answer it using summary statistic(s) and/or visualization(s)."
  },
  {
    "objectID": "homework.html#lecture04",
    "href": "homework.html#lecture04",
    "title": "HomeWork",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"all-ages.csv\")\ndf\nresult = df.groupby([\"Major\"]).sum().sort_values([\"Unemployment_rate\"])\nprint(result)\n\n                                            Major_code  \\\nMajor                                                    \nEDUCATIONAL ADMINISTRATION AND SUPERVISION        2301   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING            2411   \nPHARMACOLOGY                                      3607   \nMATERIALS SCIENCE                                 5008   \nMATHEMATICS AND COMPUTER SCIENCE                  4005   \n...                                                ...   \nLIBRARY SCIENCE                                   3501   \nSCHOOL STUDENT COUNSELING                         2303   \nMILITARY TECHNOLOGIES                             3801   \nCLINICAL PSYCHOLOGY                               5202   \nMISCELLANEOUS FINE ARTS                           6099   \n\n                                                                 Major_category  \\\nMajor                                                                             \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                            Education   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                              Engineering   \nPHARMACOLOGY                                             Biology & Life Science   \nMATERIALS SCIENCE                                                   Engineering   \nMATHEMATICS AND COMPUTER SCIENCE                        Computers & Mathematics   \n...                                                                         ...   \nLIBRARY SCIENCE                                                       Education   \nSCHOOL STUDENT COUNSELING                                             Education   \nMILITARY TECHNOLOGIES                       Industrial Arts & Consumer Services   \nCLINICAL PSYCHOLOGY                                    Psychology & Social Work   \nMISCELLANEOUS FINE ARTS                                                    Arts   \n\n                                            Total  Employed  \\\nMajor                                                         \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   4037      3113   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       6264      4120   \nPHARMACOLOGY                                 5015      3481   \nMATERIALS SCIENCE                            7208      5866   \nMATHEMATICS AND COMPUTER SCIENCE             7184      5874   \n...                                           ...       ...   \nLIBRARY SCIENCE                             16193      7091   \nSCHOOL STUDENT COUNSELING                    2396      1492   \nMILITARY TECHNOLOGIES                        4315      1650   \nCLINICAL PSYCHOLOGY                          7638      5128   \nMISCELLANEOUS FINE ARTS                      8511      6431   \n\n                                            Employed_full_time_year_round  \\\nMajor                                                                       \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                           2468   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                               3350   \nPHARMACOLOGY                                                         2579   \nMATERIALS SCIENCE                                                    4505   \nMATHEMATICS AND COMPUTER SCIENCE                                     5039   \n...                                                                   ...   \nLIBRARY SCIENCE                                                      4330   \nSCHOOL STUDENT COUNSELING                                            1093   \nMILITARY TECHNOLOGIES                                                1708   \nCLINICAL PSYCHOLOGY                                                  3297   \nMISCELLANEOUS FINE ARTS                                              3802   \n\n                                            Unemployed  Unemployment_rate  \\\nMajor                                                                       \nEDUCATIONAL ADMINISTRATION AND SUPERVISION           0           0.000000   \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING               0           0.000000   \nPHARMACOLOGY                                        57           0.016111   \nMATERIALS SCIENCE                                  134           0.022333   \nMATHEMATICS AND COMPUTER SCIENCE                   150           0.024900   \n...                                                ...                ...   \nLIBRARY SCIENCE                                    743           0.094843   \nSCHOOL STUDENT COUNSELING                          169           0.101746   \nMILITARY TECHNOLOGIES                              187           0.101796   \nCLINICAL PSYCHOLOGY                                587           0.102712   \nMISCELLANEOUS FINE ARTS                           1190           0.156147   \n\n                                            Median  P25th     P75th  \nMajor                                                                \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   58000  44750   79000.0  \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       85000  55000  125000.0  \nPHARMACOLOGY                                 60000  35000  105000.0  \nMATERIALS SCIENCE                            75000  60000  100000.0  \nMATHEMATICS AND COMPUTER SCIENCE             92000  53000  136000.0  \n...                                            ...    ...       ...  \nLIBRARY SCIENCE                              40000  30000   55000.0  \nSCHOOL STUDENT COUNSELING                    41000  33200   50000.0  \nMILITARY TECHNOLOGIES                        64000  39750   90000.0  \nCLINICAL PSYCHOLOGY                          45000  26100   62000.0  \nMISCELLANEOUS FINE ARTS                      45000  30000   60000.0  \n\n[173 rows x 10 columns]\n\n\n\n### 按照专业分组，并把失业率从低到高升序排列\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\n\n\n\n\n\n\n\n\nRank\nMajor_code\nMajor\nTotal\nMen\nWomen\nMajor_category\nShareWomen\nSample_size\nEmployed\n...\nPart_time\nFull_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\nCollege_jobs\nNon_college_jobs\nLow_wage_jobs\n\n\n\n\n0\n1\n2419\nPETROLEUM ENGINEERING\n2339.0\n2057.0\n282.0\nEngineering\n0.120564\n36\n1976\n...\n270\n1207\n37\n0.018381\n110000\n95000\n125000\n1534\n364\n193\n\n\n1\n2\n2416\nMINING AND MINERAL ENGINEERING\n756.0\n679.0\n77.0\nEngineering\n0.101852\n7\n640\n...\n170\n388\n85\n0.117241\n75000\n55000\n90000\n350\n257\n50\n\n\n2\n3\n2415\nMETALLURGICAL ENGINEERING\n856.0\n725.0\n131.0\nEngineering\n0.153037\n3\n648\n...\n133\n340\n16\n0.024096\n73000\n50000\n105000\n456\n176\n0\n\n\n3\n4\n2417\nNAVAL ARCHITECTURE AND MARINE ENGINEERING\n1258.0\n1123.0\n135.0\nEngineering\n0.107313\n16\n758\n...\n150\n692\n40\n0.050125\n70000\n43000\n80000\n529\n102\n0\n\n\n4\n5\n2405\nCHEMICAL ENGINEERING\n32260.0\n21239.0\n11021.0\nEngineering\n0.341631\n289\n25694\n...\n5180\n16697\n1672\n0.061098\n65000\n50000\n75000\n18314\n4440\n972\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n169\n3609\nZOOLOGY\n8409.0\n3050.0\n5359.0\nBiology & Life Science\n0.637293\n47\n6259\n...\n2190\n3602\n304\n0.046320\n26000\n20000\n39000\n2771\n2947\n743\n\n\n169\n170\n5201\nEDUCATIONAL PSYCHOLOGY\n2854.0\n522.0\n2332.0\nPsychology & Social Work\n0.817099\n7\n2125\n...\n572\n1211\n148\n0.065112\n25000\n24000\n34000\n1488\n615\n82\n\n\n170\n171\n5202\nCLINICAL PSYCHOLOGY\n2838.0\n568.0\n2270.0\nPsychology & Social Work\n0.799859\n13\n2101\n...\n648\n1293\n368\n0.149048\n25000\n25000\n40000\n986\n870\n622\n\n\n171\n172\n5203\nCOUNSELING PSYCHOLOGY\n4626.0\n931.0\n3695.0\nPsychology & Social Work\n0.798746\n21\n3777\n...\n965\n2738\n214\n0.053621\n23400\n19200\n26000\n2403\n1245\n308\n\n\n172\n173\n3501\nLIBRARY SCIENCE\n1098.0\n134.0\n964.0\nEducation\n0.877960\n2\n742\n...\n237\n410\n87\n0.104946\n22000\n20000\n22000\n288\n338\n192\n\n\n\n\n173 rows × 21 columns\n\n\n\nresult = df.groupby([“Major”]).sum().sort_values([“ShareWomen”],ascending=False) print(result)\n\n### 按照专业分组，将女生占比从高到低降序排列\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\nresult = df.groupby([\"Major\"]).sum().sort_values([\"ShareWomen\"],ascending=False)\nprint(result)\n\n                                               Rank  Major_code     Total  \\\nMajor                                                                       \nEARLY CHILDHOOD EDUCATION                       165        2307   37589.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   164        6102   38279.0   \nMEDICAL ASSISTING SERVICES                       52        6104   11123.0   \nELEMENTARY EDUCATION                            139        2304  170862.0   \nFAMILY AND CONSUMER SCIENCES                    151        2901   58001.0   \n...                                             ...         ...       ...   \nMINING AND MINERAL ENGINEERING                    2        2416     756.0   \nCONSTRUCTION SERVICES                            27        5601   18498.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      67        2504    4790.0   \nFOOD SCIENCE                                     22        1104       0.0   \nMILITARY TECHNOLOGIES                            74        3801     124.0   \n\n                                                   Men     Women  \\\nMajor                                                              \nEARLY CHILDHOOD EDUCATION                       1167.0   36422.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   1225.0   37054.0   \nMEDICAL ASSISTING SERVICES                       803.0   10320.0   \nELEMENTARY EDUCATION                           13029.0  157833.0   \nFAMILY AND CONSUMER SCIENCES                    5166.0   52835.0   \n...                                                ...       ...   \nMINING AND MINERAL ENGINEERING                   679.0      77.0   \nCONSTRUCTION SERVICES                          16820.0    1678.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     4419.0     371.0   \nFOOD SCIENCE                                       0.0       0.0   \nMILITARY TECHNOLOGIES                            124.0       0.0   \n\n                                                                    Major_category  \\\nMajor                                                                                \nEARLY CHILDHOOD EDUCATION                                                Education   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                               Health   \nMEDICAL ASSISTING SERVICES                                                  Health   \nELEMENTARY EDUCATION                                                     Education   \nFAMILY AND CONSUMER SCIENCES                   Industrial Arts & Consumer Services   \n...                                                                            ...   \nMINING AND MINERAL ENGINEERING                                         Engineering   \nCONSTRUCTION SERVICES                          Industrial Arts & Consumer Services   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                            Engineering   \nFOOD SCIENCE                                       Agriculture & Natural Resources   \nMILITARY TECHNOLOGIES                          Industrial Arts & Consumer Services   \n\n                                               ShareWomen  Sample_size  \\\nMajor                                                                    \nEARLY CHILDHOOD EDUCATION                        0.968954          342   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES    0.967998           95   \nMEDICAL ASSISTING SERVICES                       0.927807           67   \nELEMENTARY EDUCATION                             0.923745         1629   \nFAMILY AND CONSUMER SCIENCES                     0.910933          518   \n...                                                   ...          ...   \nMINING AND MINERAL ENGINEERING                   0.101852            7   \nCONSTRUCTION SERVICES                            0.090713          295   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      0.077453           71   \nFOOD SCIENCE                                     0.000000           36   \nMILITARY TECHNOLOGIES                            0.000000            4   \n\n                                               Employed  Full_time  Part_time  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                         32551      27569       7001   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES     29763      19975      13862   \nMEDICAL ASSISTING SERVICES                         9168       5643       4107   \nELEMENTARY EDUCATION                             149339     123177      37965   \nFAMILY AND CONSUMER SCIENCES                      46624      36747      15872   \n...                                                 ...        ...        ...   \nMINING AND MINERAL ENGINEERING                      640        556        170   \nCONSTRUCTION SERVICES                             16318      15690       1751   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES        4186       4175        247   \nFOOD SCIENCE                                       3149       2558       1121   \nMILITARY TECHNOLOGIES                                 0        111          0   \n\n                                               Full_time_year_round  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                                     20748   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                 14460   \nMEDICAL ASSISTING SERVICES                                     4290   \nELEMENTARY EDUCATION                                          86540   \nFAMILY AND CONSUMER SCIENCES                                  26906   \n...                                                             ...   \nMINING AND MINERAL ENGINEERING                                  388   \nCONSTRUCTION SERVICES                                         12313   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                    3607   \nFOOD SCIENCE                                                   1735   \nMILITARY TECHNOLOGIES                                           111   \n\n                                               Unemployed  Unemployment_rate  \\\nMajor                                                                          \nEARLY CHILDHOOD EDUCATION                            1360           0.040105   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES        1487           0.047584   \nMEDICAL ASSISTING SERVICES                            407           0.042507   \nELEMENTARY EDUCATION                                 7297           0.046586   \nFAMILY AND CONSUMER SCIENCES                         3355           0.067128   \n...                                                   ...                ...   \nMINING AND MINERAL ENGINEERING                         85           0.117241   \nCONSTRUCTION SERVICES                                1042           0.060023   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES           250           0.056357   \nFOOD SCIENCE                                          338           0.096931   \nMILITARY TECHNOLOGIES                                   0           0.000000   \n\n                                               Median  P25th  P75th  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                       28000  21000  35000   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   28000  20000  40000   \nMEDICAL ASSISTING SERVICES                      42000  30000  65000   \nELEMENTARY EDUCATION                            32000  23400  38000   \nFAMILY AND CONSUMER SCIENCES                    30000  22900  40000   \n...                                               ...    ...    ...   \nMINING AND MINERAL ENGINEERING                  75000  55000  90000   \nCONSTRUCTION SERVICES                           50000  36000  60000   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     40000  27000  52000   \nFOOD SCIENCE                                    53000  32000  70000   \nMILITARY TECHNOLOGIES                           40000  40000  40000   \n\n                                               College_jobs  Non_college_jobs  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                             23515              7705   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES         19957              9404   \nMEDICAL ASSISTING SERVICES                             2091              6948   \nELEMENTARY EDUCATION                                 108085             36972   \nFAMILY AND CONSUMER SCIENCES                          20985             20133   \n...                                                     ...               ...   \nMINING AND MINERAL ENGINEERING                          350               257   \nCONSTRUCTION SERVICES                                  3275              5351   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES            1861              2121   \nFOOD SCIENCE                                           1183              1274   \nMILITARY TECHNOLOGIES                                     0                 0   \n\n                                               Low_wage_jobs  \nMajor                                                         \nEARLY CHILDHOOD EDUCATION                               2868  \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES           5125  \nMEDICAL ASSISTING SERVICES                              1270  \nELEMENTARY EDUCATION                                   11502  \nFAMILY AND CONSUMER SCIENCES                            5248  \n...                                                      ...  \nMINING AND MINERAL ENGINEERING                            50  \nCONSTRUCTION SERVICES                                    703  \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES              406  \nFOOD SCIENCE                                             485  \nMILITARY TECHNOLOGIES                                      0  \n\n[173 rows x 20 columns]\n\n\n\n### 按照专业分组，将女生占比从高到低降序排列\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\na=df['Median'].groupby(df['Major_category']).sum()\na.plot.bar()\nplt.show()\n\n\n\n\n\n\n\n\nQuestions:What should I major in?\nThe answer: Engineering"
  },
  {
    "objectID": "homework.html#lecture05",
    "href": "homework.html#lecture05",
    "title": "HomeWork",
    "section": "",
    "text": "import pandas as pd\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\ndf = pd.read_csv('plastic-waste.csv')\ndf_clean = df.dropna(subset=['plastic_waste_per_cap', 'continent'])\n\nQuenstion1:Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? The answer:With the histogram, it is possible to observe differences in the per capita distribution of plastic waste across continents. For example, Africa shows a higher peak waste output and North America shows a wider distribution.\n\n# Create histograms faceted by continent\np_histogram = ggplot(df_clean, aes(x='plastic_waste_per_cap')) + \\\n    geom_histogram(bins=30, fill='blue', color='black', alpha=0.7) + \\\n    facet_wrap('continent') + \\\n    ggtitle('Distribution of Plastic Waste per Capita by Continent') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Frequency')\n\n\np_histogram.show()\n\n   \n   \n\n\nQuenstion2:Convert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots? The answer:Fiddle plots show the complete distribution of the data, showing the shape of the data and multiple peaks in the graph. Box plots provide explicit statistical information that violin plots only reflect through shape.\n\n# Violin plots\np_violin = ggplot(df, aes(x='continent', y='plastic_waste_per_cap', fill='continent')) + \\\n    geom_violin(alpha=0.7) + \\\n    geom_boxplot(width=0.1, fill='white', color='black') + \\\n    ggtitle('Violin Plot of Plastic Waste per Capita by Continent') + \\\n    xlab('Continent') + \\\n    ylab('Plastic Waste per Capita')\n\np_violin.show()\n\n   \n   \n\n\nQuenstion3:Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. The answer:The scatter plot presents the relationship between plastic waste per capita and poorly managed waste per capita, checking if there is a positive or other relationship.\n\n# Scatterplot\nif 'mismanaged_plastic_waste_per_cap' in df.columns:\n    p_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap')) + \\\n        geom_point(size=3, alpha=0.6) + \\\n        ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita') + \\\n        xlab('Plastic Waste per Capita') + \\\n        ylab('Mismanaged Plastic Waste per Capita')\n    \n    p_scatter.show()\n\n   \n   \n\n\nQuenstion4:Colour the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? The answer:By distinguishing continents by color, it is possible to observe differences between continents. Certain continents may exhibit specific patterns or clusters, such as the gradual rise of Africa.\n\n# Colored scatterplot\np_scatter_colored = ggplot(df, aes(x='plastic_waste_per_cap', y='mismanaged_plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste vs. Mismanaged Plastic Waste per Capita by Continent') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Mismanaged Plastic Waste per Capita')\n\np_scatter_colored.show()\n\n   \n   \n\n\nQuenstion5:Visualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated? The answer:The visualization of the relationship between the two demographic variables shows the association between plastic waste per capita and total population and coastal population. Through scatterplot analysis, plastic waste per capita exhibits a stronger linear relationship with coastal population.\n\n# Plastic waste per capita vs Total population\np_pop_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='total_pop', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste per Capita vs. Total Population') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Total Population')\n\np_pop_scatter.show()\n\n   \n   \n\n\n\n# Plastic waste per capita vs Coastal population\np_coastal_scatter = ggplot(df, aes(x='plastic_waste_per_cap', y='coastal_pop', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Plastic Waste per Capita vs. Coastal Population') + \\\n    xlab('Plastic Waste per Capita') + \\\n    ylab('Coastal Population')\n\np_coastal_scatter.show()\n\n   \n   \n\n\n\np_coastal_scatter = ggplot(df, aes(x='coastal_pop', y='plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.6) + \\\n    ggtitle('Coastal Population vs. Plastic Waste per Capita') + \\\n    xlab('coastal pop') + \\\n    ylab('plastic waste per cap')\n\np_coastal_scatter.show()\n\n   \n   \n\n\n\np_coastal_scatter.show()\ndf['coastal_population_proportion'] = df['coastal_pop'] / df['total_pop']\ndf_filtered = df\ndf_filtered = df_filtered[(df_filtered['plastic_waste_per_cap'] &lt;= 0.6) & \n(df_filtered['coastal_population_proportion'] &lt;= 1.6)]\np_scatter = ggplot(df_filtered, aes(x='coastal_population_proportion', y='plastic_waste_per_cap', color='continent')) + \\\n    geom_point(size=3, alpha=0.7) + \\\n    geom_smooth(method='lm', color='black', se=True, linetype='solid', size=1) + \\\n    ggtitle('Plastic Waste per Capita vs Coastal Population Proportion') + \\\n    xlab('Coastal Population Proportion') + \\\n    ylab('Plastic Waste per Capita')\n\n\np_scatter.show()"
  },
  {
    "objectID": "homework.html#lecture01",
    "href": "homework.html#lecture01",
    "title": "HomeWork",
    "section": "",
    "text": "This chapter will take you through some of the essential parts of a Python workflow.\nPrerequisites You’ll need an installation of Python and Visual Studio Code with the Python extensions to get to grips with this chapter. If you haven’t installed those yet, head back to {ref}code-preliminaries and follow the instructions there.\nWorking with Python scripts and the interactive window As a reminder, the figure below shows the typical layout of Visual Studio Code.\nA typical user view in Visual Studio Code\nWhen you create a new script (File-&gt;New File-&gt;Save as ’your_script_name.py), it will appear in the part of the screen labelled as 3.\nTo run a script, select the code you want to run, right click, and select “Run Selection/Line in Interactive Window”. You can also hit shift + enter if you set this shortcut up; if you haven’t it’s well worth doing and you can find the instructions in {ref}code-preliminaries.\nUsing the “Run Selection/Line in Interactive Window” option or using the shortcut will cause panel 5 in the above diagram (the interactive window) to appear, where you will see the code run and the outputs of your script appear.\n{tip} If you have an issue getting the code to run in the interactive window, first check the instructions in {ref}code-preliminaries. If you’re still having issues, it may be that Visual Studio Code isn’t sure which Python to run, or where Python is on your system. To fix the latter problem, hit the “Select kernel” button in the top right-hand side of the interactive window. When you are first writing a script, it’s useful to be able to move back and forth between the script and the interactive window. You might execute a line of code (put the cursor on the relevant line and hit shift and enter) in the interactive window, then manually write out some code in the interactive window’s execution box (seen at the bottom of panel 5 saying “Type code here…”), and then explore some of the variables you’ve created with the variable explorer (using the button “Variables”) at the top of the interactive window.\nBut, once you’ve honed the code in your script, it’s good to make the script a complete analytical process that you are happy running end-to-end and that—for production or ‘final’ work—you would use the “Run Current File in Interactive Window” option to run all the way through. This is good practice because what is in your script is reproducible but what you’ve entered manually in the interactive window is not. And you want the outputs from your code to be reproducible and understandable by others (including future you!), but this is hard if there are undocumented extra lines of code that you only did on the fly via the interactive window’s execution box.\nUsing installed packages and modules We already saw how to install packages in {ref}code-preliminaries. If you forgot, look back at how to do this now. In short, packages are installed using the command line or, on Windows, the Anaconda prompt. With either of these open, type conda install packagename and hit enter to both search for and install the package you need.\nWhat about using a package that you’ve installed? That’s what we’ll look at now.\nLet’s see an example of using the powerful numerical library numpy. There are different ways to import packages to use within a script or notebook; you can import the entire package in one go or just import the functions you need (if you know their names). When an entire package is imported, you can give it any name you like and the convention for numpy is to import it as the shortened ‘np’. All of the functions and methods of the package can be accessed by typing np followed by . and then typing the function name. This convention of importing packages with a given name makes your code easier to read, because you know exactly which package is doing what, and avoids any conflicts when functions from different packages have the same name.\nAs well as demonstrating importing the whole package for numpy, the example below shows importing just one specific function from numpy, inv, which does matrix inversion. Note that because inv was imported separately it can be used without an np prefix.\nimport numpy as np from numpy.linalg import inv\nmatrix = np.array([[4.0, 2.0, 4.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\nprint(“Matrix:”) print(matrix)\ninv_mat = inv(matrix) print(“Inverse:”) print(inv_mat) We could have imported all of numpy and it used it without extension using from numpy import * but this is considered bad practice as it fills our ‘namespace’ with function names that might clash with other packages and it’s less easy to read because you don’t know which function came from which package (one of Python’s mantras is “explicit is better than implict”). However, some packages are designed to be used like this, so, for example, you will see from lets_plot import * in this book.\n{note} If you want to check what packages you have installed in your Python environment, run conda list on your computer’s command line (aka the terminal or command prompt). Sometimes you might forget what a function you have imported does! Or at least, you might not be sure what all of the optional arguments are. In Visual Studio Code, you can just hover your cursor over the name of the function and a box will come up that tells you everything you need to know about it. This box is auto-generated by doc-strings; information that is written in text just under a function’s definition (def statement).\nAn alternative way to see what a function does is to use a wonderful package called rich that does many things including providing an inspect() function. You will need to use pip to install rich by running pip install rich on the command line. Here’s an example of using rich’s inpsect method on the inv() function we imported above (methods=True reports all of the functionality of inv()):\nfrom rich import inspect\ninspect(inv, help=True) {admonition} Write a code block that imports the numpy function numpy.linalg.det() as det(). Run inspect() on it. Find the determinant of [[4, 3], [1, 7]]. Modules Sometimes, you will want to call in some code from a different script that you wrote (rather than from a package provided by someone else). Imagine you have several scripts with code in, a, b, and c, all of which need to use the same underlying function that you have written. What do you do? (Note that “script with code in” is just a text file that has a .py extension and contains code.)\nA central tenet of good coding is that you do not repeat yourself. Therefore, a bad solution to this problem would be to copy and paste the same code into all three of the scripts. A good solution is to write the code that’s need just once in a separate ‘utility’ script and have the other scripts import that one function. This also adheres to another important programming principle: that of writing modular code.\nThis schematic shows the kind of situation we’re talking about:\nimport networkx as nx import matplotlib.pyplot as plt import matplotlib_inline.backend_inline\n\n\n\nplt.style.use( “https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt” ) matplotlib_inline.backend_inline.set_matplotlib_formats(“svg”)\ngraph = nx.DiGraph() graph.add_edges_from( [ (“Utility script”, “code file a”), (“Utility script”, “code file b”), (“code file a”, “code file c”), (“code file b”, “code file c”), (“Utility script”, “code file c”), ] ) colour_node = “#AFCBFF” fixed_pos = nx.spring_layout(graph, seed=100) nx.draw(graph, pos=fixed_pos, with_labels=True, node_size=6000, node_color=colour_node) extent = 1.4 plt.xlim(-extent, extent) plt.ylim(-extent, extent) plt.show(); How can we give code files a, b, and c access to the functions etc in the “Utility script”? We would define a file ‘utilities.py’ that had the following function in that we would like to use in the other code files:\n\n\n\ndef really_useful_func(number): return number*10 Then, in ‘code_script_a.py’, we would write:\nimport utilities as utils\nprint(utils.really_useful_func(20)) An alternative is to just import the function we want, with the name we want:\nfrom utilities import really_useful_func as ru_fn\nprint(ru_fn(30)) Another important example is the case where you want to run ‘utilities.py’ as a standalone script, but still want to borrow functions from it to run in other scripts. There’s a way to do this. Let’s change utilities.py to\n\n\n\ndef really_useful_func(number): return number*10\ndef default_func(): print(‘Script has run’)\nif name == ‘main’: default_func() What this says is that if we call ‘utilities.py’ from the command line, eg\npython utilities.py It will return Script has run because, by executing the script alone, we are asking for anything in the main block defined at the end of the file to be run. But we can still import anything from utilities into other scripts as before–and in that case it is not the main script, but an import, and so the main block will not be executed by default.\nYou can important several functions at once from a module (aka another script file) like this:\nfrom utilities import really_useful_func, default_func {admonition} Write your own utilities.py that has a super_useful_func that accepts a number and returns the number divided by 10. In another script, main.py, try a) importing all of utilities and running super_useful_func on a number and, b), importing just super_useful_func from utilities and running it on a number. Reading and writing files Although most applications in economics will use the pandas package to read and write tabular data, it’s sometimes useful to know how to read and write arbitrary files using the built-in Python libraries too. To open a file\nopen(‘filename’, mode) where mode could be r for read, a for append, w for write, and x to create a file. Create a file called text_example.txt and write a single line in it, ‘hello world’. To open the file and print the text, use:\nwith open(‘text_example.txt’) as f: text_in = f.read()\nprint(text_in) ‘hello world!’ is the new line character. Now let’s try adding a line to the file:\nwith open(‘text_example.txt’, ‘a’) as f: f.write(‘this is another line’) Writing and reading files using the with command is a quick and convenient shorthand for the less concise open, action, close pattern. For example, the above example can also be written as:\nf = open(‘text_example.txt’, ‘a’) f.write(‘this is another line’) f.close() Although this short example shows opening and writing a text file, this approach can be used to edit a wide range of file extensions including .json, .xml, .csv, .tsv, and many more, including binary files in addition to plain text files."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "wujindi’s website and data analysis portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio.\n\nHere, I’ll feature my projects for the Fall 2024 Modern Applied Data Analysis class\nPlease use the Menu Bar above to look around."
  },
  {
    "objectID": "index.html#hello-and-thanks-for-visiting",
    "href": "index.html#hello-and-thanks-for-visiting",
    "title": "wujindi’s website and data analysis portfolio",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio.\n\nHere, I’ll feature my projects for the Fall 2024 Modern Applied Data Analysis class\nPlease use the Menu Bar above to look around."
  }
]